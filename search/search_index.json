{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Reference Solution Implementations Application modernization is a journey of moving existing applications to a more modern cloud-native infrastructure. A high level overview of key application modernization concepts is available in the Application Modernization Field Guide and the IBM Architecture Center Application Modernization reference architecture Solutions There are several approaches to application modernization and provided are key reference implementations for approaching your implementation Runtime modernization -- Updating the application runtime to a suitable cloud-native framework and deploying in Red Hat OpenShift Spring modernization -- Updated a prior Spring application by updating to the latest Spring Boot and deploying in Red Hat OpenShift Operation modernization -- Repackaging the application to deploy within a container but maintaining a monolith application without changes to the application or runtime Additional Resources Application Modernization assets Installing Cloud Pak for Applications","title":"Introduction"},{"location":"#reference-solution-implementations","text":"Application modernization is a journey of moving existing applications to a more modern cloud-native infrastructure. A high level overview of key application modernization concepts is available in the Application Modernization Field Guide and the IBM Architecture Center Application Modernization reference architecture","title":"Reference Solution Implementations"},{"location":"#solutions","text":"There are several approaches to application modernization and provided are key reference implementations for approaching your implementation Runtime modernization -- Updating the application runtime to a suitable cloud-native framework and deploying in Red Hat OpenShift Spring modernization -- Updated a prior Spring application by updating to the latest Spring Boot and deploying in Red Hat OpenShift Operation modernization -- Repackaging the application to deploy within a container but maintaining a monolith application without changes to the application or runtime","title":"Solutions"},{"location":"#additional-resources","text":"Application Modernization assets Installing Cloud Pak for Applications","title":"Additional Resources"},{"location":"liberty/","text":"CloudPak for Applications: Runtime Modernization Solution Introduction Runtime modernization moves an application to a 'built for the cloud' runtime with the least amount of effort. WebSphere Liberty is a fast, dynamic, and easy-to-use Java application server, built on the open source Open Liberty project. Ideal or the cloud, Liberty is a combination of IBM technology and open source software, with fast startup times (<2 seconds), no server restarts to pick up changes, and a simple XML configuration. However, WebSphere Liberty doesn't support all of the legacy Java EE and WebSphere proprietary functionality and some code changes maybe required to move an existing application to the new runtime. Effort is also required to move the application configuration from traditional WebSphere to WebSphere Liberty's XML configuration files. This path gets the application on to a cloud-ready runtime container which is easy to use and portable. However, the application is mostly unchanged and has not been 'modernized' to a newer architecture such as micro-services Applications deployed on the WebSphere Liberty container runtime can be build, deployed and managed with the same common technologies and methodologies that would be used by cloud-native (built for the cloud) applications. The diagram below shows the high level decision flow where IBM Cloud Transformation Advisor is used to analyze existing assets and a decision is made to move the monolithic application to the Liberty container. This repository holds a solution that is the result of a runtime modernization for an existing WebSphere Java EE application that was moved from WebSphere ND v8.5.5 to WebSphere Liberty and deployed by the IBM CloudPak for Applications to RedHat OpenShift. Application Overview The Customer Order Services application is a simple store-front shopping application, built during the early days of the Web 2.0 movement. Users interact directly with a browser-based interface and manage their cart to submit orders. This application is built using the traditional 3-Tier Architecture model, with an HTTP server, an application server, and a supporting database. There are several components of the overall application architecture: Starting with the database, the application leverages two SQL-based databases running on IBM DB2 . The application exposes its data model through an Enterprise JavaBean layer, named CustomerOrderServices . This components leverages the Java Persistence API to exposed the backend data model to calling services with minimal coding effort. This build of the application uses JavaEE6 features for EJBs and JPA. The next tier of the application, named CustomerOrderServicesWeb , exposes the necessary business APIs via REST-based web services. This component leverages the JAX-RS libraries for creating Java-based REST services with minimal coding effort. This build of the application is using JAX-RS 1.1 version of the respective capability. The application's user interface is exposed through the CustomerOrderServicesWeb component as well, in the form of a Dojo Toolkit -based JavaScript application. Delivering the user interface and business APIs in the same component is one major inhibitor our migration strategy will help to alleviate in the long-term. Finally, there is an additional integration testing component, named CustomerOrderServicesTest that is built to quickly validate an application's build and deployment to a given application server. This test component contains both JPA and JAX-RS -based tests. How the Application was Modernized In order to modernize the application from WebSphere ND v8.5.5 to WebSphere Liberty running on OpenShift, the application went through analysis , build and deploy phases. Analysis IBM Cloud Transformation Advisor was used to analyze the existing Customer Order Services application and the WebSphere ND runtime. The steps were: Install IBM Cloud Transformation Advisor either in to a Kubernetes Cluster or locally Download and execute the Data Collector against the existing WebSphere ND runtime Upload the results of the data collection in to IBM Cloud Transformation Advisor and review the analysis. A screenshot of the analysis is shown below: In the case of the CustomerOrderServicesApp.ear application, IBM Cloud Transformation Advisor has determined that the migration to WebSphere Liberty on Private Cloud is of Moderate complexity and that there are two Severe Issues that have been detected. Drilling down in to Detailed Migration Analysis Report that is part of the application analysis, it is apparent that IBM Cloud Transformation Advisor has detected that there are issues with lookups for Enterprise JavaBeans and with accessing the Apache Wink APIs. Behavior change on lookups for Enterprise JavaBeans In Liberty, EJB components are not bound to a server root Java Naming and Directory Interface (JNDI) namespace as they are in WebSphere Application Server traditional. The fix for this is to change the three classes that use ejblocal to use the correct URL for Liberty The user of system provided Apache Wink APIs requires configuration To use system-provided third-party APIs in Liberty applications, you must configure the applications to include the APIs. In WebSphere Application Server traditional, these APIs are available without configuration. This is a configuration only change and can be achieved by using a classloader definition in the Liberty server.xml file. 5. In summary, some minimal code changes are required to move this application to the WebSphere Liberty runtime and the decision was taken to proceed with these code changes. Detailed, step-by-step instructions on how to replicate these steps are provided here Build The build phase made changes to source code and created the WebSphere Liberty configuration artifacts. The steps were: Make the simple code changes required for the EJB lookups which were recommended by IBM Cloud Transformation Advisor. The three Java classes that should be modified to look up Enterprise JavaBeans differently are shown in the detailed analysis view of IBM Cloud Transformation Advisor: Below is an example of the code changes required for one of the three Java classes. The org.pwte.example.resources.CategoryResource.java is changed from using ejblocal on line 28 as shown below: Before: java ... InitialContext().lookup(\"ejblocal:org.pwte.example.service.ProductSearchService\"); ... After: java ... InitialContext().lookup(\"java:app/CustomerOrderServices/ProductSearchServiceImpl!org.pwte.example.service.ProductSearchService\"); ... The WebSphere Liberty runtime configuration files server.xml , server.env and jvm.options were created from the templates provided by IBM Cloud Transformation Advisor. The final versions of files can be found here: server.xml server.env jvm.options WebSphere Liberty was configured for application monitoring using Prometheus and the Prometheus JMX Exporter. This was necessary to integrate WebSphere Liberty with the RedHat OpenShift monitoring framework. The Dockerfile required to build the immutable Docker Image containing the application and WebSphere Liberty was created from the template provided by IBM Cloud Transformation Advisor. The final file can be found here: Dockerfile The containerized application was tested locally before the code and configuration files were committed to the git repository Detailed, step-by-step instructions on how to replicate these steps are provided here Deploy The deploy phase created the Jenkins, Kubernetes and RedHat OpenShift artifacts required to automate the build and deployment pipeline for the application. For illustration purposes, the application was deployed to three different RedHat OpenShift projects to simulate development , staging and production . The diagram below shows the flow through the pipeline. A more detailed description can be found here The steps were: Configure the RedHat OpenShift Cluster for WebSphere by creating the necessary SecurityContextConstraints definition. The file can be found here: scc.yaml Create the RedHat OpenShift build template that would be used to define the RedHat OpenShift artifacts related to the build process including ImageStream and BuildConfig definitions. The file can be found here: template-libery-build.yaml Create the RedHat OpenShift deployment template that would be used to define the RedHat OpenShift artifacts related to the Customer Order Services application including DeploymentConfig , Service and Route definitions. The file can be found here: template-libery-deploy.yaml Create the Jenkins Jenkinsfile for the pipeline. The Jenkinsfile defines the steps that the pipeline takes to build the Customer Order Services application EAR file, create an immutable Docker Image and then move the image through the dev , stage and prod environments. The file can be found here: Jenkinsfile Create the build project, load the build template and configure Jenkins Create the dev , stage and prod projects and load the deployment template Verify the pipeline. Detailed, step-by-step instructions on how to replicate these steps are provided here Deploy the Application The following steps will deploy the modernized Customer Order Services application in a WebSphere Liberty container to a RedHat OpenShift cluster. Prerequisites You will need the following: Git CLI RedHat OpenShift 3.11 with Cluster Admin permissions oc CLI DB2 Database Getting the project repository You can clone the repository from its main GitHub repository page and checkout the appropriate branch for this version of the application. git clone https : // github . com / ibm - cloud - architecture / cloudpak - for - applications . git cd cloudpak - for - applications git checkout liberty Create application database infrastructure As said in the prerequisites section above, the Customer Order Services application uses uses DB2 as its database. Follow these steps to create the appropriate database, tables and data the application needs to: Copy the createOrderDB.sql and initialDataSet.sql files you can find in the Common directory of this repository over to the db2 host machine (or git clone the repository) in order to execute them later. Ssh into the db2 host Change to the db2 instance user: `su {database_instance_name}`` Start db2: db2start Create the ORDERDB database: db2 create database ORDERDB Connect to the ORDERDB database: db2 connect to ORDERDB Execute the createOrderDB.sql script you copied over in step 1 in order to create the appropriate tables, relationships, primary keys, etc: db2 -tf createOrderDB.sql Execute the initialDataSet.sql script you copied over in step 1 to populate the ORDERDB database with the needed initial data set: db2 -tf initialDataSet.sql If you want to re-run the scripts, please make sure you drop the databases and create them again. Create the Security Context Constraint In order to deploy and run the WebSphere Liberty Docker image in an OpenShift cluster, we first need to configure certain security aspects for the cluster. The Security Context Constraint provided here grants the service account that the WebSphere Liberty Docker container is running under the required privileges to function correctly. A cluster administrator can use the file provided here with the following command to create the Security Context Constraint (SCC): cd Deployment / OpenShift oc apply - f ssc . yaml Create the projects Four RedHat OpenShift projects are required in this scenario: Build: this project will contain the Jenkins server and the artifacts used to build the application image Dev: this is the development environment for this application Stage: this is the staging environment for this application Prod: this is the production environment for this application The file provided here contains the definitions for the four projects in a single file to make creation easier Issue the command shown below to create the projects oc create - f liberty - projects . yaml Create a service account It is a good Kubernetes practice to create a service account for your applications. A service account provides an identity for processes that run in a Pod. In this step we will create a new service account with the name websphere in each of the dev , stage and prod projects and add the Security Context Constraint created above to them. Issue the commands shown below to create the websphere service account and bind the ibm-websphere-scc to it in each of the projects: oc create serviceaccount websphere - n cos - liberty - dev oc create serviceaccount websphere - n cos - liberty - stage oc create serviceaccount websphere - n cos - liberty - prod oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - liberty - dev oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - liberty - stage oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - liberty - prod Deploy Jenkins Some RedHat OpenShift clusters are configured to automatically provision a Jenkins instance in a build project. The steps below can be used if your cluster is not configured for automatic Jenkins provisioning: oc project cos - liberty - build oc new - app jenkins - persistent Update the Jenkins service account During provisioning of the Jenkins master a service account with the name jenkins is created. This service account has privileges to create new artifacts only in the project that it is running in. In this scenario Jenkins will need to create artifacts in the dev , stage and prod projects. Issue the commands below to allow the jenkins service account to edit artifacts in the dev , stage and prod projects. oc policy add - role - to - user edit system : serviceaccount : cos - liberty - build : jenkins - n cos - liberty - dev oc policy add - role - to - user edit system : serviceaccount : cos - liberty - build : jenkins - n cos - liberty - stage oc policy add - role - to - user edit system : serviceaccount : cos - liberty - build : jenkins - n cos - liberty - prod Import the deployment templates RedHat OpenShift templates are used to make artifact creation easier and repeatable. The template definition provided here defines a Kubernetes Service , Route and DeploymentConfig for the CustomerOrderServices application. The gse-liberty-deploy template defines the following: service listening on ports 9080 , 9443 and 9082 route to expose the 9443 port externally DeploymentConfig to host the WebSphere Liberty container. The image for the container is taken from the ImageStream that will be populated by the Jenkins pipeline. environment variables are defined for the DB2 database used by the application allowing for environment specific information to be injected Probes for liveness and readiness are defined to check port 9443 is active The securityContext is set to allow read/write access to the filesystem and to run the container as user 1001 The deployment will be updated if a new image is loaded to the ImageStream or if a change to the configuration is detected. Issue the commands below to load the template named gse-liberty-deploy in the dev , stage and prod projects. oc create - f template - liberty - deploy . yaml - n cos - liberty - dev oc create - f template - liberty - deploy . yaml - n cos - liberty - stage oc create - f template - liberty - deploy . yaml - n cos - liberty - prod Create the deployment definitions In this step the gse-liberty-deploy template will be used to create a RedHat OpenShift application named cos-liberty in the dev , stage and prod namespaces. The result will be: service listening on ports 9080 , 9443 and 9082 route to expose the 9443 port externally DeploymentConfig to host the WebSphere Liberty container. The deployment config will wait for a docker image to be loaded in to the ImageStream by the Jenkins pipeline. Issue the following commands to create the applications from the template: oc new - app gse - liberty - deploy - p APPLICATION_NAME = cos - liberty - p DB2_HOST =< your DB2 host > - p DB2_PORT =< your DB2 host > - p DB2_USER =< your DB2 user > - p DB2_PASSWORD =< your DB2 password > - n cos - liberty - dev oc new - app gse - liberty - deploy - p APPLICATION_NAME = cos - liberty - p DB2_HOST =< your DB2 host > - p DB2_PORT =< your DB2 host > - p DB2_USER =< your DB2 user > - p DB2_PASSWORD =< your DB2 password > - n cos - liberty - stage oc new - app gse - liberty - deploy - p APPLICATION_NAME = cos - liberty - p DB2_HOST =< your DB2 host > - p DB2_PORT =< your DB2 host > - p DB2_USER =< your DB2 user > - p DB2_PASSWORD =< your DB2 password > - n cos - liberty - prod Import the build templates In this step a template for the build process will be loaded in to the build project. The template provided here defines the following artifacts: An ImageStream for the application image. This will be populated by the Jenkins Pipeline An ImageStream for WebSphere Liberty which will pull down the latest version of the ibmcom/websphere-liberty:kernel-ubi-min image and will monitor DockerHub for any updates. A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub Parameters to allow the WebSphere Liberty image and GitHub repository to be provided when the template is instantiated Issue the commands below to load the template named gse-liberty-build in the build projects. oc create - f template - liberty - build . yaml - n cos - liberty - build Create the build definitions In this step the gse-liberty-build template will be used to create a RedHat OpenShift application named cos-liberty in the build namespaces. The result will be: An ImageStream for the application image. This will be populated by the Jenkins Pipeline An ImageStream for WebSphere Liberty which will pull down the latest version of the ibmcom/websphere-liberty:kernel-ubi-min image and will monitor DockerHub for any updates. A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub (with the URL provided as a parameter when the application is created) Issue the following commands to create the application from the template: oc new - app gse - liberty - build - p APPLICATION_NAME = cos - liberty - p SOURCE_URL = \" https://github.com/ibm-cloud-architecture/cloudpak-for-applications \" - n cos - liberty - build Run the pipeline The newly created pipeline can be started from the RedHat OpenShift console which allows access to the Jenkins logs but also tracks the progress in the OCP console. Navigate to Application Console --> Customer Order Services on Liberty - Build --> Builds --> Pipelines and click the Start Pipeline button 2. When the pipeline starts, click the view log link to go to the Jenkins administration console. Note that it may take a couple of minutes before the view log link appears on the first pipeline build 3. When prompted, log in with your OpenShift account and grant the required access permissions. The Jenkins console log will be displayed as shown below: 4. Return to the OpenShift Console and track the progress of the pipeline 5. The pipeline will eventually stop at the Promotion Gate for approval to deploy to Production. Click the Input Required link as shown below 6. When the Promote application to Production question is displayed, click Proceed 7. Return to the OpenShift Console and validate that the pipeline is now complete Validate the Application Now that the pipeline is complete, validate the Customer Order Services application is deployed and running in dev , stage and prod In the OpenShift Console, navigate to Application Console --> Customer Order Services on Liberty - Dev --> Applications --> Deployments and click on the link in the Latest Version column Information about the deployment will be displayed including the image that is being used (note the tag on the image as it will be the same in the stage and prod deployments). After a few minutes the container will be marked as ready Click Applications --> Routes and click on the route for the application. Note that the URL is < application_name >-< project_name >.< ocp cluster url >. In this case the project name is cos-liberty-dev Add /CustomerOrderServicesWeb to the end of the URL in the browser to access the application Log in to the application with username: rbarcia and password: bl0wfish Repeat the validations for the stage and prod Projects. Summary This application has been modified from the initial WebSphere ND v8.5.5 version to run on WebSphere Liberty and deployed by the IBM CloudPak for Applications.","title":"Runtime modernization"},{"location":"liberty/#cloudpak-for-applications-runtime-modernization-solution","text":"","title":"CloudPak for Applications: Runtime Modernization Solution"},{"location":"liberty/#introduction","text":"Runtime modernization moves an application to a 'built for the cloud' runtime with the least amount of effort. WebSphere Liberty is a fast, dynamic, and easy-to-use Java application server, built on the open source Open Liberty project. Ideal or the cloud, Liberty is a combination of IBM technology and open source software, with fast startup times (<2 seconds), no server restarts to pick up changes, and a simple XML configuration. However, WebSphere Liberty doesn't support all of the legacy Java EE and WebSphere proprietary functionality and some code changes maybe required to move an existing application to the new runtime. Effort is also required to move the application configuration from traditional WebSphere to WebSphere Liberty's XML configuration files. This path gets the application on to a cloud-ready runtime container which is easy to use and portable. However, the application is mostly unchanged and has not been 'modernized' to a newer architecture such as micro-services Applications deployed on the WebSphere Liberty container runtime can be build, deployed and managed with the same common technologies and methodologies that would be used by cloud-native (built for the cloud) applications. The diagram below shows the high level decision flow where IBM Cloud Transformation Advisor is used to analyze existing assets and a decision is made to move the monolithic application to the Liberty container. This repository holds a solution that is the result of a runtime modernization for an existing WebSphere Java EE application that was moved from WebSphere ND v8.5.5 to WebSphere Liberty and deployed by the IBM CloudPak for Applications to RedHat OpenShift.","title":"Introduction"},{"location":"liberty/#application-overview","text":"The Customer Order Services application is a simple store-front shopping application, built during the early days of the Web 2.0 movement. Users interact directly with a browser-based interface and manage their cart to submit orders. This application is built using the traditional 3-Tier Architecture model, with an HTTP server, an application server, and a supporting database. There are several components of the overall application architecture: Starting with the database, the application leverages two SQL-based databases running on IBM DB2 . The application exposes its data model through an Enterprise JavaBean layer, named CustomerOrderServices . This components leverages the Java Persistence API to exposed the backend data model to calling services with minimal coding effort. This build of the application uses JavaEE6 features for EJBs and JPA. The next tier of the application, named CustomerOrderServicesWeb , exposes the necessary business APIs via REST-based web services. This component leverages the JAX-RS libraries for creating Java-based REST services with minimal coding effort. This build of the application is using JAX-RS 1.1 version of the respective capability. The application's user interface is exposed through the CustomerOrderServicesWeb component as well, in the form of a Dojo Toolkit -based JavaScript application. Delivering the user interface and business APIs in the same component is one major inhibitor our migration strategy will help to alleviate in the long-term. Finally, there is an additional integration testing component, named CustomerOrderServicesTest that is built to quickly validate an application's build and deployment to a given application server. This test component contains both JPA and JAX-RS -based tests.","title":"Application Overview"},{"location":"liberty/#how-the-application-was-modernized","text":"In order to modernize the application from WebSphere ND v8.5.5 to WebSphere Liberty running on OpenShift, the application went through analysis , build and deploy phases.","title":"How the Application was Modernized"},{"location":"liberty/#analysis","text":"IBM Cloud Transformation Advisor was used to analyze the existing Customer Order Services application and the WebSphere ND runtime. The steps were: Install IBM Cloud Transformation Advisor either in to a Kubernetes Cluster or locally Download and execute the Data Collector against the existing WebSphere ND runtime Upload the results of the data collection in to IBM Cloud Transformation Advisor and review the analysis. A screenshot of the analysis is shown below: In the case of the CustomerOrderServicesApp.ear application, IBM Cloud Transformation Advisor has determined that the migration to WebSphere Liberty on Private Cloud is of Moderate complexity and that there are two Severe Issues that have been detected. Drilling down in to Detailed Migration Analysis Report that is part of the application analysis, it is apparent that IBM Cloud Transformation Advisor has detected that there are issues with lookups for Enterprise JavaBeans and with accessing the Apache Wink APIs. Behavior change on lookups for Enterprise JavaBeans In Liberty, EJB components are not bound to a server root Java Naming and Directory Interface (JNDI) namespace as they are in WebSphere Application Server traditional. The fix for this is to change the three classes that use ejblocal to use the correct URL for Liberty The user of system provided Apache Wink APIs requires configuration To use system-provided third-party APIs in Liberty applications, you must configure the applications to include the APIs. In WebSphere Application Server traditional, these APIs are available without configuration. This is a configuration only change and can be achieved by using a classloader definition in the Liberty server.xml file. 5. In summary, some minimal code changes are required to move this application to the WebSphere Liberty runtime and the decision was taken to proceed with these code changes. Detailed, step-by-step instructions on how to replicate these steps are provided here","title":"Analysis"},{"location":"liberty/#build","text":"The build phase made changes to source code and created the WebSphere Liberty configuration artifacts. The steps were: Make the simple code changes required for the EJB lookups which were recommended by IBM Cloud Transformation Advisor. The three Java classes that should be modified to look up Enterprise JavaBeans differently are shown in the detailed analysis view of IBM Cloud Transformation Advisor: Below is an example of the code changes required for one of the three Java classes. The org.pwte.example.resources.CategoryResource.java is changed from using ejblocal on line 28 as shown below: Before: java ... InitialContext().lookup(\"ejblocal:org.pwte.example.service.ProductSearchService\"); ... After: java ... InitialContext().lookup(\"java:app/CustomerOrderServices/ProductSearchServiceImpl!org.pwte.example.service.ProductSearchService\"); ... The WebSphere Liberty runtime configuration files server.xml , server.env and jvm.options were created from the templates provided by IBM Cloud Transformation Advisor. The final versions of files can be found here: server.xml server.env jvm.options WebSphere Liberty was configured for application monitoring using Prometheus and the Prometheus JMX Exporter. This was necessary to integrate WebSphere Liberty with the RedHat OpenShift monitoring framework. The Dockerfile required to build the immutable Docker Image containing the application and WebSphere Liberty was created from the template provided by IBM Cloud Transformation Advisor. The final file can be found here: Dockerfile The containerized application was tested locally before the code and configuration files were committed to the git repository Detailed, step-by-step instructions on how to replicate these steps are provided here","title":"Build"},{"location":"liberty/#deploy","text":"The deploy phase created the Jenkins, Kubernetes and RedHat OpenShift artifacts required to automate the build and deployment pipeline for the application. For illustration purposes, the application was deployed to three different RedHat OpenShift projects to simulate development , staging and production . The diagram below shows the flow through the pipeline. A more detailed description can be found here The steps were: Configure the RedHat OpenShift Cluster for WebSphere by creating the necessary SecurityContextConstraints definition. The file can be found here: scc.yaml Create the RedHat OpenShift build template that would be used to define the RedHat OpenShift artifacts related to the build process including ImageStream and BuildConfig definitions. The file can be found here: template-libery-build.yaml Create the RedHat OpenShift deployment template that would be used to define the RedHat OpenShift artifacts related to the Customer Order Services application including DeploymentConfig , Service and Route definitions. The file can be found here: template-libery-deploy.yaml Create the Jenkins Jenkinsfile for the pipeline. The Jenkinsfile defines the steps that the pipeline takes to build the Customer Order Services application EAR file, create an immutable Docker Image and then move the image through the dev , stage and prod environments. The file can be found here: Jenkinsfile Create the build project, load the build template and configure Jenkins Create the dev , stage and prod projects and load the deployment template Verify the pipeline. Detailed, step-by-step instructions on how to replicate these steps are provided here","title":"Deploy"},{"location":"liberty/#deploy-the-application","text":"The following steps will deploy the modernized Customer Order Services application in a WebSphere Liberty container to a RedHat OpenShift cluster.","title":"Deploy the Application"},{"location":"liberty/#prerequisites","text":"You will need the following: Git CLI RedHat OpenShift 3.11 with Cluster Admin permissions oc CLI DB2 Database","title":"Prerequisites"},{"location":"liberty/#getting-the-project-repository","text":"You can clone the repository from its main GitHub repository page and checkout the appropriate branch for this version of the application. git clone https : // github . com / ibm - cloud - architecture / cloudpak - for - applications . git cd cloudpak - for - applications git checkout liberty","title":"Getting the project repository"},{"location":"liberty/#create-application-database-infrastructure","text":"As said in the prerequisites section above, the Customer Order Services application uses uses DB2 as its database. Follow these steps to create the appropriate database, tables and data the application needs to: Copy the createOrderDB.sql and initialDataSet.sql files you can find in the Common directory of this repository over to the db2 host machine (or git clone the repository) in order to execute them later. Ssh into the db2 host Change to the db2 instance user: `su {database_instance_name}`` Start db2: db2start Create the ORDERDB database: db2 create database ORDERDB Connect to the ORDERDB database: db2 connect to ORDERDB Execute the createOrderDB.sql script you copied over in step 1 in order to create the appropriate tables, relationships, primary keys, etc: db2 -tf createOrderDB.sql Execute the initialDataSet.sql script you copied over in step 1 to populate the ORDERDB database with the needed initial data set: db2 -tf initialDataSet.sql If you want to re-run the scripts, please make sure you drop the databases and create them again.","title":"Create application database infrastructure"},{"location":"liberty/#create-the-security-context-constraint","text":"In order to deploy and run the WebSphere Liberty Docker image in an OpenShift cluster, we first need to configure certain security aspects for the cluster. The Security Context Constraint provided here grants the service account that the WebSphere Liberty Docker container is running under the required privileges to function correctly. A cluster administrator can use the file provided here with the following command to create the Security Context Constraint (SCC): cd Deployment / OpenShift oc apply - f ssc . yaml","title":"Create the Security Context Constraint"},{"location":"liberty/#create-the-projects","text":"Four RedHat OpenShift projects are required in this scenario: Build: this project will contain the Jenkins server and the artifacts used to build the application image Dev: this is the development environment for this application Stage: this is the staging environment for this application Prod: this is the production environment for this application The file provided here contains the definitions for the four projects in a single file to make creation easier Issue the command shown below to create the projects oc create - f liberty - projects . yaml","title":"Create the projects"},{"location":"liberty/#create-a-service-account","text":"It is a good Kubernetes practice to create a service account for your applications. A service account provides an identity for processes that run in a Pod. In this step we will create a new service account with the name websphere in each of the dev , stage and prod projects and add the Security Context Constraint created above to them. Issue the commands shown below to create the websphere service account and bind the ibm-websphere-scc to it in each of the projects: oc create serviceaccount websphere - n cos - liberty - dev oc create serviceaccount websphere - n cos - liberty - stage oc create serviceaccount websphere - n cos - liberty - prod oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - liberty - dev oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - liberty - stage oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - liberty - prod","title":"Create a service account"},{"location":"liberty/#deploy-jenkins","text":"Some RedHat OpenShift clusters are configured to automatically provision a Jenkins instance in a build project. The steps below can be used if your cluster is not configured for automatic Jenkins provisioning: oc project cos - liberty - build oc new - app jenkins - persistent","title":"Deploy Jenkins"},{"location":"liberty/#update-the-jenkins-service-account","text":"During provisioning of the Jenkins master a service account with the name jenkins is created. This service account has privileges to create new artifacts only in the project that it is running in. In this scenario Jenkins will need to create artifacts in the dev , stage and prod projects. Issue the commands below to allow the jenkins service account to edit artifacts in the dev , stage and prod projects. oc policy add - role - to - user edit system : serviceaccount : cos - liberty - build : jenkins - n cos - liberty - dev oc policy add - role - to - user edit system : serviceaccount : cos - liberty - build : jenkins - n cos - liberty - stage oc policy add - role - to - user edit system : serviceaccount : cos - liberty - build : jenkins - n cos - liberty - prod","title":"Update the Jenkins service account"},{"location":"liberty/#import-the-deployment-templates","text":"RedHat OpenShift templates are used to make artifact creation easier and repeatable. The template definition provided here defines a Kubernetes Service , Route and DeploymentConfig for the CustomerOrderServices application. The gse-liberty-deploy template defines the following: service listening on ports 9080 , 9443 and 9082 route to expose the 9443 port externally DeploymentConfig to host the WebSphere Liberty container. The image for the container is taken from the ImageStream that will be populated by the Jenkins pipeline. environment variables are defined for the DB2 database used by the application allowing for environment specific information to be injected Probes for liveness and readiness are defined to check port 9443 is active The securityContext is set to allow read/write access to the filesystem and to run the container as user 1001 The deployment will be updated if a new image is loaded to the ImageStream or if a change to the configuration is detected. Issue the commands below to load the template named gse-liberty-deploy in the dev , stage and prod projects. oc create - f template - liberty - deploy . yaml - n cos - liberty - dev oc create - f template - liberty - deploy . yaml - n cos - liberty - stage oc create - f template - liberty - deploy . yaml - n cos - liberty - prod","title":"Import the deployment templates"},{"location":"liberty/#create-the-deployment-definitions","text":"In this step the gse-liberty-deploy template will be used to create a RedHat OpenShift application named cos-liberty in the dev , stage and prod namespaces. The result will be: service listening on ports 9080 , 9443 and 9082 route to expose the 9443 port externally DeploymentConfig to host the WebSphere Liberty container. The deployment config will wait for a docker image to be loaded in to the ImageStream by the Jenkins pipeline. Issue the following commands to create the applications from the template: oc new - app gse - liberty - deploy - p APPLICATION_NAME = cos - liberty - p DB2_HOST =< your DB2 host > - p DB2_PORT =< your DB2 host > - p DB2_USER =< your DB2 user > - p DB2_PASSWORD =< your DB2 password > - n cos - liberty - dev oc new - app gse - liberty - deploy - p APPLICATION_NAME = cos - liberty - p DB2_HOST =< your DB2 host > - p DB2_PORT =< your DB2 host > - p DB2_USER =< your DB2 user > - p DB2_PASSWORD =< your DB2 password > - n cos - liberty - stage oc new - app gse - liberty - deploy - p APPLICATION_NAME = cos - liberty - p DB2_HOST =< your DB2 host > - p DB2_PORT =< your DB2 host > - p DB2_USER =< your DB2 user > - p DB2_PASSWORD =< your DB2 password > - n cos - liberty - prod","title":"Create the deployment definitions"},{"location":"liberty/#import-the-build-templates","text":"In this step a template for the build process will be loaded in to the build project. The template provided here defines the following artifacts: An ImageStream for the application image. This will be populated by the Jenkins Pipeline An ImageStream for WebSphere Liberty which will pull down the latest version of the ibmcom/websphere-liberty:kernel-ubi-min image and will monitor DockerHub for any updates. A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub Parameters to allow the WebSphere Liberty image and GitHub repository to be provided when the template is instantiated Issue the commands below to load the template named gse-liberty-build in the build projects. oc create - f template - liberty - build . yaml - n cos - liberty - build","title":"Import the build templates"},{"location":"liberty/#create-the-build-definitions","text":"In this step the gse-liberty-build template will be used to create a RedHat OpenShift application named cos-liberty in the build namespaces. The result will be: An ImageStream for the application image. This will be populated by the Jenkins Pipeline An ImageStream for WebSphere Liberty which will pull down the latest version of the ibmcom/websphere-liberty:kernel-ubi-min image and will monitor DockerHub for any updates. A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub (with the URL provided as a parameter when the application is created) Issue the following commands to create the application from the template: oc new - app gse - liberty - build - p APPLICATION_NAME = cos - liberty - p SOURCE_URL = \" https://github.com/ibm-cloud-architecture/cloudpak-for-applications \" - n cos - liberty - build","title":"Create the build definitions"},{"location":"liberty/#run-the-pipeline","text":"The newly created pipeline can be started from the RedHat OpenShift console which allows access to the Jenkins logs but also tracks the progress in the OCP console. Navigate to Application Console --> Customer Order Services on Liberty - Build --> Builds --> Pipelines and click the Start Pipeline button 2. When the pipeline starts, click the view log link to go to the Jenkins administration console. Note that it may take a couple of minutes before the view log link appears on the first pipeline build 3. When prompted, log in with your OpenShift account and grant the required access permissions. The Jenkins console log will be displayed as shown below: 4. Return to the OpenShift Console and track the progress of the pipeline 5. The pipeline will eventually stop at the Promotion Gate for approval to deploy to Production. Click the Input Required link as shown below 6. When the Promote application to Production question is displayed, click Proceed 7. Return to the OpenShift Console and validate that the pipeline is now complete","title":"Run the pipeline"},{"location":"liberty/#validate-the-application","text":"Now that the pipeline is complete, validate the Customer Order Services application is deployed and running in dev , stage and prod In the OpenShift Console, navigate to Application Console --> Customer Order Services on Liberty - Dev --> Applications --> Deployments and click on the link in the Latest Version column Information about the deployment will be displayed including the image that is being used (note the tag on the image as it will be the same in the stage and prod deployments). After a few minutes the container will be marked as ready Click Applications --> Routes and click on the route for the application. Note that the URL is < application_name >-< project_name >.< ocp cluster url >. In this case the project name is cos-liberty-dev Add /CustomerOrderServicesWeb to the end of the URL in the browser to access the application Log in to the application with username: rbarcia and password: bl0wfish Repeat the validations for the stage and prod Projects.","title":"Validate the Application"},{"location":"liberty/#summary","text":"This application has been modified from the initial WebSphere ND v8.5.5 version to run on WebSphere Liberty and deployed by the IBM CloudPak for Applications.","title":"Summary"},{"location":"liberty/liberty-analyze/","text":"Liberty - Analyze This section covers how to use IBM Cloud Transformation Advisor to analyze an existing traditional WebSphere application. For this scenario the WebSphere Liberty runtime is chosen as the target runtime and the intention is to migrate this application with minimal code changes. WebSphere Liberty is a fast, dynamic, and easy-to-use Java application server, built on the open source Open Liberty project. Ideal or the cloud, Liberty is a combination of IBM technology and open source software, with fast startup times (<2 seconds), no server restarts to pick up changes, and a simple XML configuration. Summary This section has the following steps: Introduction to IBM Cloud Transformation Advisor Install IBM Cloud Transformation Advisor Download and run the Data Collector Upload and analyze the results Determine the migration/modernization path and next steps Introduction to IBM Cloud Transformation Advisor IBM Cloud Transformation Advisor helps you access, analyze and modernize middleware based apps into IBM Cloud(s). It categorizes Java EE apps and MQ queue managers as simple, medium and complex based on migration complexity and provides guidance for modernization. IBM Cloud Transformation Advisor can accelerate the process to move your on-premises apps to cloud, minimize migration errors and risks, and reduce time to market in five steps. You can use IBM Transformation Advisor for these activities: - Identify the Java EE programming models in an app - Determine the complexity of apps by reviewing a high-level inventory of the content and structure of each app - Highlight the Java EE programming model and WebSphere API differences between the profile types - Learn any Java EE specification implementation differences that might affect the app The tool also provides suggestions for the right-fit IBM WebSphere Application Server edition and offers advice, practices, and potential solutions to move apps to WebSphere Liberty or to newer versions of WebSphere Application Server. Install IBM Cloud Transformation Advisor IBM Cloud Transformation Advisor is split in to two components (the analysis engine and the data collector ). You can choose to between installing the analysis engine in to an IBM Cloud Private Cluster or locally on a machine with Docker. Installing IBM Cloud Transformation Advisor in to your IBM Cloud Private Cluster Installing IBM Cloud Transformation Advisor Beta Edition locally Download the Data Collector Once IBM Cloud Transformation Advisor is installed, it is necessary to create a new Workspace and Collection and then download the Data Collector that will be used to examine the existing environment and applications. Open IBM Cloud Transformation Advisor in a browser and click the button to create a new Workspace Enter a Workspace name such as CloudPak_for_Applications and click Next Enter a Collection name such as WAS855_AppSrv01 and click Let's go When the No recommendations available page is displayed, click the Data Collector button When the Data Collector page is displayed, select the Source Operating System for your environment and click the Download button to download the Data Collector. This results in a file with a name similar to transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz being downloaded. Run the Data Collector Upload the Data Collector zip file that was downloaded from IBM Cloud Transformation Advisor in the previous step to the machine that the WebSphere ND Deployment Manager or the Standalone WebSphere Application Server is installed. The directory used arbitrary. Navigate to the directory you uploaded the transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz file to and issue the following commands to extract the Data Collector: bash mkdir datacollector cd datacollector mv transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz . tar -zxvf transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz cd transformationadvisor-* It is necessary to modify the scan performed by the Data Collector to include the org.pwte package as the Data Collector doesn't scan org.* packages by default. Open the conf/customCmd.properties file and modify it as shown below: bash evaluation=--evaluate --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp --includePackages=org.pwte migration_liberty=--analyze --sourceAppServer=was855 --targetAppServer=liberty --targetCloud=dockerIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp migration_was=--analyze --sourceAppServer=was855 --targetAppServer=was90 --targetCloud=vmIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp #inventory=--inventory --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #featureList=--featureList --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #java_opt=-Xmx2g The following command assumes that WebSphere Application Server v855 is installed to /opt/IBM/WebSphere/AppServer855 with a profile named AppSrv01 and that the administration user is wasadmin with a password of wasadmin . Modify and issue the following command as necessary to execute the Data Collector against the WebSphere environment: bash ./bin/transformationadvisor -w /opt/IBM/WebSphere/AppServer855 -p AppSrv01 wasadmin wasadmin When prompted, accept the license agreement . The Data Collection process will now start and will analyze all of the applications installed in the WebSphere Application Server environment and will also collect the related Java EE artifacts such as Data Sources and JMS definitions. When the analysis is complete, the Data Collector will attempt to upload the collection results to IBM Cloud Transformation Advisor. If this is successful, you can skip to the Analyze the Recommendations section. If not, you will receive an error at the end of Data Collection and will find a file named AppSrv01.zip in your current directory as shown below. bash ~/datacollector/transformationadvisor-1.9.6# ls -la *.zip -rw-r--r-- 1 root root 625493 Jun 12 12:58 AppSrv01.zip Download this Data Collector Results zip file ready for uploading to IBM Cloud Transformation Advisor in the next section Upload the Data Collector results In this section the results from the Data Collector will be uploaded to IBM Cloud Transformation Advisor. In the IBM Cloud Transformation Advisor web browser session, click the Recomendations link in the top left corner and then click the Upload data button as shown below When the Upload data dialog is displayed, use the Drop or Add File button to select the Data Collector Results zip file that was downloaded in the previous section. Click Upload After a few moments the upload of the data collector results will be completed. Analyze the Recommendations Once the Data Collector Results have been uploaded to IBM Cloud Transformation Advisor a set of recommendations will be created and shown on the Recommendations page. In this section the recommendations will be analyzed and interpreted. The default recommendations are based on a target runtime of Liberty on Private Cloud . The Data Collector analyzed all of the applications running on the traditional WebSphere profile a displays a row in the chart for each application. In the case of the CustomerOrderServicesApp.ear application, IBM Cloud Transformation Advisor has determined that the migration to WebSphere Traditional on Private Cloud is of Moderate complexity and that there are two Severe Issues that have been detected. Click on the CustomerOrderServicesApp.ear application name to see more information. Review the analysis results and scroll down to the Technology Issues section. Note that IBM Cloud Transformation Advisor has detected that there are issues with lookups for Enterprise JavaBeans and with accessing the Apache Wink APIs. In order to review the IBM Cloud Transformation Advisor results in more detail, scroll to the bottom of the analysis page and click on the Analysis Report link When the warning dialog is displayed, click OK The Detailed Migration Analysis Report will be displayed which show the results of the migration rules that were executed by the Data Collector and returned results. Scroll down to the Severe Rules section and click on the Show rule help link for each of the results. Review the recommendations. Behavior change on lookups for Enterprise JavaBeans In Liberty, EJB components are not bound to a server root Java Naming and Directory Interface (JNDI) namespace as they are in WebSphere Application Server traditional. The fix for this is to change the three classes that use ejblocal to use the correct URL for Liberty The user of system provided Apache Wink APIs requires configuration To use system-provided third-party APIs in Liberty applications, you must configure the applications to include the APIs. In WebSphere Application Server traditional, these APIs are available without configuration. This is a configuration only change and can be achieved by using a classloader definition in the Liberty server.xml file. Final Analysis The intention of this traditional WebSphere V855 --> Liberty (Private Cloud) scenario is to migrate the Customer Order Services application to the cloud-ready new runtime with minimal code changes. IBM Cloud Transformation Advisor was used to analyze the application for compatibility with traditional WebSphere V9 (Private Cloud) and determined that only small changes to three classes would be required. While this scenario will move the application to the cloud-ready WebSphere Liberty runtime in a container, it will not modernize the application architecture and code in any way. Now proceed to the Liberty - Build section where the process of making the code changes and configuring the Liberty runtime will be covered step-by-step","title":"Liberty - Analyze"},{"location":"liberty/liberty-analyze/#liberty-analyze","text":"This section covers how to use IBM Cloud Transformation Advisor to analyze an existing traditional WebSphere application. For this scenario the WebSphere Liberty runtime is chosen as the target runtime and the intention is to migrate this application with minimal code changes. WebSphere Liberty is a fast, dynamic, and easy-to-use Java application server, built on the open source Open Liberty project. Ideal or the cloud, Liberty is a combination of IBM technology and open source software, with fast startup times (<2 seconds), no server restarts to pick up changes, and a simple XML configuration.","title":"Liberty - Analyze"},{"location":"liberty/liberty-analyze/#summary","text":"This section has the following steps: Introduction to IBM Cloud Transformation Advisor Install IBM Cloud Transformation Advisor Download and run the Data Collector Upload and analyze the results Determine the migration/modernization path and next steps","title":"Summary"},{"location":"liberty/liberty-analyze/#introduction-to-ibm-cloud-transformation-advisor","text":"IBM Cloud Transformation Advisor helps you access, analyze and modernize middleware based apps into IBM Cloud(s). It categorizes Java EE apps and MQ queue managers as simple, medium and complex based on migration complexity and provides guidance for modernization. IBM Cloud Transformation Advisor can accelerate the process to move your on-premises apps to cloud, minimize migration errors and risks, and reduce time to market in five steps. You can use IBM Transformation Advisor for these activities: - Identify the Java EE programming models in an app - Determine the complexity of apps by reviewing a high-level inventory of the content and structure of each app - Highlight the Java EE programming model and WebSphere API differences between the profile types - Learn any Java EE specification implementation differences that might affect the app The tool also provides suggestions for the right-fit IBM WebSphere Application Server edition and offers advice, practices, and potential solutions to move apps to WebSphere Liberty or to newer versions of WebSphere Application Server.","title":"Introduction to IBM Cloud Transformation Advisor"},{"location":"liberty/liberty-analyze/#install-ibm-cloud-transformation-advisor","text":"IBM Cloud Transformation Advisor is split in to two components (the analysis engine and the data collector ). You can choose to between installing the analysis engine in to an IBM Cloud Private Cluster or locally on a machine with Docker. Installing IBM Cloud Transformation Advisor in to your IBM Cloud Private Cluster Installing IBM Cloud Transformation Advisor Beta Edition locally","title":"Install IBM Cloud Transformation Advisor"},{"location":"liberty/liberty-analyze/#download-the-data-collector","text":"Once IBM Cloud Transformation Advisor is installed, it is necessary to create a new Workspace and Collection and then download the Data Collector that will be used to examine the existing environment and applications. Open IBM Cloud Transformation Advisor in a browser and click the button to create a new Workspace Enter a Workspace name such as CloudPak_for_Applications and click Next Enter a Collection name such as WAS855_AppSrv01 and click Let's go When the No recommendations available page is displayed, click the Data Collector button When the Data Collector page is displayed, select the Source Operating System for your environment and click the Download button to download the Data Collector. This results in a file with a name similar to transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz being downloaded.","title":"Download the Data Collector"},{"location":"liberty/liberty-analyze/#run-the-data-collector","text":"Upload the Data Collector zip file that was downloaded from IBM Cloud Transformation Advisor in the previous step to the machine that the WebSphere ND Deployment Manager or the Standalone WebSphere Application Server is installed. The directory used arbitrary. Navigate to the directory you uploaded the transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz file to and issue the following commands to extract the Data Collector: bash mkdir datacollector cd datacollector mv transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz . tar -zxvf transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz cd transformationadvisor-* It is necessary to modify the scan performed by the Data Collector to include the org.pwte package as the Data Collector doesn't scan org.* packages by default. Open the conf/customCmd.properties file and modify it as shown below: bash evaluation=--evaluate --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp --includePackages=org.pwte migration_liberty=--analyze --sourceAppServer=was855 --targetAppServer=liberty --targetCloud=dockerIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp migration_was=--analyze --sourceAppServer=was855 --targetAppServer=was90 --targetCloud=vmIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp #inventory=--inventory --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #featureList=--featureList --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #java_opt=-Xmx2g The following command assumes that WebSphere Application Server v855 is installed to /opt/IBM/WebSphere/AppServer855 with a profile named AppSrv01 and that the administration user is wasadmin with a password of wasadmin . Modify and issue the following command as necessary to execute the Data Collector against the WebSphere environment: bash ./bin/transformationadvisor -w /opt/IBM/WebSphere/AppServer855 -p AppSrv01 wasadmin wasadmin When prompted, accept the license agreement . The Data Collection process will now start and will analyze all of the applications installed in the WebSphere Application Server environment and will also collect the related Java EE artifacts such as Data Sources and JMS definitions. When the analysis is complete, the Data Collector will attempt to upload the collection results to IBM Cloud Transformation Advisor. If this is successful, you can skip to the Analyze the Recommendations section. If not, you will receive an error at the end of Data Collection and will find a file named AppSrv01.zip in your current directory as shown below. bash ~/datacollector/transformationadvisor-1.9.6# ls -la *.zip -rw-r--r-- 1 root root 625493 Jun 12 12:58 AppSrv01.zip Download this Data Collector Results zip file ready for uploading to IBM Cloud Transformation Advisor in the next section","title":"Run the Data Collector"},{"location":"liberty/liberty-analyze/#upload-the-data-collector-results","text":"In this section the results from the Data Collector will be uploaded to IBM Cloud Transformation Advisor. In the IBM Cloud Transformation Advisor web browser session, click the Recomendations link in the top left corner and then click the Upload data button as shown below When the Upload data dialog is displayed, use the Drop or Add File button to select the Data Collector Results zip file that was downloaded in the previous section. Click Upload After a few moments the upload of the data collector results will be completed.","title":"Upload the Data Collector results"},{"location":"liberty/liberty-analyze/#analyze-the-recommendations","text":"Once the Data Collector Results have been uploaded to IBM Cloud Transformation Advisor a set of recommendations will be created and shown on the Recommendations page. In this section the recommendations will be analyzed and interpreted. The default recommendations are based on a target runtime of Liberty on Private Cloud . The Data Collector analyzed all of the applications running on the traditional WebSphere profile a displays a row in the chart for each application. In the case of the CustomerOrderServicesApp.ear application, IBM Cloud Transformation Advisor has determined that the migration to WebSphere Traditional on Private Cloud is of Moderate complexity and that there are two Severe Issues that have been detected. Click on the CustomerOrderServicesApp.ear application name to see more information. Review the analysis results and scroll down to the Technology Issues section. Note that IBM Cloud Transformation Advisor has detected that there are issues with lookups for Enterprise JavaBeans and with accessing the Apache Wink APIs. In order to review the IBM Cloud Transformation Advisor results in more detail, scroll to the bottom of the analysis page and click on the Analysis Report link When the warning dialog is displayed, click OK The Detailed Migration Analysis Report will be displayed which show the results of the migration rules that were executed by the Data Collector and returned results. Scroll down to the Severe Rules section and click on the Show rule help link for each of the results. Review the recommendations. Behavior change on lookups for Enterprise JavaBeans In Liberty, EJB components are not bound to a server root Java Naming and Directory Interface (JNDI) namespace as they are in WebSphere Application Server traditional. The fix for this is to change the three classes that use ejblocal to use the correct URL for Liberty The user of system provided Apache Wink APIs requires configuration To use system-provided third-party APIs in Liberty applications, you must configure the applications to include the APIs. In WebSphere Application Server traditional, these APIs are available without configuration. This is a configuration only change and can be achieved by using a classloader definition in the Liberty server.xml file.","title":"Analyze the Recommendations"},{"location":"liberty/liberty-analyze/#final-analysis","text":"The intention of this traditional WebSphere V855 --> Liberty (Private Cloud) scenario is to migrate the Customer Order Services application to the cloud-ready new runtime with minimal code changes. IBM Cloud Transformation Advisor was used to analyze the application for compatibility with traditional WebSphere V9 (Private Cloud) and determined that only small changes to three classes would be required. While this scenario will move the application to the cloud-ready WebSphere Liberty runtime in a container, it will not modernize the application architecture and code in any way. Now proceed to the Liberty - Build section where the process of making the code changes and configuring the Liberty runtime will be covered step-by-step","title":"Final Analysis"},{"location":"liberty/liberty-build/","text":"Liberty - Build This section covers how to make the minimal code changes required to allow the application to run on the Liberty runtime and how to create the configuration required for the Liberty runtime. The Liberty runtime and Docker image configuration will take in to account factors such as the requirement to inject environment specific configuration (such as JDBC Datasource User IDs and Passwords) at runtime and how to integrate with the Logging and Monitoring framework of the Cloud platform. The final versions of the files created in this section can be found in the liberty branch of this repo Summary This section has the following steps: Make minimal code changes to the application based on the analysis from IBM Cloud Transformation Advisor Create the WebSphere Liberty configuration file server.xml for the Customer Order Services application. This includes: listing the WebSphere Liberty runtime features that are required configuring a DB2 Data Source configuring a local user registry using environment variables in the server.xml file to allow configuration to be injected in to the runtime environment dynamically such as the DB2 user and password. Configuring WebSphere Liberty to expose internal metrics in a way that they can be collected by Prometheus Creating a Dockerfile and running the application in a Docker container locally Code changes In the previous section IBM Cloud Transformation Advisor highlighted a change in the way that Enterprise JavaBeans are looked up in Liberty. Behavior change on lookups for Enterprise JavaBeans In Liberty, EJB components are not bound to a server root Java Naming and Directory Interface (JNDI) namespace as they are in WebSphere Application Server traditional. The fix for this is to change the three classes that use ejblocal to use the correct URL for Liberty The three Java classes that should be modified to look up Enterprise JavaBeans differently are shown in the detailed analysis view of IBM Cloud Transformation Advisor (it isn't necessary to change the ibm-web-bnd.xml file in the Test project in this scenario) to be in the CustomerOrderServicesWeb project org.pwte.example.resources.CategoryResource.java is changed from using ejblocal on line 28 as shown below: ... InitialContext (). lookup ( \"ejblocal:org.pwte.example.service.ProductSearchService\" ); ... to use java:app as shown below: ... InitialContext (). lookup ( \"java:app/CustomerOrderServices/ProductSearchServiceImpl!org.pwte.example.service.ProductSearchService\" ); ... line 53 of org.pwte.example.resources.CustomerOrderResource.java is changed as shown below: ... ctx . lookup ( \"java:app/CustomerOrderServices/CustomerOrderServicesImpl!org.pwte.example.service.CustomerOrderServices\" ); ... line 38 of org.pwte.example.resources.ProductResource.java is changed as shown below: ... InitialContext (). lookup ( \"java:app/CustomerOrderServices/ProductSearchServiceImpl!org.pwte.example.service.ProductSearchService\" ); ... This completes all of the required code changes for the Customer Order Services application to run on the WebSphere Liberty runtime. Create the WebSphere Liberty server.xml file WebSphere Liberty is configured by exception. The runtime environment operates from a set of built-in configuration default settings, and you only need to specify configuration that overrides those default settings. You do this by editing either the server.xml file or another XML file that is included in server.xml at run time. The configuration has the following characteristics: Described in XML files. Human-readable, and editable in a text editor. Small, easy to back up, and easy to copy to another system. Shareable across an application development team. Composable, so that features can easily add their own configuration to the system. Extensibly-typed, so you don't have to modify the current configuration to work with later versions of the runtime environment. Dynamically responsive to updates. Forgiving, so that missing values are assumed and unrecognized properties are ignored. Features are the units of functionality by which you control the pieces of the runtime environment that are loaded into a particular server. They are the primary mechanism that makes the server composable. The list of features that you specify in the server configuration provides a functional server. See Liberty features for more information. In this section, the server.xml file to prepare the Liberty server to run the Customer Order Services application will be reviewed. The file can be found here . and is shown below: <server> <featureManager> <feature> appSecurity-2.0 </feature> <feature> ldapRegistry-3.0 </feature> <feature> localConnector-1.0 </feature> <feature> ejbLite-3.1 </feature> <feature> jaxrs-1.1 </feature> <feature> jdbc-4.1 </feature> <feature> jpa-2.0 </feature> <feature> jsp-2.3 </feature> <feature> servlet-3.1 </feature> <feature> monitor-1.0 </feature> </featureManager> <library id= \"DB2Lib\" > <fileset dir= \"/opt/ibm/wlp/usr/shared/resources/db2\" includes= \"db2jcc4.jar db2jcc_license_cu.jar\" /> </library> <dataSource id= \"OrderDS\" jndiName= \"jdbc/orderds\" type= \"javax.sql.XADataSource\" > <jdbcDriver libraryRef= \"DB2Lib\" /> <properties.db2.jcc databaseName= \"${env.DB2_DBNAME}\" password= \"${env.DB2_PASSWORD}\" portNumber= \"${env.DB2_PORT}\" serverName= \"${env.DB2_HOST}\" user= \"${env.DB2_USER}\" /> <connectionManager agedTimeout= \"0\" connectionTimeout= \"180\" maxIdleTime= \"1800\" maxPoolSize= \"10\" minPoolSize= \"1\" reapTime= \"180\" /> </dataSource> <httpEndpoint host= \"*\" httpPort= \"9080\" httpsPort= \"9443\" id= \"defaultHttpEndpoint\" > <tcpOptions soReuseAddr= \"true\" /> </httpEndpoint> <keyStore id= \"defaultKeyStore\" password= \"whodunit\" /> <!-- User and group security definitions --> <basicRegistry id= \"basic\" realm= \"customRealm\" > <user name= \"rbarcia\" password= \"bl0wfish\" /> <group name= \"SecureShopper\" > <member name= \"rbarcia\" /> </group> </basicRegistry> <applicationMonitor updateTrigger= \"mbean\" /> <application id= \"customerOrderServicesApp\" name= \"CustomerOrderServicesApp-0.1.0-SNAPSHOT.ear\" type= \"ear\" location= \"CustomerOrderServicesApp-0.1.0-SNAPSHOT.ear\" > <classloader apiTypeVisibility= \"spec, ibm-api, third-party\" /> </application> </server> The features section contains the following features: ejbLite-3.1 : the feature required to run simple Stateless Session Enterprise Java Beans jpa-2.0 , jdbc-4.1 : the features required for the JPA 2.0 runtime jaxrs-1-1 : the feature required for the JAX-RS 1.1 runtime servlet-3.1 , jsp-2.3 : the features required for Servlets and JSPs ldapRegistry-3.0 , appSecurity-2.0 : the features required to secure access to the application using LDAP or a local user registry monitor-1.0 : this feature enables the Performance Monitoring Infrastructure (PMI) for other features the server is running. Monitoring data is accessible through standard MXBeans. The library and dataSource sections configure a JDBC Driver for DB2 and a DataSource for the database used by the application. Note that some values have a {env.} prefix which is used to allow the values to be read from the environment instead of being hard-coded. This will be discussed in more detail in the Environment Specific Configuration section later. The basicRegistry section configures a local user registry with a single user ( rbarcia ) and a single group ( SecureShopper ). This is often used for development and testing when an LDAP server isn't available and is sufficient for this scenario. The applicationMonitor tag is used with Eclipse and triggers the application to be reloaded when a deployment from Eclipse occurs. The application tag describes the CustomerOrderServices EAR with an id , name and location . In this case the default location of the apps folder is used. The classloader tag is used to allow the application access to the Java Classes that make up the JEE specification ( spec ), some IBM provided APIs ( ibm-api ) and third-party classes such as apache-wink as these classes are hidden from the application by default. The classloader is configured becuase IBM Cloud Transformation Advisor highlighted that the application needed access to the Apach Wink APIs: The user of system provided Apache Wink APIs requires configuration To use system-provided third-party APIs in Liberty applications, you must configure the applications to include the APIs. In WebSphere Application Server traditional, these APIs are available without configuration. This is a configuration only change and can be achieved by using a classloader definition in the Liberty server.xml file. Environment Specific Configuration Applications deployed to an Application Server typically require access to backend services such as Databases, Message Queues, LDAP servers and other applications. Connection information for the backend services might include URL, Hostname, Port, UserID and Password and this information is often different depending on the environment (e.g. Dev, Test or Production) that the application is deployed to. In the case of the Customer Order Services application, a connection to a DB2 database is required. In order to take advantage of the portability provided by a platform such as Kubernetes and the speed of DevOps it is necessary to deploy applications in immutable containers using automation. In this context, immutable means that a container will not be modified during its life: no updates, no patches, no configuration changes. If you need to update the application code or apply a patch, you build a new image and deploy it. In order to achieve this, it is necessary to inject environment specific configuration in to the container in each environment (Database UserIDs and Passwords for example) instead of hard-coding the values in properties files in the container. The goal is to deploy the application in a portable, immutable container and in order to achieve that any environment specific configuration should be externalized from the container image and injected by the platform. In the Kubernetes world this is achieved using Environment Variables, ConfigMaps and Secrets. Using the env. prefex in the WebSphere Liberty server.xml forces the runtime to use the value of the environment variable when the server starts. See the documentation for more details. In the snippet below from the Customer Order Services application server.xml file, the host , portNumber , databaseName , user and password have all been externalized as environment variables which allows this application image to be moved between environments that have different databases without changing the image. <dataSource id= \"OrderDS\" jndiName= \"jdbc/orderds\" type= \"javax.sql.XADataSource\" > <jdbcDriver libraryRef= \"DB2Lib\" /> <properties.db2.jcc databaseName= \"${env.DB2_DBNAME}\" password= \"${env.DB2_PASSWORD}\" portNumber= \"${env.DB2_PORT}\" serverName= \"${env.DB2_HOST}\" user= \"${env.DB2_USER}\" /> <connectionManager agedTimeout= \"0\" connectionTimeout= \"180\" maxIdleTime= \"1800\" maxPoolSize= \"10\" minPoolSize= \"1\" reapTime= \"180\" /> </dataSource> In a native development environment (non-Docker) a server.env file can be used to inject the environment variables directly in to WebSphere Liberty when it starts. This avoids having to set environment variables on the development machine. This file is placed in the same folder as the server.xml file and is read by Liberty on startup. The file can be found here . and is shown below: DB2_HOST = 172 . 16 . 52 . 252 DB2_PORT = 50000 DB2_DBNAME = ORDERDB DB2_USER = db2inst1 DB2_PASSWORD = db2inst1 Monitoring Prometheus and Grafana are used by the IBM CloudPak for Application to monitor applications running in Kubernetes. Prometheus is a systems and service monitoring system. It collects metrics from configured targets at given intervals, evaluates rule expressions, displays the results, and can trigger alerts if some condition is observed to be true. Prometheus has several components for the collection of Time Series Data, an Alert Manager and a central Prometheus Server which scrapes and stores the data. The data is visualized using a Grafana instance. Kubernetes Monitoring Prometheus can monitor Kubernetes and the application pods. For Kubernetes data is related to the overall cluster state such as pod state (running, pending, error etc), cluster CPU usage and cluster RAM usage. Individual pod data is also available including CPU and RAM usage. This data is useful for an overall cluster view but it doesn't provide application specific data. Application Monitoring Prometheus can pull metrics data from applications using scraping . It is the responsibility of the application to make their metrics available on a /metrics endpoint which Prometheus scrapes and stores the data in its database for later visualization using Grafana. An example of the results from a /metrics scrape are shown below. Metrics related to connection pools, thread pools, CPU, memory and JVM heap usage are more useful for a Java application that the basic pod metrics available by default. base : classloader_current_loaded_class_count 16266 base : classloader_total_loaded_class_count 16372 base : classloader_total_unloaded_class_count 106 base : cpu_available_processors 8 base : cpu_process_cpu_load_percent 8.262355256251326 E - 4 base : gc_global_count 245 base : gc_global_time_seconds 13.756 base : gc_scavenge_count 9467 base : gc_scavenge_time_seconds 21.759 base : jvm_uptime_seconds 346146.925 base : memory_committed_heap_bytes 7.1892992E7 base : memory_max_heap_bytes 5.36870912E8 base : memory_used_heap_bytes 5.0641512E7 base : thread_count 77 base : thread_daemon_count 74 base : thread_max_count 91 vendor : connectionpool_jdbc_orderds_connection_handles 0 vendor : connectionpool_jdbc_orderds_create_total 7 vendor : connectionpool_jdbc_orderds_destroy_total 7 vendor : connectionpool_jdbc_orderds_free_connections 0 vendor : connectionpool_jdbc_orderds_in_use_time_total_seconds 0.71 vendor : connectionpool_jdbc_orderds_managed_connections 0 vendor : connectionpool_jdbc_orderds_queued_requests_total 0 vendor : connectionpool_jdbc_orderds_used_connections_total 7 vendor : connectionpool_jdbc_orderds_wait_time_total_seconds 0.0 vendor : servlet_com_ibm_ws_microprofile_metrics_public_public_metrics_rest_proxy_servlet_request_total 5771 vendor : servlet_com_ibm_ws_microprofile_metrics_public_public_metrics_rest_proxy_servlet_response_time_total_seconds 27.157690424000002 vendor : servlet_customer_order_services_app_org_pwte_example_app_customer_services_app_request_total 6 vendor : servlet_customer_order_services_app_org_pwte_example_app_customer_services_app_response_time_total_seconds 1.889112088 vendor : session_default_host_metrics_active_sessions 0 vendor : session_default_host_metrics_create_total 1 vendor : session_default_host_metrics_invalidated_total 1 vendor : session_default_host_metrics_invalidatedby_timeout_total 1 vendor : session_default_host_metrics_live_sessions 0 vendor : threadpool_default_executor_active_threads 1 vendor : threadpool_default_executor_size 24 Metrics scraping with WebSphere Liberty WebSphere Liberty provides a monitoring-1.0 feature which enables the Performance Monitoring Infrastructure (PMI) for other features the server is running and makes the monitoring data available via MXBeans. Prometheus cannot scrape MXBeans . WebSphere Liberty also provides a mpMetrics feature which takes the data from the MXBeans and exposes it on a /metrics endpoint that can be read by Prometheus. However, the mpMetrics feature is only available to applications using JavaEE7 or newer features. The Customer Order Services application uses the jpa-2.0 , jaxrs-1.1 and ejbLite-3.1 features which are older than JavaEE7 and is therefore unable to use the mpMetrics feature and can't expose a /metrics endpoint for Prometheus. Prometheus JMX exporter A solution to the problem that the Customer Order Services application has is to use Prometheus JMX exporter . The Prometheus JMX exporter connects to any MXBeans on a JVM, retrieves their data and exposes the results on a /metrics endpoint so that the data can be scraped by Prometheus. The JMX exporter is made up of a JAR file and a configuration file . The configuration file used for the Customer Order Services application is shown below. --- startDelaySeconds : 0 ssl : false lowercaseOutputName : false lowercaseOutputLabelNames : false In order to connect the JMX exporter and configuration file to Liberty on startup, a jvm.options file is required. The jvm.options file is shown below. - javaagent : / opt / ibm / wlp / usr / shared / resources / jmx_exporter / jmx_prometheus_javaagent - 0 . 11 . 0 . jar = 9081 : / opt / ibm / wlp / usr / shared / resources / jmx_exporter / jmx - config . yml An example of the dashboard that is now possible in Grafana is shown below. In this case the dashboard shows data about Servlets and Thread Pools. More details related to the metrics exposed by WebSphere Liberty and how to visualize them with Grafana will be covered in the Liberty - Deploy section Project Structure The git project for the Customer Order Services application already has the following structure: \u251c\u2500\u2500 CustomerOrderServices | \u251c\u2500\u2500 ejbModule | | \u2514\u2500\u2500 < source code > \u2502 \u2514\u2500\u2500 pom . xml \u251c\u2500\u2500 CustomerOrderServicesApp \u2502 \u251c\u2500\u2500 META - INF | | \u2514\u2500\u2500 < descriptor files > \u2502 \u2514\u2500\u2500 pom . xml \u251c\u2500\u2500 CustomerOrderServiceProject \u2502 \u2514\u2500\u2500 pom . xml \u251c\u2500\u2500 CustomerOrderServiceTest \u2502 \u251c\u2500\u2500 WebContent \u2502 \u251c\u2500\u2500 src | | \u2514\u2500\u2500 < source code > \u2502 \u2514\u2500\u2500 pom . xml \u2514\u2500\u2500 CustomerOrderServiceWeb \u251c\u2500\u2500 WebContent \u251c\u2500\u2500 src | \u2514\u2500\u2500 < source code > \u2514\u2500\u2500 pom . xml It is now necessary to add the WebSphere Liberty, DB2 and JMX Exporter files to the git project so that they can be pulled in to the Docker image during Docker build. The following folders and files have been added to the project: \u251c\u2500\u2500 resources | \u251c\u2500\u2500 db2 | | \u2514\u2500\u2500 db2jcc4 . jar | | \u2514\u2500\u2500 db2jcc_license_cu . jar \u2502 \u2514\u2500\u2500 jmx_exporter | | \u2514\u2500\u2500 jmx - config . yml | | \u2514\u2500\u2500 jmx_prometheus_javaagent - 0 . 11 . 0 . jar \u2514\u2500\u2500 liberty \u2514\u2500\u2500 server . xml \u2514\u2500\u2500 server . env \u2514\u2500\u2500 jvm . options Create the Docker Dockerfile Once the application code changes have been made and WebSphere Liberty configuration has been created the next step is to build a Docker image that contains the application and configuration. The build script to create a Docker image is a Dockerfile . The Dockerfile for the Customer Order Services application is shown below. The file can be found here FROM ibmcom / websphere - liberty : kernel - ubi - min COPY --chown=1001:0 ./liberty/server.xml /config COPY --chown=1001:0 ./liberty/jvm.options /config ARG SSL = false ARG MP_MONITORING = false ARG HTTP_ENDPOINT = false COPY --chown=1001:0 ./CustomerOrderServicesApp/target/CustomerOrderServicesApp-0.1.0-SNAPSHOT.ear /config/apps/CustomerOrderServicesApp-0.1.0-SNAPSHOT.ear COPY --chown=1001:0 ./resources/ /opt/ibm/wlp/usr/shared/resources/ USER 1001 RUN configure . sh IBM provides a set of WebSphere Liberty Docker images that are part of the IBM CloudPak for Applications. This scenario uses the Docker images that are based on the RedHat Universal Base Image . The ibmcom/websphere-liberty:kernel-ubi-min image contains only the smallest components of the Liberty server. The features used by the application are loaded in to the Docker image later in the RUN configure.sh step. The first two COPY commands copy the server.xml and jvm.options files to the /config folder on the WebSphere Liberty image. Later in the Dockerfile there are other COPY commands to copy the application that has been created by Maven to /config/apps and the DB2 drivers , JMX exporter JAR and JMX exporter configuration file to the shared resources folder for Liberty. Run the application locally Now that the application code changes have been made, the WebSphere Liberty configuration has been created and the Dockerfile is ready, the next step is to build a Docker image and run an instance locally to validate that the application runs correctly. Clone the GitHub repo and switch to the liberty branch using the following steps: git clone SOME_REPO cd cloudpak - for - applications git checkout liberty Use Maven to build the application cd CustomerOrderServicesProject mvn clean package The CustomerOrderServicesApp-0.1.0-SNAPSHOT.ear file should now be present in CustomerOrderServicesApp/target/ 3. Modify the liberty/server.env file for your environment as shown below: DB2_HOST = 172 .16.52.252 DB2_PORT = 50000 DB2_DBNAME = ORDERDB DB2_USER = db2inst1 DB2_PASSWORD = db2inst1 Build the Docker image using the following commands: cd .. / cloudpak - for - applications docker build - t customerorderservices - local : 1 . 0 . The end of the docker build command output should be similar to that shown below: ... All assets were successfully installed . Start product validation ... Product validation completed successfully . + sort - z + xargs - 0 - n 1 - r - I '{}' java - jar '{}' --installLocation /opt/ibm/wlp + find / opt / ibm / fixes - type f - name '*.jar' - print0 + xargs - 0 - r chmod - R g + rw + find / opt / ibm / wlp - perm - g = w - print0 + / opt / ibm / wlp / bin / server start Starting server defaultServer . Server defaultServer started with process ID 127 . + / opt / ibm / wlp / bin / server stop Stopping server defaultServer . Server defaultServer stopped . + rm - rf / output / resources / security / / output / messaging / logs / console . log / logs / messages . log / logs / messages_19 . 06 . 25 _16 . 12 . 03 . 0 . log / opt / ibm / wlp / output / . classCache + chmod - R g + rwx / opt / ibm / wlp / output / defaultServer + find / opt / ibm / wlp - type d - perm - g = x - print0 + xargs - 0 - r chmod - R g + rwx Removing intermediate container c08b8b001320 ---> 2897b25fa45e Successfully built 2897 b25fa45e Successfully tagged customerorderservices - local : 1 . 0 Run the newly created Docker image using the commands below: docker run --name customerorderservices-local -d -p 9081:9081 -p 9443:9443 -v server.env:/config/server.env customerorderservices-local:1.0 This command injects the server.env file in the correct location so that it is loaded by WebSphere Liberty on startup. 6. Check the logs for the WebSphere Liberty server using: docker logs customerorderservices - local The result should be that the server is started without error and the application is loaded: root @ gas - twas90 : ~/ djm / latest / cloudpak - for - applications / liberty # docker logs customerorderservices - local Launching defaultServer ( WebSphere Application Server 19 . 0 . 0 . 5 / wlp - 1 . 0 . 28 . cl190520190522 - 2227 ) on IBM J9 VM , version 8 . 0 . 5 . 36 - pxa6480sr5fp36 - 20190510 _01 ( SR5 FP36 ) ( en_US ) [ AUDIT ] CWWKE0001I : The server defaultServer has been launched . [ AUDIT ] CWWKE0100I : This product is licensed for development , and limited production use . The full license terms can be viewed here : https : // public . dhe . ibm . com / ibmdl / export / pub / software / websphere / wasdev / license / base_ilan / ilan / 19 . 0 . 0 . 5 / lafiles / en . html [ AUDIT ] CWWKG0093A : Processing configuration drop - ins resource : / opt / ibm / wlp / usr / servers / defaultServer / configDropins / defaults / keystore . xml [ AUDIT ] CWWKG0102I : Found conflicting settings for defaultKeyStore instance of keyStore configuration . Property password has conflicting values : Secure value is set in file : / opt / ibm / wlp / usr / servers / defaultServer / configDropins / defaults / keystore . xml . Secure value is set in file : / opt / ibm / wlp / usr / servers / defaultServer / server . xml . Property password will be set to the value defined in file : / opt / ibm / wlp / usr / servers / defaultServer / server . xml . [ AUDIT ] CWWKZ0058I : Monitoring dropins for applications . [ AUDIT ] CWWKS4104A : LTPA keys created in 1 . 573 seconds . LTPA key file : / opt / ibm / wlp / output / defaultServer / resources / security / ltpa . keys [ AUDIT ] CWPKI0803A : SSL certificate created in 5 . 334 seconds . SSL key file : / opt / ibm / wlp / output / defaultServer / resources / security / key . p12 [ AUDIT ] CWWKT0016I : Web application available ( default_host ) : http : // 57 edcfea336d : 9080 / CustomerOrderServicesWeb / [ AUDIT ] CWWKT0016I : Web application available ( default_host ) : http : // 57 edcfea336d : 9080 / CustomerOrderServicesTest / [ AUDIT ] CWWKZ0001I : Application CustomerOrderServicesApp - 0 . 1 . 0 - SNAPSHOT . ear started in 2 . 837 seconds . [ AUDIT ] CWWKF0012I : The server installed the following features : [ appSecurity - 2 . 0 , beanValidation - 1 . 0 , distributedMap - 1 . 0 , ejbLite - 3 . 1 , el - 3 . 0 , federatedRegistry - 1 . 0 , jaxrs - 1 . 1 , jdbc - 4 . 1 , jndi - 1 . 0 , jpa - 2 . 0 , json - 1 . 0 , jsp - 2 . 3 , ldapRegistry - 3 . 0 , localConnector - 1 . 0 , monitor - 1 . 0 , servlet - 3 . 1 , ssl - 1 . 0 ]. [ AUDIT ] CWWKF0011I : The defaultServer server is ready to run a smarter planet . The defaultServer server started in 12 . 838 seconds . Access the application in a browser using https://127.0.0.1:9443/CustomerOrderServicesWeb . Login using rbarcia and bl0wfish and then add an item to the shopping cart Validate that the /metrics endpoint is available at http://127.0.0.1:9081/metrics Stop the Docker container using the commands below: docker stop customerorderservices-local Review and Next Steps The intention of this traditional WebSphere V855 --> Liberty (Private Cloud) scenario is to migrate the Customer Order Services application to the cloud-ready new runtime with minimal code changes. In this section you have moved the application to WebSphere Liberty and tested it locally in a Docker container. Now proceed to the Liberty - Deploy section where the process of automating the deployment of the application to a Kubernetes runtime will be covered step-by-step","title":"Liberty - Build"},{"location":"liberty/liberty-build/#liberty-build","text":"This section covers how to make the minimal code changes required to allow the application to run on the Liberty runtime and how to create the configuration required for the Liberty runtime. The Liberty runtime and Docker image configuration will take in to account factors such as the requirement to inject environment specific configuration (such as JDBC Datasource User IDs and Passwords) at runtime and how to integrate with the Logging and Monitoring framework of the Cloud platform. The final versions of the files created in this section can be found in the liberty branch of this repo","title":"Liberty - Build"},{"location":"liberty/liberty-build/#summary","text":"This section has the following steps: Make minimal code changes to the application based on the analysis from IBM Cloud Transformation Advisor Create the WebSphere Liberty configuration file server.xml for the Customer Order Services application. This includes: listing the WebSphere Liberty runtime features that are required configuring a DB2 Data Source configuring a local user registry using environment variables in the server.xml file to allow configuration to be injected in to the runtime environment dynamically such as the DB2 user and password. Configuring WebSphere Liberty to expose internal metrics in a way that they can be collected by Prometheus Creating a Dockerfile and running the application in a Docker container locally","title":"Summary"},{"location":"liberty/liberty-build/#code-changes","text":"In the previous section IBM Cloud Transformation Advisor highlighted a change in the way that Enterprise JavaBeans are looked up in Liberty. Behavior change on lookups for Enterprise JavaBeans In Liberty, EJB components are not bound to a server root Java Naming and Directory Interface (JNDI) namespace as they are in WebSphere Application Server traditional. The fix for this is to change the three classes that use ejblocal to use the correct URL for Liberty The three Java classes that should be modified to look up Enterprise JavaBeans differently are shown in the detailed analysis view of IBM Cloud Transformation Advisor (it isn't necessary to change the ibm-web-bnd.xml file in the Test project in this scenario) to be in the CustomerOrderServicesWeb project org.pwte.example.resources.CategoryResource.java is changed from using ejblocal on line 28 as shown below: ... InitialContext (). lookup ( \"ejblocal:org.pwte.example.service.ProductSearchService\" ); ... to use java:app as shown below: ... InitialContext (). lookup ( \"java:app/CustomerOrderServices/ProductSearchServiceImpl!org.pwte.example.service.ProductSearchService\" ); ... line 53 of org.pwte.example.resources.CustomerOrderResource.java is changed as shown below: ... ctx . lookup ( \"java:app/CustomerOrderServices/CustomerOrderServicesImpl!org.pwte.example.service.CustomerOrderServices\" ); ... line 38 of org.pwte.example.resources.ProductResource.java is changed as shown below: ... InitialContext (). lookup ( \"java:app/CustomerOrderServices/ProductSearchServiceImpl!org.pwte.example.service.ProductSearchService\" ); ... This completes all of the required code changes for the Customer Order Services application to run on the WebSphere Liberty runtime.","title":"Code changes"},{"location":"liberty/liberty-build/#create-the-websphere-liberty-serverxml-file","text":"WebSphere Liberty is configured by exception. The runtime environment operates from a set of built-in configuration default settings, and you only need to specify configuration that overrides those default settings. You do this by editing either the server.xml file or another XML file that is included in server.xml at run time. The configuration has the following characteristics: Described in XML files. Human-readable, and editable in a text editor. Small, easy to back up, and easy to copy to another system. Shareable across an application development team. Composable, so that features can easily add their own configuration to the system. Extensibly-typed, so you don't have to modify the current configuration to work with later versions of the runtime environment. Dynamically responsive to updates. Forgiving, so that missing values are assumed and unrecognized properties are ignored. Features are the units of functionality by which you control the pieces of the runtime environment that are loaded into a particular server. They are the primary mechanism that makes the server composable. The list of features that you specify in the server configuration provides a functional server. See Liberty features for more information. In this section, the server.xml file to prepare the Liberty server to run the Customer Order Services application will be reviewed. The file can be found here . and is shown below: <server> <featureManager> <feature> appSecurity-2.0 </feature> <feature> ldapRegistry-3.0 </feature> <feature> localConnector-1.0 </feature> <feature> ejbLite-3.1 </feature> <feature> jaxrs-1.1 </feature> <feature> jdbc-4.1 </feature> <feature> jpa-2.0 </feature> <feature> jsp-2.3 </feature> <feature> servlet-3.1 </feature> <feature> monitor-1.0 </feature> </featureManager> <library id= \"DB2Lib\" > <fileset dir= \"/opt/ibm/wlp/usr/shared/resources/db2\" includes= \"db2jcc4.jar db2jcc_license_cu.jar\" /> </library> <dataSource id= \"OrderDS\" jndiName= \"jdbc/orderds\" type= \"javax.sql.XADataSource\" > <jdbcDriver libraryRef= \"DB2Lib\" /> <properties.db2.jcc databaseName= \"${env.DB2_DBNAME}\" password= \"${env.DB2_PASSWORD}\" portNumber= \"${env.DB2_PORT}\" serverName= \"${env.DB2_HOST}\" user= \"${env.DB2_USER}\" /> <connectionManager agedTimeout= \"0\" connectionTimeout= \"180\" maxIdleTime= \"1800\" maxPoolSize= \"10\" minPoolSize= \"1\" reapTime= \"180\" /> </dataSource> <httpEndpoint host= \"*\" httpPort= \"9080\" httpsPort= \"9443\" id= \"defaultHttpEndpoint\" > <tcpOptions soReuseAddr= \"true\" /> </httpEndpoint> <keyStore id= \"defaultKeyStore\" password= \"whodunit\" /> <!-- User and group security definitions --> <basicRegistry id= \"basic\" realm= \"customRealm\" > <user name= \"rbarcia\" password= \"bl0wfish\" /> <group name= \"SecureShopper\" > <member name= \"rbarcia\" /> </group> </basicRegistry> <applicationMonitor updateTrigger= \"mbean\" /> <application id= \"customerOrderServicesApp\" name= \"CustomerOrderServicesApp-0.1.0-SNAPSHOT.ear\" type= \"ear\" location= \"CustomerOrderServicesApp-0.1.0-SNAPSHOT.ear\" > <classloader apiTypeVisibility= \"spec, ibm-api, third-party\" /> </application> </server> The features section contains the following features: ejbLite-3.1 : the feature required to run simple Stateless Session Enterprise Java Beans jpa-2.0 , jdbc-4.1 : the features required for the JPA 2.0 runtime jaxrs-1-1 : the feature required for the JAX-RS 1.1 runtime servlet-3.1 , jsp-2.3 : the features required for Servlets and JSPs ldapRegistry-3.0 , appSecurity-2.0 : the features required to secure access to the application using LDAP or a local user registry monitor-1.0 : this feature enables the Performance Monitoring Infrastructure (PMI) for other features the server is running. Monitoring data is accessible through standard MXBeans. The library and dataSource sections configure a JDBC Driver for DB2 and a DataSource for the database used by the application. Note that some values have a {env.} prefix which is used to allow the values to be read from the environment instead of being hard-coded. This will be discussed in more detail in the Environment Specific Configuration section later. The basicRegistry section configures a local user registry with a single user ( rbarcia ) and a single group ( SecureShopper ). This is often used for development and testing when an LDAP server isn't available and is sufficient for this scenario. The applicationMonitor tag is used with Eclipse and triggers the application to be reloaded when a deployment from Eclipse occurs. The application tag describes the CustomerOrderServices EAR with an id , name and location . In this case the default location of the apps folder is used. The classloader tag is used to allow the application access to the Java Classes that make up the JEE specification ( spec ), some IBM provided APIs ( ibm-api ) and third-party classes such as apache-wink as these classes are hidden from the application by default. The classloader is configured becuase IBM Cloud Transformation Advisor highlighted that the application needed access to the Apach Wink APIs: The user of system provided Apache Wink APIs requires configuration To use system-provided third-party APIs in Liberty applications, you must configure the applications to include the APIs. In WebSphere Application Server traditional, these APIs are available without configuration. This is a configuration only change and can be achieved by using a classloader definition in the Liberty server.xml file.","title":"Create the WebSphere Liberty server.xml file"},{"location":"liberty/liberty-build/#environment-specific-configuration","text":"Applications deployed to an Application Server typically require access to backend services such as Databases, Message Queues, LDAP servers and other applications. Connection information for the backend services might include URL, Hostname, Port, UserID and Password and this information is often different depending on the environment (e.g. Dev, Test or Production) that the application is deployed to. In the case of the Customer Order Services application, a connection to a DB2 database is required. In order to take advantage of the portability provided by a platform such as Kubernetes and the speed of DevOps it is necessary to deploy applications in immutable containers using automation. In this context, immutable means that a container will not be modified during its life: no updates, no patches, no configuration changes. If you need to update the application code or apply a patch, you build a new image and deploy it. In order to achieve this, it is necessary to inject environment specific configuration in to the container in each environment (Database UserIDs and Passwords for example) instead of hard-coding the values in properties files in the container. The goal is to deploy the application in a portable, immutable container and in order to achieve that any environment specific configuration should be externalized from the container image and injected by the platform. In the Kubernetes world this is achieved using Environment Variables, ConfigMaps and Secrets. Using the env. prefex in the WebSphere Liberty server.xml forces the runtime to use the value of the environment variable when the server starts. See the documentation for more details. In the snippet below from the Customer Order Services application server.xml file, the host , portNumber , databaseName , user and password have all been externalized as environment variables which allows this application image to be moved between environments that have different databases without changing the image. <dataSource id= \"OrderDS\" jndiName= \"jdbc/orderds\" type= \"javax.sql.XADataSource\" > <jdbcDriver libraryRef= \"DB2Lib\" /> <properties.db2.jcc databaseName= \"${env.DB2_DBNAME}\" password= \"${env.DB2_PASSWORD}\" portNumber= \"${env.DB2_PORT}\" serverName= \"${env.DB2_HOST}\" user= \"${env.DB2_USER}\" /> <connectionManager agedTimeout= \"0\" connectionTimeout= \"180\" maxIdleTime= \"1800\" maxPoolSize= \"10\" minPoolSize= \"1\" reapTime= \"180\" /> </dataSource> In a native development environment (non-Docker) a server.env file can be used to inject the environment variables directly in to WebSphere Liberty when it starts. This avoids having to set environment variables on the development machine. This file is placed in the same folder as the server.xml file and is read by Liberty on startup. The file can be found here . and is shown below: DB2_HOST = 172 . 16 . 52 . 252 DB2_PORT = 50000 DB2_DBNAME = ORDERDB DB2_USER = db2inst1 DB2_PASSWORD = db2inst1","title":"Environment Specific Configuration"},{"location":"liberty/liberty-build/#monitoring","text":"Prometheus and Grafana are used by the IBM CloudPak for Application to monitor applications running in Kubernetes. Prometheus is a systems and service monitoring system. It collects metrics from configured targets at given intervals, evaluates rule expressions, displays the results, and can trigger alerts if some condition is observed to be true. Prometheus has several components for the collection of Time Series Data, an Alert Manager and a central Prometheus Server which scrapes and stores the data. The data is visualized using a Grafana instance.","title":"Monitoring"},{"location":"liberty/liberty-build/#kubernetes-monitoring","text":"Prometheus can monitor Kubernetes and the application pods. For Kubernetes data is related to the overall cluster state such as pod state (running, pending, error etc), cluster CPU usage and cluster RAM usage. Individual pod data is also available including CPU and RAM usage. This data is useful for an overall cluster view but it doesn't provide application specific data.","title":"Kubernetes Monitoring"},{"location":"liberty/liberty-build/#application-monitoring","text":"Prometheus can pull metrics data from applications using scraping . It is the responsibility of the application to make their metrics available on a /metrics endpoint which Prometheus scrapes and stores the data in its database for later visualization using Grafana. An example of the results from a /metrics scrape are shown below. Metrics related to connection pools, thread pools, CPU, memory and JVM heap usage are more useful for a Java application that the basic pod metrics available by default. base : classloader_current_loaded_class_count 16266 base : classloader_total_loaded_class_count 16372 base : classloader_total_unloaded_class_count 106 base : cpu_available_processors 8 base : cpu_process_cpu_load_percent 8.262355256251326 E - 4 base : gc_global_count 245 base : gc_global_time_seconds 13.756 base : gc_scavenge_count 9467 base : gc_scavenge_time_seconds 21.759 base : jvm_uptime_seconds 346146.925 base : memory_committed_heap_bytes 7.1892992E7 base : memory_max_heap_bytes 5.36870912E8 base : memory_used_heap_bytes 5.0641512E7 base : thread_count 77 base : thread_daemon_count 74 base : thread_max_count 91 vendor : connectionpool_jdbc_orderds_connection_handles 0 vendor : connectionpool_jdbc_orderds_create_total 7 vendor : connectionpool_jdbc_orderds_destroy_total 7 vendor : connectionpool_jdbc_orderds_free_connections 0 vendor : connectionpool_jdbc_orderds_in_use_time_total_seconds 0.71 vendor : connectionpool_jdbc_orderds_managed_connections 0 vendor : connectionpool_jdbc_orderds_queued_requests_total 0 vendor : connectionpool_jdbc_orderds_used_connections_total 7 vendor : connectionpool_jdbc_orderds_wait_time_total_seconds 0.0 vendor : servlet_com_ibm_ws_microprofile_metrics_public_public_metrics_rest_proxy_servlet_request_total 5771 vendor : servlet_com_ibm_ws_microprofile_metrics_public_public_metrics_rest_proxy_servlet_response_time_total_seconds 27.157690424000002 vendor : servlet_customer_order_services_app_org_pwte_example_app_customer_services_app_request_total 6 vendor : servlet_customer_order_services_app_org_pwte_example_app_customer_services_app_response_time_total_seconds 1.889112088 vendor : session_default_host_metrics_active_sessions 0 vendor : session_default_host_metrics_create_total 1 vendor : session_default_host_metrics_invalidated_total 1 vendor : session_default_host_metrics_invalidatedby_timeout_total 1 vendor : session_default_host_metrics_live_sessions 0 vendor : threadpool_default_executor_active_threads 1 vendor : threadpool_default_executor_size 24","title":"Application Monitoring"},{"location":"liberty/liberty-build/#metrics-scraping-with-websphere-liberty","text":"WebSphere Liberty provides a monitoring-1.0 feature which enables the Performance Monitoring Infrastructure (PMI) for other features the server is running and makes the monitoring data available via MXBeans. Prometheus cannot scrape MXBeans . WebSphere Liberty also provides a mpMetrics feature which takes the data from the MXBeans and exposes it on a /metrics endpoint that can be read by Prometheus. However, the mpMetrics feature is only available to applications using JavaEE7 or newer features. The Customer Order Services application uses the jpa-2.0 , jaxrs-1.1 and ejbLite-3.1 features which are older than JavaEE7 and is therefore unable to use the mpMetrics feature and can't expose a /metrics endpoint for Prometheus.","title":"Metrics scraping with WebSphere Liberty"},{"location":"liberty/liberty-build/#prometheus-jmx-exporter","text":"A solution to the problem that the Customer Order Services application has is to use Prometheus JMX exporter . The Prometheus JMX exporter connects to any MXBeans on a JVM, retrieves their data and exposes the results on a /metrics endpoint so that the data can be scraped by Prometheus. The JMX exporter is made up of a JAR file and a configuration file . The configuration file used for the Customer Order Services application is shown below. --- startDelaySeconds : 0 ssl : false lowercaseOutputName : false lowercaseOutputLabelNames : false In order to connect the JMX exporter and configuration file to Liberty on startup, a jvm.options file is required. The jvm.options file is shown below. - javaagent : / opt / ibm / wlp / usr / shared / resources / jmx_exporter / jmx_prometheus_javaagent - 0 . 11 . 0 . jar = 9081 : / opt / ibm / wlp / usr / shared / resources / jmx_exporter / jmx - config . yml An example of the dashboard that is now possible in Grafana is shown below. In this case the dashboard shows data about Servlets and Thread Pools. More details related to the metrics exposed by WebSphere Liberty and how to visualize them with Grafana will be covered in the Liberty - Deploy section","title":"Prometheus JMX exporter"},{"location":"liberty/liberty-build/#project-structure","text":"The git project for the Customer Order Services application already has the following structure: \u251c\u2500\u2500 CustomerOrderServices | \u251c\u2500\u2500 ejbModule | | \u2514\u2500\u2500 < source code > \u2502 \u2514\u2500\u2500 pom . xml \u251c\u2500\u2500 CustomerOrderServicesApp \u2502 \u251c\u2500\u2500 META - INF | | \u2514\u2500\u2500 < descriptor files > \u2502 \u2514\u2500\u2500 pom . xml \u251c\u2500\u2500 CustomerOrderServiceProject \u2502 \u2514\u2500\u2500 pom . xml \u251c\u2500\u2500 CustomerOrderServiceTest \u2502 \u251c\u2500\u2500 WebContent \u2502 \u251c\u2500\u2500 src | | \u2514\u2500\u2500 < source code > \u2502 \u2514\u2500\u2500 pom . xml \u2514\u2500\u2500 CustomerOrderServiceWeb \u251c\u2500\u2500 WebContent \u251c\u2500\u2500 src | \u2514\u2500\u2500 < source code > \u2514\u2500\u2500 pom . xml It is now necessary to add the WebSphere Liberty, DB2 and JMX Exporter files to the git project so that they can be pulled in to the Docker image during Docker build. The following folders and files have been added to the project: \u251c\u2500\u2500 resources | \u251c\u2500\u2500 db2 | | \u2514\u2500\u2500 db2jcc4 . jar | | \u2514\u2500\u2500 db2jcc_license_cu . jar \u2502 \u2514\u2500\u2500 jmx_exporter | | \u2514\u2500\u2500 jmx - config . yml | | \u2514\u2500\u2500 jmx_prometheus_javaagent - 0 . 11 . 0 . jar \u2514\u2500\u2500 liberty \u2514\u2500\u2500 server . xml \u2514\u2500\u2500 server . env \u2514\u2500\u2500 jvm . options","title":"Project Structure"},{"location":"liberty/liberty-build/#create-the-docker-dockerfile","text":"Once the application code changes have been made and WebSphere Liberty configuration has been created the next step is to build a Docker image that contains the application and configuration. The build script to create a Docker image is a Dockerfile . The Dockerfile for the Customer Order Services application is shown below. The file can be found here FROM ibmcom / websphere - liberty : kernel - ubi - min COPY --chown=1001:0 ./liberty/server.xml /config COPY --chown=1001:0 ./liberty/jvm.options /config ARG SSL = false ARG MP_MONITORING = false ARG HTTP_ENDPOINT = false COPY --chown=1001:0 ./CustomerOrderServicesApp/target/CustomerOrderServicesApp-0.1.0-SNAPSHOT.ear /config/apps/CustomerOrderServicesApp-0.1.0-SNAPSHOT.ear COPY --chown=1001:0 ./resources/ /opt/ibm/wlp/usr/shared/resources/ USER 1001 RUN configure . sh IBM provides a set of WebSphere Liberty Docker images that are part of the IBM CloudPak for Applications. This scenario uses the Docker images that are based on the RedHat Universal Base Image . The ibmcom/websphere-liberty:kernel-ubi-min image contains only the smallest components of the Liberty server. The features used by the application are loaded in to the Docker image later in the RUN configure.sh step. The first two COPY commands copy the server.xml and jvm.options files to the /config folder on the WebSphere Liberty image. Later in the Dockerfile there are other COPY commands to copy the application that has been created by Maven to /config/apps and the DB2 drivers , JMX exporter JAR and JMX exporter configuration file to the shared resources folder for Liberty.","title":"Create the Docker Dockerfile"},{"location":"liberty/liberty-build/#run-the-application-locally","text":"Now that the application code changes have been made, the WebSphere Liberty configuration has been created and the Dockerfile is ready, the next step is to build a Docker image and run an instance locally to validate that the application runs correctly. Clone the GitHub repo and switch to the liberty branch using the following steps: git clone SOME_REPO cd cloudpak - for - applications git checkout liberty Use Maven to build the application cd CustomerOrderServicesProject mvn clean package The CustomerOrderServicesApp-0.1.0-SNAPSHOT.ear file should now be present in CustomerOrderServicesApp/target/ 3. Modify the liberty/server.env file for your environment as shown below: DB2_HOST = 172 .16.52.252 DB2_PORT = 50000 DB2_DBNAME = ORDERDB DB2_USER = db2inst1 DB2_PASSWORD = db2inst1 Build the Docker image using the following commands: cd .. / cloudpak - for - applications docker build - t customerorderservices - local : 1 . 0 . The end of the docker build command output should be similar to that shown below: ... All assets were successfully installed . Start product validation ... Product validation completed successfully . + sort - z + xargs - 0 - n 1 - r - I '{}' java - jar '{}' --installLocation /opt/ibm/wlp + find / opt / ibm / fixes - type f - name '*.jar' - print0 + xargs - 0 - r chmod - R g + rw + find / opt / ibm / wlp - perm - g = w - print0 + / opt / ibm / wlp / bin / server start Starting server defaultServer . Server defaultServer started with process ID 127 . + / opt / ibm / wlp / bin / server stop Stopping server defaultServer . Server defaultServer stopped . + rm - rf / output / resources / security / / output / messaging / logs / console . log / logs / messages . log / logs / messages_19 . 06 . 25 _16 . 12 . 03 . 0 . log / opt / ibm / wlp / output / . classCache + chmod - R g + rwx / opt / ibm / wlp / output / defaultServer + find / opt / ibm / wlp - type d - perm - g = x - print0 + xargs - 0 - r chmod - R g + rwx Removing intermediate container c08b8b001320 ---> 2897b25fa45e Successfully built 2897 b25fa45e Successfully tagged customerorderservices - local : 1 . 0 Run the newly created Docker image using the commands below: docker run --name customerorderservices-local -d -p 9081:9081 -p 9443:9443 -v server.env:/config/server.env customerorderservices-local:1.0 This command injects the server.env file in the correct location so that it is loaded by WebSphere Liberty on startup. 6. Check the logs for the WebSphere Liberty server using: docker logs customerorderservices - local The result should be that the server is started without error and the application is loaded: root @ gas - twas90 : ~/ djm / latest / cloudpak - for - applications / liberty # docker logs customerorderservices - local Launching defaultServer ( WebSphere Application Server 19 . 0 . 0 . 5 / wlp - 1 . 0 . 28 . cl190520190522 - 2227 ) on IBM J9 VM , version 8 . 0 . 5 . 36 - pxa6480sr5fp36 - 20190510 _01 ( SR5 FP36 ) ( en_US ) [ AUDIT ] CWWKE0001I : The server defaultServer has been launched . [ AUDIT ] CWWKE0100I : This product is licensed for development , and limited production use . The full license terms can be viewed here : https : // public . dhe . ibm . com / ibmdl / export / pub / software / websphere / wasdev / license / base_ilan / ilan / 19 . 0 . 0 . 5 / lafiles / en . html [ AUDIT ] CWWKG0093A : Processing configuration drop - ins resource : / opt / ibm / wlp / usr / servers / defaultServer / configDropins / defaults / keystore . xml [ AUDIT ] CWWKG0102I : Found conflicting settings for defaultKeyStore instance of keyStore configuration . Property password has conflicting values : Secure value is set in file : / opt / ibm / wlp / usr / servers / defaultServer / configDropins / defaults / keystore . xml . Secure value is set in file : / opt / ibm / wlp / usr / servers / defaultServer / server . xml . Property password will be set to the value defined in file : / opt / ibm / wlp / usr / servers / defaultServer / server . xml . [ AUDIT ] CWWKZ0058I : Monitoring dropins for applications . [ AUDIT ] CWWKS4104A : LTPA keys created in 1 . 573 seconds . LTPA key file : / opt / ibm / wlp / output / defaultServer / resources / security / ltpa . keys [ AUDIT ] CWPKI0803A : SSL certificate created in 5 . 334 seconds . SSL key file : / opt / ibm / wlp / output / defaultServer / resources / security / key . p12 [ AUDIT ] CWWKT0016I : Web application available ( default_host ) : http : // 57 edcfea336d : 9080 / CustomerOrderServicesWeb / [ AUDIT ] CWWKT0016I : Web application available ( default_host ) : http : // 57 edcfea336d : 9080 / CustomerOrderServicesTest / [ AUDIT ] CWWKZ0001I : Application CustomerOrderServicesApp - 0 . 1 . 0 - SNAPSHOT . ear started in 2 . 837 seconds . [ AUDIT ] CWWKF0012I : The server installed the following features : [ appSecurity - 2 . 0 , beanValidation - 1 . 0 , distributedMap - 1 . 0 , ejbLite - 3 . 1 , el - 3 . 0 , federatedRegistry - 1 . 0 , jaxrs - 1 . 1 , jdbc - 4 . 1 , jndi - 1 . 0 , jpa - 2 . 0 , json - 1 . 0 , jsp - 2 . 3 , ldapRegistry - 3 . 0 , localConnector - 1 . 0 , monitor - 1 . 0 , servlet - 3 . 1 , ssl - 1 . 0 ]. [ AUDIT ] CWWKF0011I : The defaultServer server is ready to run a smarter planet . The defaultServer server started in 12 . 838 seconds . Access the application in a browser using https://127.0.0.1:9443/CustomerOrderServicesWeb . Login using rbarcia and bl0wfish and then add an item to the shopping cart Validate that the /metrics endpoint is available at http://127.0.0.1:9081/metrics Stop the Docker container using the commands below: docker stop customerorderservices-local","title":"Run the application locally"},{"location":"liberty/liberty-build/#review-and-next-steps","text":"The intention of this traditional WebSphere V855 --> Liberty (Private Cloud) scenario is to migrate the Customer Order Services application to the cloud-ready new runtime with minimal code changes. In this section you have moved the application to WebSphere Liberty and tested it locally in a Docker container. Now proceed to the Liberty - Deploy section where the process of automating the deployment of the application to a Kubernetes runtime will be covered step-by-step","title":"Review and Next Steps"},{"location":"liberty/liberty-deploy/","text":"Liberty - Deploy This section covers how to deploy the application to RedHat OpenShift using an automated CI/CD pipeline. The diagram below shows the flow of the pipeline which starts when the developer checks their code in to Git and ends with the application being deployed in Production. the developer checks their code in to git a webhook automatically triggers the Jenkins Pipeline in the RHOS cluster the pipeline checks out the code from git and uses maven to build and test the application the oc start build command is used to build a docker image for the application the image is added to the ImageStream and pushed to the docker registry in the RHOS cluster the image is then tagged for the dev project the deployment running in the dev project is restarted using the newly created image the image is then tagged for the stage project the deployment running in the stage project is restarted using the newly created image the developer is prompted to approve the deployment to production the image is then tagged for the prod project the deployment running in the prod project is restarted using the newly created image The final versions of the files created in this section can be found in the liberty branch of this repo Summary This section has the following steps: RedHat OpenShift terminology and constructs The Jenkinsfile for the pipeline Create the Security Context Constraint required for WebSphere to run on RedHat OpenShift Create the build , dev , stage and prod projects Create a new Service Account and bind it to the Security Context Constraint in the dev , stage and prod projects Deploy Jenkins in the build namespace Grant the jenkins service account edit privileges in the dev , stage and prod projects Import the deployment template in to the dev , stage and prod projects Create the deployment config , service and route definitions using the template in the dev , stage and prod projects Import the build template in to the build project Create the ImageStream and BuildConfig definitions in the build project Trigger the pipeline and validate the application is running RedHat OpenShift constructs It is assumed that the reader is familiar with the following basic constructs provided by Kubernetes: Pod Deployment ServiceAccount Service RedHat OpenShift has the following constructs that are used in this scenario: - Route - ImageStream - BuildConfig - DeploymentConfig - SecurityContextConstraints - Application - Template The Jenkinsfile for the pipeline A Jenkinsfile contains the definition of a pipeline in a format that can be stored in source control. In addition to standard Jenkins Pipeline Syntax, the OpenShift provides a Domain Specific Language (DSL) through the OpenShift Jenkins Client Plug-in. OpenShift DSL is easily readable that interacts with OpenShift API server, giving more control over the build, deployment, and promotion of applications on OpenShift cluster. The Jenkinsfile for this application can be found here The Jenkinsfile has the following stages : preamble : outputs some variable valus to the console Maven Build : builds the application EAR file using maven Unit Test : tests the application using maven Build Liberty App Image : uses the BuildConfig to build the Docker Image using the provided Dockerfile . This automatically tags the image in the build Project's ImageStream Promote to Dev : tags the image to the dev Project's ImageStream which results in the deployment in the dev Project being restarted with the new image Promote to Stage : tags the image to the stage Project's ImageStream which results in the deployment in the stage Project being restarted with the new image Promotion gate : prompts the user to approve promotion to production Promote to Prod : the image to the prod Project's ImageStream which results in the deployment in the prod Project being restarted with the new image Create the Security Context Constraint In order to deploy and run the WebSphere Liberty Docker image in an OpenShift cluster, we first need to configure certain security aspects for the cluster. The Security Context Constraint provided here grants the service account that the WebSphere Liberty Docker container is running under the required privileges to function correctly. A cluster administrator can use the file provided here with the following command to create the Security Context Constraint (SCC): oc apply - f ssc . yaml Create the projects Four RedHat OpenShift projects are required in this scenario: - Build: this project will contain the Jenkins server and the artifacts used to build the application image - Dev: this is the development environment for this application - Stage: this is the staging environment for this application - Prod: this is the production environment for this application The file provided here contains the definitions for the four projects in a single file to make creation easier Issue the command shown below to create the projects oc create - f liberty - projects . yaml Create a service account It is a good Kubernetes practice to create a service account for your applications. A service account provides an identity for processes that run in a Pod. In this step we will create a new service account with the name websphere in each of the dev , stage and prod projects and add the Security Context Constraint created above to them. Issue the commands shown below to create the websphere service account and bind the ibm-websphere-scc to it in each of the projects: oc create serviceaccount websphere - n cos - liberty - dev oc create serviceaccount websphere - n cos - liberty - stage oc create serviceaccount websphere - n cos - liberty - prod oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - liberty - dev oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - liberty - stage oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - liberty - prod Deploy Jenkins Some RedHat OpenShift clusters are configured to automatically provision a Jenkins instance in a build project. The steps below can be used if your cluster is not configured for automatic Jenkins provisioning: oc project cos - liberty - build oc new - app jenkins - persistent Update the Jenkins service account During provisioning of the Jenkins master a service account with the name jenkins is created. This service account has privileges to create new artifacts only in the project that it is running in. In this scenario Jenkins will need to create artifacts in the dev , stage and prod projects. Issue the commands below to allow the jenkins service account to edit artifacts in the dev , stage and prod projects. oc policy add - role - to - user edit system : serviceaccount : cos - liberty - build : jenkins - n cos - liberty - dev oc policy add - role - to - user edit system : serviceaccount : cos - liberty - build : jenkins - n cos - liberty - stage oc policy add - role - to - user edit system : serviceaccount : cos - liberty - build : jenkins - n cos - liberty - prod Import the deployment templates RedHat OpenShift templates are used to make artifact creation easier and repeatable. The template definition provided here defines a Kubernetes Service , Route and DeploymentConfig for the CustomerOrderServices application. The gse-liberty-deploy template defines the following: - service listening on ports 9080 , 9443 and 9082 - route to expose the 9443 port externally - DeploymentConfig to host the WebSphere Liberty container. - The image for the container is taken from the ImageStream that will be populated by the Jenkins pipeline. - environment variables are defined for the DB2 database used by the application allowing for environment specific information to be injected - Probes for liveness and readiness are defined to check port 9443 is active - The securityContext is set to allow read/write access to the filesystem and to run the container as user 1001 - The deployment will be updated if a new image is loaded to the ImageStream or if a change to the configuration is detected. Issue the commands below to load the template named gse-liberty-deploy in the dev , stage and prod projects. oc create - f template - liberty - deploy . yaml - n cos - liberty - dev oc create - f template - liberty - deploy . yaml - n cos - liberty - stage oc create - f template - liberty - deploy . yaml - n cos - liberty - prod Create the deployment definitions In this step the gse-liberty-deploy template will be used to create a RedHat OpenShift application named cos-liberty in the dev , stage and prod namespaces. The result will be: - service listening on ports 9080 , 9443 and 9082 - route to expose the 9443 port externally - DeploymentConfig to host the WebSphere Liberty container. The deployment config will wait for a docker image to be loaded in to the ImageStream by the Jenkins pipeline. Issue the following commands to create the applications from the template: oc new - app gse - liberty - deploy - p APPLICATION_NAME = cos - liberty - p DB2_HOST =< your DB2 host > - p DB2_PORT =< your DB2 host > - p DB2_USER =< your DB2 user > - p DB2_PASSWORD =< your DB2 password > - n cos - liberty - dev oc new - app gse - liberty - deploy - p APPLICATION_NAME = cos - liberty - p DB2_HOST =< your DB2 host > - p DB2_PORT =< your DB2 host > - p DB2_USER =< your DB2 user > - p DB2_PASSWORD =< your DB2 password > - n cos - liberty - stage oc new - app gse - liberty - deploy - p APPLICATION_NAME = cos - liberty - p DB2_HOST =< your DB2 host > - p DB2_PORT =< your DB2 host > - p DB2_USER =< your DB2 user > - p DB2_PASSWORD =< your DB2 password > - n cos - liberty - prod Import the build templates In this step a template for the build process will be loaded in to the build project. The template provided here defines the following artifacts: An ImageStream for the application image. This will be populated by the Jenkins Pipeline An ImageStream for WebSphere Liberty which will pull down the latest version of the ibmcom/websphere-liberty:kernel-ubi-min image and will monitor DockerHub for any updates. A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub Parameters to allow the WebSphere Liberty image and GitHub repository to be provided when the template is instantiated Issue the commands below to load the template named gse-liberty-build in the build projects. oc create - f template - liberty - build . yaml - n cos - liberty - build Create the build definitions In this step the gse-liberty-build template will be used to create a RedHat OpenShift application named cos-liberty in the build namespaces. The result will be: - An ImageStream for the application image. This will be populated by the Jenkins Pipeline - An ImageStream for WebSphere Liberty which will pull down the latest version of the ibmcom/websphere-liberty:kernel-ubi-min image and will monitor DockerHub for any updates. - A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image - A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub (with the URL provided as a parameter when the application is created) Issue the following commands to create the application from the template: oc new - app gse - liberty - build - p APPLICATION_NAME = cos - liberty - p SOURCE_URL = \" https://github.com/ibm-cloud-architecture/cloudpak-for-applications \" - n cos - liberty - build Run the pipeline The newly created pipeline can be started from the RedHat OpenShift console which allows access to the Jenkins logs but also tracks the progress in the OCP console. Navigate to Application Console --> Customer Order Services on Liberty - Build --> Builds --> Pipelines and click the Start Pipeline button When the pipeline starts, click the view log link to go to the Jenkins administration console. Note that it may take a couple of minutes before the view log link appears on the first pipeline build When prompted, log in with your OpenShift account and grant the required access permissions. The Jenkins console log will be displayed as shown below: Return to the OpenShift Console and track the progress of the pipeline The pipeline will eventually stop at the Promotion Gate for approval to deploy to Production. Click the Input Required link as shown below When the Promote application to Production question is displayed, click Proceed Return to the OpenShift Console and validate that the pipeline is now complete Validate the Deployments Now that the pipeline is complete, validate the Customer Order Services application is deployed and running in dev , stage and prod In the OpenShift Console, navigate to Application Console --> Customer Order Services on Liberty - Dev --> Applications --> Deployments and click on the link in the Latest Version column Information about the deployment will be displayed including the image that is being used (note the tag on the image as it will be the same in the stage and prod deployments). After a few minutes the container will be marked as ready Click Applications --> Routes and click on the route for the application. Note that the URL is < application_name >-< project_name >.< ocp cluster url >. In this case the project name is cos-liberty-dev Add /CustomerOrderServicesWeb to the end of the URL in the browser to access the application Repeat the validations for the stage and prod Projects. Review and Next Steps In this section you configured a CI/CD pipeline for the CustomerOrderServices application that builds a single immutable image for the latest version of the application and then deploys it to three different environments.","title":"Liberty - Deploy"},{"location":"liberty/liberty-deploy/#liberty-deploy","text":"This section covers how to deploy the application to RedHat OpenShift using an automated CI/CD pipeline. The diagram below shows the flow of the pipeline which starts when the developer checks their code in to Git and ends with the application being deployed in Production. the developer checks their code in to git a webhook automatically triggers the Jenkins Pipeline in the RHOS cluster the pipeline checks out the code from git and uses maven to build and test the application the oc start build command is used to build a docker image for the application the image is added to the ImageStream and pushed to the docker registry in the RHOS cluster the image is then tagged for the dev project the deployment running in the dev project is restarted using the newly created image the image is then tagged for the stage project the deployment running in the stage project is restarted using the newly created image the developer is prompted to approve the deployment to production the image is then tagged for the prod project the deployment running in the prod project is restarted using the newly created image The final versions of the files created in this section can be found in the liberty branch of this repo","title":"Liberty - Deploy"},{"location":"liberty/liberty-deploy/#summary","text":"This section has the following steps: RedHat OpenShift terminology and constructs The Jenkinsfile for the pipeline Create the Security Context Constraint required for WebSphere to run on RedHat OpenShift Create the build , dev , stage and prod projects Create a new Service Account and bind it to the Security Context Constraint in the dev , stage and prod projects Deploy Jenkins in the build namespace Grant the jenkins service account edit privileges in the dev , stage and prod projects Import the deployment template in to the dev , stage and prod projects Create the deployment config , service and route definitions using the template in the dev , stage and prod projects Import the build template in to the build project Create the ImageStream and BuildConfig definitions in the build project Trigger the pipeline and validate the application is running","title":"Summary"},{"location":"liberty/liberty-deploy/#redhat-openshift-constructs","text":"It is assumed that the reader is familiar with the following basic constructs provided by Kubernetes: Pod Deployment ServiceAccount Service RedHat OpenShift has the following constructs that are used in this scenario: - Route - ImageStream - BuildConfig - DeploymentConfig - SecurityContextConstraints - Application - Template","title":"RedHat OpenShift constructs"},{"location":"liberty/liberty-deploy/#the-jenkinsfile-for-the-pipeline","text":"A Jenkinsfile contains the definition of a pipeline in a format that can be stored in source control. In addition to standard Jenkins Pipeline Syntax, the OpenShift provides a Domain Specific Language (DSL) through the OpenShift Jenkins Client Plug-in. OpenShift DSL is easily readable that interacts with OpenShift API server, giving more control over the build, deployment, and promotion of applications on OpenShift cluster. The Jenkinsfile for this application can be found here The Jenkinsfile has the following stages : preamble : outputs some variable valus to the console Maven Build : builds the application EAR file using maven Unit Test : tests the application using maven Build Liberty App Image : uses the BuildConfig to build the Docker Image using the provided Dockerfile . This automatically tags the image in the build Project's ImageStream Promote to Dev : tags the image to the dev Project's ImageStream which results in the deployment in the dev Project being restarted with the new image Promote to Stage : tags the image to the stage Project's ImageStream which results in the deployment in the stage Project being restarted with the new image Promotion gate : prompts the user to approve promotion to production Promote to Prod : the image to the prod Project's ImageStream which results in the deployment in the prod Project being restarted with the new image","title":"The Jenkinsfile for the pipeline"},{"location":"liberty/liberty-deploy/#create-the-security-context-constraint","text":"In order to deploy and run the WebSphere Liberty Docker image in an OpenShift cluster, we first need to configure certain security aspects for the cluster. The Security Context Constraint provided here grants the service account that the WebSphere Liberty Docker container is running under the required privileges to function correctly. A cluster administrator can use the file provided here with the following command to create the Security Context Constraint (SCC): oc apply - f ssc . yaml","title":"Create the Security Context Constraint"},{"location":"liberty/liberty-deploy/#create-the-projects","text":"Four RedHat OpenShift projects are required in this scenario: - Build: this project will contain the Jenkins server and the artifacts used to build the application image - Dev: this is the development environment for this application - Stage: this is the staging environment for this application - Prod: this is the production environment for this application The file provided here contains the definitions for the four projects in a single file to make creation easier Issue the command shown below to create the projects oc create - f liberty - projects . yaml","title":"Create the projects"},{"location":"liberty/liberty-deploy/#create-a-service-account","text":"It is a good Kubernetes practice to create a service account for your applications. A service account provides an identity for processes that run in a Pod. In this step we will create a new service account with the name websphere in each of the dev , stage and prod projects and add the Security Context Constraint created above to them. Issue the commands shown below to create the websphere service account and bind the ibm-websphere-scc to it in each of the projects: oc create serviceaccount websphere - n cos - liberty - dev oc create serviceaccount websphere - n cos - liberty - stage oc create serviceaccount websphere - n cos - liberty - prod oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - liberty - dev oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - liberty - stage oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - liberty - prod","title":"Create a service account"},{"location":"liberty/liberty-deploy/#deploy-jenkins","text":"Some RedHat OpenShift clusters are configured to automatically provision a Jenkins instance in a build project. The steps below can be used if your cluster is not configured for automatic Jenkins provisioning: oc project cos - liberty - build oc new - app jenkins - persistent","title":"Deploy Jenkins"},{"location":"liberty/liberty-deploy/#update-the-jenkins-service-account","text":"During provisioning of the Jenkins master a service account with the name jenkins is created. This service account has privileges to create new artifacts only in the project that it is running in. In this scenario Jenkins will need to create artifacts in the dev , stage and prod projects. Issue the commands below to allow the jenkins service account to edit artifacts in the dev , stage and prod projects. oc policy add - role - to - user edit system : serviceaccount : cos - liberty - build : jenkins - n cos - liberty - dev oc policy add - role - to - user edit system : serviceaccount : cos - liberty - build : jenkins - n cos - liberty - stage oc policy add - role - to - user edit system : serviceaccount : cos - liberty - build : jenkins - n cos - liberty - prod","title":"Update the Jenkins service account"},{"location":"liberty/liberty-deploy/#import-the-deployment-templates","text":"RedHat OpenShift templates are used to make artifact creation easier and repeatable. The template definition provided here defines a Kubernetes Service , Route and DeploymentConfig for the CustomerOrderServices application. The gse-liberty-deploy template defines the following: - service listening on ports 9080 , 9443 and 9082 - route to expose the 9443 port externally - DeploymentConfig to host the WebSphere Liberty container. - The image for the container is taken from the ImageStream that will be populated by the Jenkins pipeline. - environment variables are defined for the DB2 database used by the application allowing for environment specific information to be injected - Probes for liveness and readiness are defined to check port 9443 is active - The securityContext is set to allow read/write access to the filesystem and to run the container as user 1001 - The deployment will be updated if a new image is loaded to the ImageStream or if a change to the configuration is detected. Issue the commands below to load the template named gse-liberty-deploy in the dev , stage and prod projects. oc create - f template - liberty - deploy . yaml - n cos - liberty - dev oc create - f template - liberty - deploy . yaml - n cos - liberty - stage oc create - f template - liberty - deploy . yaml - n cos - liberty - prod","title":"Import the deployment templates"},{"location":"liberty/liberty-deploy/#create-the-deployment-definitions","text":"In this step the gse-liberty-deploy template will be used to create a RedHat OpenShift application named cos-liberty in the dev , stage and prod namespaces. The result will be: - service listening on ports 9080 , 9443 and 9082 - route to expose the 9443 port externally - DeploymentConfig to host the WebSphere Liberty container. The deployment config will wait for a docker image to be loaded in to the ImageStream by the Jenkins pipeline. Issue the following commands to create the applications from the template: oc new - app gse - liberty - deploy - p APPLICATION_NAME = cos - liberty - p DB2_HOST =< your DB2 host > - p DB2_PORT =< your DB2 host > - p DB2_USER =< your DB2 user > - p DB2_PASSWORD =< your DB2 password > - n cos - liberty - dev oc new - app gse - liberty - deploy - p APPLICATION_NAME = cos - liberty - p DB2_HOST =< your DB2 host > - p DB2_PORT =< your DB2 host > - p DB2_USER =< your DB2 user > - p DB2_PASSWORD =< your DB2 password > - n cos - liberty - stage oc new - app gse - liberty - deploy - p APPLICATION_NAME = cos - liberty - p DB2_HOST =< your DB2 host > - p DB2_PORT =< your DB2 host > - p DB2_USER =< your DB2 user > - p DB2_PASSWORD =< your DB2 password > - n cos - liberty - prod","title":"Create the deployment definitions"},{"location":"liberty/liberty-deploy/#import-the-build-templates","text":"In this step a template for the build process will be loaded in to the build project. The template provided here defines the following artifacts: An ImageStream for the application image. This will be populated by the Jenkins Pipeline An ImageStream for WebSphere Liberty which will pull down the latest version of the ibmcom/websphere-liberty:kernel-ubi-min image and will monitor DockerHub for any updates. A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub Parameters to allow the WebSphere Liberty image and GitHub repository to be provided when the template is instantiated Issue the commands below to load the template named gse-liberty-build in the build projects. oc create - f template - liberty - build . yaml - n cos - liberty - build","title":"Import the build templates"},{"location":"liberty/liberty-deploy/#create-the-build-definitions","text":"In this step the gse-liberty-build template will be used to create a RedHat OpenShift application named cos-liberty in the build namespaces. The result will be: - An ImageStream for the application image. This will be populated by the Jenkins Pipeline - An ImageStream for WebSphere Liberty which will pull down the latest version of the ibmcom/websphere-liberty:kernel-ubi-min image and will monitor DockerHub for any updates. - A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image - A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub (with the URL provided as a parameter when the application is created) Issue the following commands to create the application from the template: oc new - app gse - liberty - build - p APPLICATION_NAME = cos - liberty - p SOURCE_URL = \" https://github.com/ibm-cloud-architecture/cloudpak-for-applications \" - n cos - liberty - build","title":"Create the build definitions"},{"location":"liberty/liberty-deploy/#run-the-pipeline","text":"The newly created pipeline can be started from the RedHat OpenShift console which allows access to the Jenkins logs but also tracks the progress in the OCP console. Navigate to Application Console --> Customer Order Services on Liberty - Build --> Builds --> Pipelines and click the Start Pipeline button When the pipeline starts, click the view log link to go to the Jenkins administration console. Note that it may take a couple of minutes before the view log link appears on the first pipeline build When prompted, log in with your OpenShift account and grant the required access permissions. The Jenkins console log will be displayed as shown below: Return to the OpenShift Console and track the progress of the pipeline The pipeline will eventually stop at the Promotion Gate for approval to deploy to Production. Click the Input Required link as shown below When the Promote application to Production question is displayed, click Proceed Return to the OpenShift Console and validate that the pipeline is now complete","title":"Run the pipeline"},{"location":"liberty/liberty-deploy/#validate-the-deployments","text":"Now that the pipeline is complete, validate the Customer Order Services application is deployed and running in dev , stage and prod In the OpenShift Console, navigate to Application Console --> Customer Order Services on Liberty - Dev --> Applications --> Deployments and click on the link in the Latest Version column Information about the deployment will be displayed including the image that is being used (note the tag on the image as it will be the same in the stage and prod deployments). After a few minutes the container will be marked as ready Click Applications --> Routes and click on the route for the application. Note that the URL is < application_name >-< project_name >.< ocp cluster url >. In this case the project name is cos-liberty-dev Add /CustomerOrderServicesWeb to the end of the URL in the browser to access the application Repeat the validations for the stage and prod Projects.","title":"Validate the Deployments"},{"location":"liberty/liberty-deploy/#review-and-next-steps","text":"In this section you configured a CI/CD pipeline for the CustomerOrderServices application that builds a single immutable image for the latest version of the application and then deploys it to three different environments.","title":"Review and Next Steps"},{"location":"liberty/liberty-sessions/","text":"Session state management considerations One of the 12 factor principles is that applications and services should be stateless. Many existing monolith applications rely heavily on HTTP sessions. When you are modernizing your application it is a good practice to rearchitect them to be stateless, but some times it is not an easy task and you need to postpone it for a while. There are following methods that you can use in container world to handle session state, depending on your requirements: - session affinity - session replication using caching mechanisms - session persistence using database Session affinity Session affinity is the simplest mechanism for maintaining session state. Using this mechanism, requests from the same session will be routed to the same POD. Selecting this solution, you have to be aware that PODs are not like on-premise servers, they may be restarted much often, in many circumstances. So this solution might be applicable to applications that need relatively short lived sessions, and can tolerate loosing session data. Deploying with session affinity Session affinity on OpenShift utilizes Route configuration. You don't have to make any changes in your application code or during building application container, all is done on deployment time. In this example, a simple application has been deployed to WebSphere Liberty that displays the contents of the session and allows updates to be made to a counter that is stored in the session as well as displaying the name of the pod that services the request. There are multiple instances of the application pod running: oc get pods | grep sample session - sample - 1 - bzpd7 1 / 1 Running 0 10 d session - sample - 1 - dnpsg 1 / 1 Running 0 10 d session - sample - 1 - g5c8k 1 / 1 Running 0 21 d On the first request, the output is as shown below with the name of the pod an empty session. Since this is first request, currently there is no data in the session. You can also see name of the pod that runs the application. After a few requests to the app, using the link or reloading the page, the the counter value that is taken from the session increases as shwon below. Note that the requests are routed to the same pod. Session affinity is working. If the pod is then deleted it will be replaced by OpenShift: oc delete pod session - sample - 1 - g5c8k pod \"session-sample-1-g5c8k\" deleted oc get pods | grep sample session - sample - 1 - 5 hzd5 1 / 1 Running 0 3 m session - sample - 1 - 7 w8gh 1 / 1 Running 0 23 s session - sample - 1 - dnpsg 1 / 1 Running 0 10 d If an attempt is then made to refresh the browser page or increment the count then the request is routed to the different pod and session state is lost: If you want to ensure that session state is not lost when pod is removed, you will need to configure session caching or session persistence. Session replication using caching mechanisms Very often session affinity is not enough for application requirements and you need session state to be preserved in case of pod failure. One of the mechanisms that could be used is session caching . WebSphere Liberty has a sessionCache-1.0 feature which provides distributed in-memory HttpSession caching. The sessionCache-1.0 feature builds on top of an existing technology called JCache (JSR 107) , which offers a standardized distributed in-memory caching API. The sessionCache-1.0 feature does not include a JCache implementation, so you need to pick one and reference it as a <library> in your server.xml . WebSphere/Open Liberty supports the following JCache implementations: - Hazelcast - WebSphere Extreme Scale - Infinispan - Ehcache Prepare DockerFile with caching configuration This article shows how to enable Hazelcast In-Memory Data Grid , as it is easily available in many private cloud solutions. Enabling Hazelcast session caching retrieves the Hazelcast client libraries from the hazelcast/hazelcast Docker image, configures Hazelcast by copying a sample hazelcast.xml, and configures the Liberty server feature sessionCache-1.0 by including the XML snippet hazelcast-sessioncache.xml. By default, the Hazelcast Discovery Plugin for Kubernetes will auto-discover its peers within the same Kubernetes namespace. To enable this functionality, the Docker image author can include the following Dockerfile snippet, and choose from either client-server or embedded topology. Modify your current Dockerfile with the following lines: ### Hazelcast Session Caching ### # Copy the Hazelcast libraries from the Hazelcast Docker image - paths for WebSphere Liberty COPY --from = hazelcast/hazelcast --chown = 1001 :0 /opt/hazelcast/lib/*.jar /opt/ibm/wlp/usr/shared/resources/hazelcast/ # Copy the Hazelcast libraries from the Hazelcast Docker image - paths for Open Liberty # COPY --from=hazelcast/hazelcast --chown=1001:0 /opt/hazelcast/lib/*.jar /opt/ol/wlp/usr/shared/resources/hazelcast/ # Instruct configure.sh to copy the client topology hazelcast.xml ARG HZ_SESSION_CACHE = client # Instruct configure.sh to copy the embedded topology hazelcast.xml and set the required system property #ARG HZ_SESSION_CACHE=embedded #ENV JAVA_TOOL_OPTIONS=\"-Dhazelcast.jcache.provider.type=server ${JAVA_TOOL_OPTIONS}\" ## This script will add the requested XML snippets and grow image to be fit-for-purpose RUN configure.sh Deploy Hazelcast in OpenShift Hazelcast can be used in OpenShift. By default it requires paid version - Hazelcast Enterprise. But you can use also free version Hazelcast OpenShift Origin . See details on this page Hazelcast for OpenShift For this article you will use Hazelcast OpenShift Origin. Deploy Hazelcast cluster Easiest way to deploy Hazelcast is to use hazelcast.yaml file provided by Hazelcast Create project for Hazelcast cluster $ oc new-project appmod-hazelcast Deploy Hazelcast cluster $ oc new-app -f hazelcast.yaml -p NAMESPACE=appmod-hazelcast Check the status of deployed cluster $ oc get all NAME READY STATUS RESTARTS AGE pod/hazelcast-0 1 /1 Running 0 1m pod/hazelcast-1 1 /1 Running 0 42s pod/hazelcast-2 0 /1 ContainerCreating 0 2s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/hazelcast-service LoadBalancer x.x.x.x y.y.y.y 5701 :32567/TCP 1m NAME DESIRED CURRENT AGE statefulset.apps/hazelcast 3 3 1m This cluster contains 3 replicas, look in one of the pods logs to check if all members are correctly detected. Look for similar messages: $ oc get pods Members { size:3, ver:7 } [ Member [ 192 .168.16.15 ] :5701 - 9dc4af56-6df8-4a65-9890-68bf99e0ea5a Member [ 192 .168.12.16 ] :5701 - f927d37c-860d-4b38-88b4-b1f3dd97bfd1 this Member [ 192 .168.22.15 ] :5701 - ec8602a7-59b0-4dab-8446-3a8f32dac845 ] Deploy OpenLiberty configured for Hazelcast The Hazelcast client configured with OpenLiberty is using Kubernetes API to find Hazelcast cluster and requires a new ServiceAccount and ClusterRoleBinding Create project for the client application: $ oc new-project appmod-hazelcast-liberty Create a new ServiceAccount: $ oc create serviceaccount hazelcast-liberty -n appmod-hazelcast-liberty Create rbac.yaml with the following content: apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : default-cluster roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : view subjects : - kind : ServiceAccount name : hazelcast-liberty namespace : appmod-hazelcast-liberty Then, apply rbac.yaml : $ kubectl apply -f rbac.yaml Note : You can be even more strict with the permissions and create your own Role. For details, please check the implementation of Hazelcast Helm Chart . The following changes are required to a WebSphere Liberty deployment on OpenShift in order to use Hazelcast: an environment variable: KUBERNETES_NAMESPACE that is set to the namespace that Hazelcast is deployed in to the deployment much be updated to use the new serviceAccount by adding the following to the template/spec section: serviceAccountName : hazelcast - liberty serviceAccount : hazelcast - liberty In order to validate that the in-memory session cache is working a similar test to the session affinity test is executed. There are two pods running the application: oc get pods NAME READY STATUS RESTARTS AGE session - hazel3 - 2 - 425 tr 1 / 1 Running 0 9 d session - hazel3 - 2 - p5gtd 1 / 1 Running 0 9 d On the first request shown below, the session is empty and the pod is session-hazel3-2-425tr After a few requests to the app, the same pod is being used and the session is being retained If the pod is then deleted it will be replaced by OpenShift: oc delete pod session - hazel3 - 2 - 425 tr pod \"session-hazel3-2-425tr\" deleted oc get pods NAME READY STATUS RESTARTS AGE session - hazel3 - 2 - p5gtd 1 / 1 Running 0 9 d session - hazel3 - 2 - wc4wd 1 / 1 Running 0 18 s If an attempt is then made to refresh the browser page or increment the count then the request is routed to the different pod ( session-hazel3-2--wc4wd ) but the session state is retained: Session persistence using database In some cases you may want to use database instead of cache for storing session data, especially if you earlier already were using database as session persistent layer. WebSphere Liberty fully supports session persistence using data base using sessionDatabase-1.0 feature. You will utilize dynamic configuration support in Liberty to configure session persistence without modifying your original server configuration. Liberty configuration Create new configuration file session-db.xml with the following contents. It configures required feature, database driver libraries, datasource, and http session management. <server description= \"Demonstrates HTTP Session Persistence Configuration\" > <featureManager> <feature> sessionDatabase-1.0 </feature> </featureManager> <library id= \"DB2Lib\" > <fileset dir= \"${server.config.dir}/resources/db2\" includes= \"*.jar\" /> </library> <dataSource id= \"SessionsDS\" > <jdbcDriver libraryRef= \"DB2Lib\" /> <properties.db2.jcc databaseName= \"${env.DB2_DBNAME}\" password= \"${env.DB2_PASSWORD}\" portNumber= \"${env.DB2_PORT}\" serverName= \"${env.DB2_HOST}\" user= \"${env.DB2_USER}\" /> <connectionManager agedTimeout= \"0\" connectionTimeout= \"180\" maxIdleTime= \"1800\" maxPoolSize= \"10\" minPoolSize= \"1\" reapTime= \"180\" /> </dataSource> <httpSessionDatabase id= \"SessionDB\" dataSourceRef= \"SessionsDS\" /> <httpSession cloneId= \"${env.HOSTNAME}\" /> </server> In the configuration you are utilizing environment variables for database access parameters such as host, port, userid, etc (e.g. ${env.DB2_HOST} ). These variables will be provided during the deployment. Modifying the Dockerfile As now solution requires additional configuration file and database driver to access session database, that needs to be added to the Dockerfile. # session persistence COPY --chown=1001:0 db2drivers/ /config/resources/db2 COPY --chown=1001:0 src/main/liberty/config/session-db.xml /config/configDropins/overrides/ Deploying to OpenShift The only changes required to an existing WebSphere Liberty deployment are to add the Environment Variables for the DB2 database that is being used as the session database: Environment Variables: DB2_HOST , DB2_PORT , DB2_DBNAME , DB2_USER and DB2_PASSWORD should be added to the deployment. The same test that has been used for session affinity and in-memory session cache can be used to validate session persistence using a database . Summary In this article we have demonstrated how to use session affinity, session caching using an in-memory cache and session persistence using a database with WebSphere Liberty on OpenShift.","title":"Session state management considerations"},{"location":"liberty/liberty-sessions/#session-state-management-considerations","text":"One of the 12 factor principles is that applications and services should be stateless. Many existing monolith applications rely heavily on HTTP sessions. When you are modernizing your application it is a good practice to rearchitect them to be stateless, but some times it is not an easy task and you need to postpone it for a while. There are following methods that you can use in container world to handle session state, depending on your requirements: - session affinity - session replication using caching mechanisms - session persistence using database","title":"Session state management considerations"},{"location":"liberty/liberty-sessions/#session-affinity","text":"Session affinity is the simplest mechanism for maintaining session state. Using this mechanism, requests from the same session will be routed to the same POD. Selecting this solution, you have to be aware that PODs are not like on-premise servers, they may be restarted much often, in many circumstances. So this solution might be applicable to applications that need relatively short lived sessions, and can tolerate loosing session data.","title":"Session affinity"},{"location":"liberty/liberty-sessions/#deploying-with-session-affinity","text":"Session affinity on OpenShift utilizes Route configuration. You don't have to make any changes in your application code or during building application container, all is done on deployment time. In this example, a simple application has been deployed to WebSphere Liberty that displays the contents of the session and allows updates to be made to a counter that is stored in the session as well as displaying the name of the pod that services the request. There are multiple instances of the application pod running: oc get pods | grep sample session - sample - 1 - bzpd7 1 / 1 Running 0 10 d session - sample - 1 - dnpsg 1 / 1 Running 0 10 d session - sample - 1 - g5c8k 1 / 1 Running 0 21 d On the first request, the output is as shown below with the name of the pod an empty session. Since this is first request, currently there is no data in the session. You can also see name of the pod that runs the application. After a few requests to the app, using the link or reloading the page, the the counter value that is taken from the session increases as shwon below. Note that the requests are routed to the same pod. Session affinity is working. If the pod is then deleted it will be replaced by OpenShift: oc delete pod session - sample - 1 - g5c8k pod \"session-sample-1-g5c8k\" deleted oc get pods | grep sample session - sample - 1 - 5 hzd5 1 / 1 Running 0 3 m session - sample - 1 - 7 w8gh 1 / 1 Running 0 23 s session - sample - 1 - dnpsg 1 / 1 Running 0 10 d If an attempt is then made to refresh the browser page or increment the count then the request is routed to the different pod and session state is lost: If you want to ensure that session state is not lost when pod is removed, you will need to configure session caching or session persistence.","title":"Deploying with session affinity"},{"location":"liberty/liberty-sessions/#session-replication-using-caching-mechanisms","text":"Very often session affinity is not enough for application requirements and you need session state to be preserved in case of pod failure. One of the mechanisms that could be used is session caching . WebSphere Liberty has a sessionCache-1.0 feature which provides distributed in-memory HttpSession caching. The sessionCache-1.0 feature builds on top of an existing technology called JCache (JSR 107) , which offers a standardized distributed in-memory caching API. The sessionCache-1.0 feature does not include a JCache implementation, so you need to pick one and reference it as a <library> in your server.xml . WebSphere/Open Liberty supports the following JCache implementations: - Hazelcast - WebSphere Extreme Scale - Infinispan - Ehcache","title":"Session replication using caching mechanisms"},{"location":"liberty/liberty-sessions/#prepare-dockerfile-with-caching-configuration","text":"This article shows how to enable Hazelcast In-Memory Data Grid , as it is easily available in many private cloud solutions. Enabling Hazelcast session caching retrieves the Hazelcast client libraries from the hazelcast/hazelcast Docker image, configures Hazelcast by copying a sample hazelcast.xml, and configures the Liberty server feature sessionCache-1.0 by including the XML snippet hazelcast-sessioncache.xml. By default, the Hazelcast Discovery Plugin for Kubernetes will auto-discover its peers within the same Kubernetes namespace. To enable this functionality, the Docker image author can include the following Dockerfile snippet, and choose from either client-server or embedded topology. Modify your current Dockerfile with the following lines: ### Hazelcast Session Caching ### # Copy the Hazelcast libraries from the Hazelcast Docker image - paths for WebSphere Liberty COPY --from = hazelcast/hazelcast --chown = 1001 :0 /opt/hazelcast/lib/*.jar /opt/ibm/wlp/usr/shared/resources/hazelcast/ # Copy the Hazelcast libraries from the Hazelcast Docker image - paths for Open Liberty # COPY --from=hazelcast/hazelcast --chown=1001:0 /opt/hazelcast/lib/*.jar /opt/ol/wlp/usr/shared/resources/hazelcast/ # Instruct configure.sh to copy the client topology hazelcast.xml ARG HZ_SESSION_CACHE = client # Instruct configure.sh to copy the embedded topology hazelcast.xml and set the required system property #ARG HZ_SESSION_CACHE=embedded #ENV JAVA_TOOL_OPTIONS=\"-Dhazelcast.jcache.provider.type=server ${JAVA_TOOL_OPTIONS}\" ## This script will add the requested XML snippets and grow image to be fit-for-purpose RUN configure.sh","title":"Prepare DockerFile with caching configuration"},{"location":"liberty/liberty-sessions/#deploy-hazelcast-in-openshift","text":"Hazelcast can be used in OpenShift. By default it requires paid version - Hazelcast Enterprise. But you can use also free version Hazelcast OpenShift Origin . See details on this page Hazelcast for OpenShift For this article you will use Hazelcast OpenShift Origin.","title":"Deploy Hazelcast in OpenShift"},{"location":"liberty/liberty-sessions/#deploy-hazelcast-cluster","text":"Easiest way to deploy Hazelcast is to use hazelcast.yaml file provided by Hazelcast Create project for Hazelcast cluster $ oc new-project appmod-hazelcast Deploy Hazelcast cluster $ oc new-app -f hazelcast.yaml -p NAMESPACE=appmod-hazelcast Check the status of deployed cluster $ oc get all NAME READY STATUS RESTARTS AGE pod/hazelcast-0 1 /1 Running 0 1m pod/hazelcast-1 1 /1 Running 0 42s pod/hazelcast-2 0 /1 ContainerCreating 0 2s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/hazelcast-service LoadBalancer x.x.x.x y.y.y.y 5701 :32567/TCP 1m NAME DESIRED CURRENT AGE statefulset.apps/hazelcast 3 3 1m This cluster contains 3 replicas, look in one of the pods logs to check if all members are correctly detected. Look for similar messages: $ oc get pods Members { size:3, ver:7 } [ Member [ 192 .168.16.15 ] :5701 - 9dc4af56-6df8-4a65-9890-68bf99e0ea5a Member [ 192 .168.12.16 ] :5701 - f927d37c-860d-4b38-88b4-b1f3dd97bfd1 this Member [ 192 .168.22.15 ] :5701 - ec8602a7-59b0-4dab-8446-3a8f32dac845 ]","title":"Deploy Hazelcast cluster"},{"location":"liberty/liberty-sessions/#deploy-openliberty-configured-for-hazelcast","text":"The Hazelcast client configured with OpenLiberty is using Kubernetes API to find Hazelcast cluster and requires a new ServiceAccount and ClusterRoleBinding Create project for the client application: $ oc new-project appmod-hazelcast-liberty Create a new ServiceAccount: $ oc create serviceaccount hazelcast-liberty -n appmod-hazelcast-liberty Create rbac.yaml with the following content: apiVersion : rbac.authorization.k8s.io/v1 kind : ClusterRoleBinding metadata : name : default-cluster roleRef : apiGroup : rbac.authorization.k8s.io kind : ClusterRole name : view subjects : - kind : ServiceAccount name : hazelcast-liberty namespace : appmod-hazelcast-liberty Then, apply rbac.yaml : $ kubectl apply -f rbac.yaml Note : You can be even more strict with the permissions and create your own Role. For details, please check the implementation of Hazelcast Helm Chart . The following changes are required to a WebSphere Liberty deployment on OpenShift in order to use Hazelcast: an environment variable: KUBERNETES_NAMESPACE that is set to the namespace that Hazelcast is deployed in to the deployment much be updated to use the new serviceAccount by adding the following to the template/spec section: serviceAccountName : hazelcast - liberty serviceAccount : hazelcast - liberty In order to validate that the in-memory session cache is working a similar test to the session affinity test is executed. There are two pods running the application: oc get pods NAME READY STATUS RESTARTS AGE session - hazel3 - 2 - 425 tr 1 / 1 Running 0 9 d session - hazel3 - 2 - p5gtd 1 / 1 Running 0 9 d On the first request shown below, the session is empty and the pod is session-hazel3-2-425tr After a few requests to the app, the same pod is being used and the session is being retained If the pod is then deleted it will be replaced by OpenShift: oc delete pod session - hazel3 - 2 - 425 tr pod \"session-hazel3-2-425tr\" deleted oc get pods NAME READY STATUS RESTARTS AGE session - hazel3 - 2 - p5gtd 1 / 1 Running 0 9 d session - hazel3 - 2 - wc4wd 1 / 1 Running 0 18 s If an attempt is then made to refresh the browser page or increment the count then the request is routed to the different pod ( session-hazel3-2--wc4wd ) but the session state is retained:","title":"Deploy OpenLiberty configured for Hazelcast"},{"location":"liberty/liberty-sessions/#session-persistence-using-database","text":"In some cases you may want to use database instead of cache for storing session data, especially if you earlier already were using database as session persistent layer. WebSphere Liberty fully supports session persistence using data base using sessionDatabase-1.0 feature. You will utilize dynamic configuration support in Liberty to configure session persistence without modifying your original server configuration.","title":"Session persistence using database"},{"location":"liberty/liberty-sessions/#liberty-configuration","text":"Create new configuration file session-db.xml with the following contents. It configures required feature, database driver libraries, datasource, and http session management. <server description= \"Demonstrates HTTP Session Persistence Configuration\" > <featureManager> <feature> sessionDatabase-1.0 </feature> </featureManager> <library id= \"DB2Lib\" > <fileset dir= \"${server.config.dir}/resources/db2\" includes= \"*.jar\" /> </library> <dataSource id= \"SessionsDS\" > <jdbcDriver libraryRef= \"DB2Lib\" /> <properties.db2.jcc databaseName= \"${env.DB2_DBNAME}\" password= \"${env.DB2_PASSWORD}\" portNumber= \"${env.DB2_PORT}\" serverName= \"${env.DB2_HOST}\" user= \"${env.DB2_USER}\" /> <connectionManager agedTimeout= \"0\" connectionTimeout= \"180\" maxIdleTime= \"1800\" maxPoolSize= \"10\" minPoolSize= \"1\" reapTime= \"180\" /> </dataSource> <httpSessionDatabase id= \"SessionDB\" dataSourceRef= \"SessionsDS\" /> <httpSession cloneId= \"${env.HOSTNAME}\" /> </server> In the configuration you are utilizing environment variables for database access parameters such as host, port, userid, etc (e.g. ${env.DB2_HOST} ). These variables will be provided during the deployment.","title":"Liberty configuration"},{"location":"liberty/liberty-sessions/#modifying-the-dockerfile","text":"As now solution requires additional configuration file and database driver to access session database, that needs to be added to the Dockerfile. # session persistence COPY --chown=1001:0 db2drivers/ /config/resources/db2 COPY --chown=1001:0 src/main/liberty/config/session-db.xml /config/configDropins/overrides/","title":"Modifying the Dockerfile"},{"location":"liberty/liberty-sessions/#deploying-to-openshift","text":"The only changes required to an existing WebSphere Liberty deployment are to add the Environment Variables for the DB2 database that is being used as the session database: Environment Variables: DB2_HOST , DB2_PORT , DB2_DBNAME , DB2_USER and DB2_PASSWORD should be added to the deployment. The same test that has been used for session affinity and in-memory session cache can be used to validate session persistence using a database .","title":"Deploying to OpenShift"},{"location":"liberty/liberty-sessions/#summary","text":"In this article we have demonstrated how to use session affinity, session caching using an in-memory cache and session persistence using a database with WebSphere Liberty on OpenShift.","title":"Summary"},{"location":"spring/","text":"Spring Modernization Spring modernization describes the process of upgrading existing Spring Framework and Spring Boot v1 applications to use Spring Boot v2 . Pivotal have made some changes to the list of supported versions of Spring that may require modernization of existing applications to Spring Boot v2. With the Open Liberty project, IBM have provided support for Spring Boot and made optimizations to the runtime and Docker images specifically for Spring Boot . There are some benefits of running Spring Boot applications on the Open Liberty runtime: Performance. Benchmarks have shows that Liberty perform better than Tomcat on both throughput and response time. Size. The memory footprint of Liberty is smaller than tomcat, but more importantly the Spring Boot libraries can be separated from the runtime libraries. When we look at the way Docker builds it's images using layers, the application portion is much smaller if we build out images in the optimized way. This means faster build time, and if you're storing every version of your application you build, the delta between versions is much smaller Support. If you're running on OpenShift using IBM Cloud Paks, using the Liberty runtime is included in your licensing model, so using Liberty comes at no additional cost, but if need any support for Liberty it's included. If you stick with standard Spring Boot with tomcat, you'll have to either run on an unsupported platform, or pay for additional support for the tomcat runtime. Consistent runtime model. Running Liberty has many best practices especially in the Kubernetes/OpenShift world. There are built in metrics and monitoring tools which are specifically designed to be integrated into OpenShift. Using Liberty allows you to leverage many if these automatic connections to better maintain your environment. This repository holds a solution that is the result of a modernization for an Spring application that was upgraded to Spring Boot on Open Liberty and deployed by the IBM CloudPak for Applications to RedHat OpenShift. Application Overview Pet Clinic is a demonstration application that was created in 2003 to show the features and functions of the Spring Framework. A description of the original application can be found in the The Pet Clinic Application section of the readme How the Application was Modernized In order to modernize the application from the Spring Framework to Spring Boot and Open Liberty running on OpenShift, the application went through code changes , build and deploy phases. Code Changes Pet Clinic was modernized by the open source community in 2013 from Spring Framework 2.5 to Spring Framework 3.0 and then in 2016 to Spring Boot. The process of making the code changes is outside of the scope of this document, however the general modernization process is described below: Spring Framework Upgrades Spring Boot Upgrades Spring Framework to Spring Boot Build The build phase created the Dockerfile for the application. A Spring Boot application JAR or WAR file is a self-contained artifact. It packages all of the application dependencies inside the final artifact alongside the application content, including an embedded server implementation, such as Tomcat, Jetty, or Undertow. The result is a fat artifact that is easy to run on any server that has a JVM. It also results in a large artifact, even for the smallest hello world Spring Boot web application. The Pet Clinic Dockerfile for the self-contained jar is shown below: FROM openliberty / open - liberty : springBoot2 - ubi - min COPY --chown=1001:0 spring-petclinic/target/spring-petclinic-2.1.0.BUILD-SNAPSHOT.jar /config/dropins/spring/ In order to optimize the Docker image, the Dual Layer Approach that IBM has created is used and resulted in the Dockerfile shown below: FROM openliberty / open - liberty : springBoot2 - ubi - min as staging USER root COPY spring - petclinic / target / spring - petclinic - 2 . 1 . 0 . BUILD - SNAPSHOT . jar / staging / fatClinic . jar RUN springBootUtility thin \\ --sourceAppPath=/staging/fatClinic.jar \\ --targetThinAppPath=/staging/thinClinic.jar \\ --targetLibCachePath=/staging/lib.index.cache FROM openliberty / open - liberty : springBoot2 - ubi - min USER root COPY --from=staging /staging/lib.index.cache /opt/ol/wlp/usr/shared/resources/lib.index.cache COPY --from=staging /staging/thinClinic.jar /config/dropins/spring/thinClinic.jar RUN chown - R 1001 . 0 / config && chmod - R g + rw / config RUN chown - R 1001 . 0 / opt / ol / wlp / usr / shared / resources / lib . index . cache && chmod - R g + rw / opt / ol / wlp / usr / shared / resources / lib . index . cache USER 1001 The final file can be found here: Dockerfile The containerized application was tested locally before the code and configuration files were committed to the git repository Deploy The deploy phase created the Jenkins, Kubernetes and Red Hat OpenShift artifacts required to automate the build and deployment pipeline for the application. For illustration purposes, the application was deployed to three different Red Hat OpenShift projects to simulate development , staging and production . The diagram below shows the flow through the pipeline. A more detailed description can be found here The steps were: Configure the Red Hat OpenShift Cluster for WebSphere by creating the necessary SecurityContextConstraints definition. The file can be found here: scc.yaml Create the Red Hat OpenShift build template that would be used to define the Red Hat OpenShift artifacts related to the build process including ImageStream and BuildConfig definitions. The file can be found here: template-libery-build.yaml Create the Red Hat OpenShift deployment template that would be used to define the Red Hat OpenShift artifacts related to the Pet Clinic application including DeploymentConfig , Service and Route definitions. The file can be found here: template-libery-deploy.yaml Create the Jenkins Jenkinsfile for the pipeline. The Jenkinsfile defines the steps that the pipeline takes to build the Pet Clinic application, create an immutable Docker Image and then move the image through the dev , stage and prod environments. The file can be found here: Jenkinsfile Create the build project, load the build template and configure Jenkins Create the dev , stage and prod projects and load the deployment template Verify the pipeline. Detailed, step-by-step instructions on how to replicate these steps are provided here Deploy the Application The following steps will deploy the modernized Pet Clinic application in a Open Liberty container to a Red Hat OpenShift cluster. Prerequisites You will need the following: Git CLI Red Hat OpenShift 3.11 with Cluster Admin permissions oc CLI DB2 Database Getting the project repository You can clone the repository from its main GitHub repository page and checkout the appropriate branch for this version of the application. git clone https : // github . com / ibm - cloud - architecture / cloudpak - for - applications . git cd cloudpak - for - applications git checkout spring Create the Security Context Constraint In order to deploy and run the Open Liberty Docker image in an OpenShift cluster, we first need to configure certain security aspects for the cluster. The Security Context Constraint provided here grants the service account that the Open Liberty Docker container is running under the required privileges to function correctly. A cluster administrator can use the file provided here with the following command to create the Security Context Constraint (SCC): cd Deployment / OpenShift oc apply - f ssc . yaml Create the projects Four Red Hat OpenShift projects are required in this scenario: - Build: this project will contain the Jenkins server and the artifacts used to build the application image - Dev: this is the development environment for this application - Stage: this is the staging environment for this application - Prod: this is the production environment for this application The file provided here contains the definitions for the four projects in a single file to make creation easier Issue the command shown below to create the projects oc create - f liberty - projects . yaml Create a service account It is a good Kubernetes practice to create a service account for your applications. A service account provides an identity for processes that run in a Pod. In this step we will create a new service account with the name websphere in each of the dev , stage and prod projects and add the Security Context Constraint created above to them. Issue the commands shown below to create the websphere service account and bind the ibm-websphere-scc to it in each of the projects: oc create serviceaccount websphere - n petclinic - liberty - dev oc create serviceaccount websphere - n petclinic - liberty - stage oc create serviceaccount websphere - n petclinic - liberty - prod oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n petclinic - liberty - dev oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n petclinic - liberty - stage oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n petclinic - liberty - prod Deploy Jenkins Some Red Hat OpenShift clusters are configured to automatically provision a Jenkins instance in a build project. The steps below can be used if your cluster is not configured for automatic Jenkins provisioning: oc project petclinic - liberty - build oc new - app jenkins - persistent Update the Jenkins service account During provisioning of the Jenkins master a service account with the name jenkins is created. This service account has privileges to create new artifacts only in the project that it is running in. In this scenario Jenkins will need to create artifacts in the dev , stage and prod projects. Issue the commands below to allow the jenkins service account to edit artifacts in the dev , stage and prod projects. oc policy add - role - to - user edit system : serviceaccount : petclinic - liberty - build : jenkins - n petclinic - liberty - dev oc policy add - role - to - user edit system : serviceaccount : petclinic - liberty - build : jenkins - n petclinic - liberty - stage oc policy add - role - to - user edit system : serviceaccount : petclinic - liberty - build : jenkins - n petclinic - liberty - prod Import the deployment templates Red Hat OpenShift templates are used to make artifact creation easier and repeatable. The template definition provided here defines a Kubernetes Service , Route and DeploymentConfig for the CustomerOrderServices application. The gse-spring-deploy template defines the following: - service listening on ports 9080 , 9443 and 9082 - route to expose the 9443 port externally - DeploymentConfig to host the Open Liberty container. - The image for the container is taken from the ImageStream that will be populated by the Jenkins pipeline. - environment variables are defined for the DB2 database used by the application allowing for environment specific information to be injected - Probes for liveness and readiness are defined to check port 9443 is active - The securityContext is set to allow read/write access to the filesystem and to run the container as user 1001 - The deployment will be updated if a new image is loaded to the ImageStream or if a change to the configuration is detected. Issue the commands below to load the template named gse-spring-deploy in the dev , stage and prod projects. oc create - f template - liberty - deploy . yaml - n petclinic - liberty - dev oc create - f template - liberty - deploy . yaml - n petclinic - liberty - stage oc create - f template - liberty - deploy . yaml - n petclinic - liberty - prod Create the deployment definitions In this step the gse-spring-deploy template will be used to create a Red Hat OpenShift application named petclinic-liberty in the dev , stage and prod namespaces. The result will be: - service listening on ports 9080 , 9443 and 9082 - route to expose the 9443 port externally - DeploymentConfig to host the Open Liberty container. The deployment config will wait for a docker image to be loaded in to the ImageStream by the Jenkins pipeline. Issue the following commands to create the applications from the template: oc new - app gse - spring - deploy - p APPLICATION_NAME = petclinic - liberty - n petclinic - liberty - dev oc new - app gse - spring - deploy - p APPLICATION_NAME = petclinic - liberty - n petclinic - liberty - stage oc new - app gse - spring - deploy - p APPLICATION_NAME = petclinic - liberty - n petclinic - liberty - prod Import the build templates In this step a template for the build process will be loaded in to the build project. The template provided here defines the following artifacts: An ImageStream for the application image. This will be populated by the Jenkins Pipeline An ImageStream for Open Liberty which will pull down the latest version of the openliberty/open-liberty:springBoot2-ubi-min image and will monitor DockerHub for any updates. A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub Parameters to allow the Open Liberty image and GitHub repository to be provided when the template is instantiated Issue the commands below to load the template named gse-springboot-build in the build projects. oc create - f template - liberty - build . yaml - n petclinic - liberty - build Create the build definitions In this step the gse-springboot-build template will be used to create a Red Hat OpenShift application named petclinic-liberty in the build namespaces. The result will be: - An ImageStream for the application image. This will be populated by the Jenkins Pipeline - An ImageStream for Open Liberty which will pull down the latest version of the openliberty/open-liberty:springBoot2-ubi-min image and will monitor DockerHub for any updates. - A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image - A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub (with the URL provided as a parameter when the application is created) Issue the following commands to create the application from the template: oc new - app gse - springboot - build - p APPLICATION_NAME = petclinic - liberty - p SOURCE_URL = \" https://github.com/ibm-cloud-architecture/cloudpak-for-applications \" - n petclinic - liberty - build Run the pipeline The newly created pipeline can be started from the Red Hat OpenShift console which allows access to the Jenkins logs but also tracks the progress in the OCP console. Navigate to Application Console --> Pet Clinic on Liberty - Build --> Builds --> Pipelines and click the Start Pipeline button When the pipeline starts, click the view log link to go to the Jenkins administration console. Note that it may take a couple of minutes before the view log link appears on the first pipeline build When prompted, log in with your OpenShift account and grant the required access permissions. The Jenkins console log will be displayed as shown below: Return to the OpenShift Console and track the progress of the pipeline The pipeline will eventually stop at the Promotion Gate for approval to deploy to Production. Click the Input Required link as shown below When the Promote application to Production question is displayed, click Proceed Return to the OpenShift Console and validate that the pipeline is now complete Validate the Application Now that the pipeline is complete, validate the Pet Clinic application is deployed and running in dev , stage and prod In the OpenShift Console, navigate to Application Console --> Pet Clinic on Liberty - Dev --> Applications --> Deployments and click on the link in the Latest Version column Information about the deployment will be displayed including the image that is being used (note the tag on the image as it will be the same in the stage and prod deployments). After a few minutes the container will be marked as ready Click Applications --> Routes and click on the route for the application. Note that the URL is < application_name >-< project_name >.< ocp cluster url >. In this case the project name is petclinic-liberty-dev The application home page will be displayed. Click Find Owners and then Find Owner to view a list of owners from the database. Repeat the validations for the stage and prod Projects. Summary This application has been modified from the initial Spring Framework version to Spring Boot v2 to run on Open Liberty and deployed by the IBM CloudPak for Applications.","title":"Spring modernization"},{"location":"spring/#spring-modernization","text":"Spring modernization describes the process of upgrading existing Spring Framework and Spring Boot v1 applications to use Spring Boot v2 . Pivotal have made some changes to the list of supported versions of Spring that may require modernization of existing applications to Spring Boot v2. With the Open Liberty project, IBM have provided support for Spring Boot and made optimizations to the runtime and Docker images specifically for Spring Boot . There are some benefits of running Spring Boot applications on the Open Liberty runtime: Performance. Benchmarks have shows that Liberty perform better than Tomcat on both throughput and response time. Size. The memory footprint of Liberty is smaller than tomcat, but more importantly the Spring Boot libraries can be separated from the runtime libraries. When we look at the way Docker builds it's images using layers, the application portion is much smaller if we build out images in the optimized way. This means faster build time, and if you're storing every version of your application you build, the delta between versions is much smaller Support. If you're running on OpenShift using IBM Cloud Paks, using the Liberty runtime is included in your licensing model, so using Liberty comes at no additional cost, but if need any support for Liberty it's included. If you stick with standard Spring Boot with tomcat, you'll have to either run on an unsupported platform, or pay for additional support for the tomcat runtime. Consistent runtime model. Running Liberty has many best practices especially in the Kubernetes/OpenShift world. There are built in metrics and monitoring tools which are specifically designed to be integrated into OpenShift. Using Liberty allows you to leverage many if these automatic connections to better maintain your environment. This repository holds a solution that is the result of a modernization for an Spring application that was upgraded to Spring Boot on Open Liberty and deployed by the IBM CloudPak for Applications to RedHat OpenShift.","title":"Spring Modernization"},{"location":"spring/#application-overview","text":"Pet Clinic is a demonstration application that was created in 2003 to show the features and functions of the Spring Framework. A description of the original application can be found in the The Pet Clinic Application section of the readme","title":"Application Overview"},{"location":"spring/#how-the-application-was-modernized","text":"In order to modernize the application from the Spring Framework to Spring Boot and Open Liberty running on OpenShift, the application went through code changes , build and deploy phases.","title":"How the Application was Modernized"},{"location":"spring/#code-changes","text":"Pet Clinic was modernized by the open source community in 2013 from Spring Framework 2.5 to Spring Framework 3.0 and then in 2016 to Spring Boot. The process of making the code changes is outside of the scope of this document, however the general modernization process is described below: Spring Framework Upgrades Spring Boot Upgrades Spring Framework to Spring Boot","title":"Code Changes"},{"location":"spring/#build","text":"The build phase created the Dockerfile for the application. A Spring Boot application JAR or WAR file is a self-contained artifact. It packages all of the application dependencies inside the final artifact alongside the application content, including an embedded server implementation, such as Tomcat, Jetty, or Undertow. The result is a fat artifact that is easy to run on any server that has a JVM. It also results in a large artifact, even for the smallest hello world Spring Boot web application. The Pet Clinic Dockerfile for the self-contained jar is shown below: FROM openliberty / open - liberty : springBoot2 - ubi - min COPY --chown=1001:0 spring-petclinic/target/spring-petclinic-2.1.0.BUILD-SNAPSHOT.jar /config/dropins/spring/ In order to optimize the Docker image, the Dual Layer Approach that IBM has created is used and resulted in the Dockerfile shown below: FROM openliberty / open - liberty : springBoot2 - ubi - min as staging USER root COPY spring - petclinic / target / spring - petclinic - 2 . 1 . 0 . BUILD - SNAPSHOT . jar / staging / fatClinic . jar RUN springBootUtility thin \\ --sourceAppPath=/staging/fatClinic.jar \\ --targetThinAppPath=/staging/thinClinic.jar \\ --targetLibCachePath=/staging/lib.index.cache FROM openliberty / open - liberty : springBoot2 - ubi - min USER root COPY --from=staging /staging/lib.index.cache /opt/ol/wlp/usr/shared/resources/lib.index.cache COPY --from=staging /staging/thinClinic.jar /config/dropins/spring/thinClinic.jar RUN chown - R 1001 . 0 / config && chmod - R g + rw / config RUN chown - R 1001 . 0 / opt / ol / wlp / usr / shared / resources / lib . index . cache && chmod - R g + rw / opt / ol / wlp / usr / shared / resources / lib . index . cache USER 1001 The final file can be found here: Dockerfile The containerized application was tested locally before the code and configuration files were committed to the git repository","title":"Build"},{"location":"spring/#deploy","text":"The deploy phase created the Jenkins, Kubernetes and Red Hat OpenShift artifacts required to automate the build and deployment pipeline for the application. For illustration purposes, the application was deployed to three different Red Hat OpenShift projects to simulate development , staging and production . The diagram below shows the flow through the pipeline. A more detailed description can be found here The steps were: Configure the Red Hat OpenShift Cluster for WebSphere by creating the necessary SecurityContextConstraints definition. The file can be found here: scc.yaml Create the Red Hat OpenShift build template that would be used to define the Red Hat OpenShift artifacts related to the build process including ImageStream and BuildConfig definitions. The file can be found here: template-libery-build.yaml Create the Red Hat OpenShift deployment template that would be used to define the Red Hat OpenShift artifacts related to the Pet Clinic application including DeploymentConfig , Service and Route definitions. The file can be found here: template-libery-deploy.yaml Create the Jenkins Jenkinsfile for the pipeline. The Jenkinsfile defines the steps that the pipeline takes to build the Pet Clinic application, create an immutable Docker Image and then move the image through the dev , stage and prod environments. The file can be found here: Jenkinsfile Create the build project, load the build template and configure Jenkins Create the dev , stage and prod projects and load the deployment template Verify the pipeline. Detailed, step-by-step instructions on how to replicate these steps are provided here","title":"Deploy"},{"location":"spring/#deploy-the-application","text":"The following steps will deploy the modernized Pet Clinic application in a Open Liberty container to a Red Hat OpenShift cluster.","title":"Deploy the Application"},{"location":"spring/#prerequisites","text":"You will need the following: Git CLI Red Hat OpenShift 3.11 with Cluster Admin permissions oc CLI DB2 Database","title":"Prerequisites"},{"location":"spring/#getting-the-project-repository","text":"You can clone the repository from its main GitHub repository page and checkout the appropriate branch for this version of the application. git clone https : // github . com / ibm - cloud - architecture / cloudpak - for - applications . git cd cloudpak - for - applications git checkout spring","title":"Getting the project repository"},{"location":"spring/#create-the-security-context-constraint","text":"In order to deploy and run the Open Liberty Docker image in an OpenShift cluster, we first need to configure certain security aspects for the cluster. The Security Context Constraint provided here grants the service account that the Open Liberty Docker container is running under the required privileges to function correctly. A cluster administrator can use the file provided here with the following command to create the Security Context Constraint (SCC): cd Deployment / OpenShift oc apply - f ssc . yaml","title":"Create the Security Context Constraint"},{"location":"spring/#create-the-projects","text":"Four Red Hat OpenShift projects are required in this scenario: - Build: this project will contain the Jenkins server and the artifacts used to build the application image - Dev: this is the development environment for this application - Stage: this is the staging environment for this application - Prod: this is the production environment for this application The file provided here contains the definitions for the four projects in a single file to make creation easier Issue the command shown below to create the projects oc create - f liberty - projects . yaml","title":"Create the projects"},{"location":"spring/#create-a-service-account","text":"It is a good Kubernetes practice to create a service account for your applications. A service account provides an identity for processes that run in a Pod. In this step we will create a new service account with the name websphere in each of the dev , stage and prod projects and add the Security Context Constraint created above to them. Issue the commands shown below to create the websphere service account and bind the ibm-websphere-scc to it in each of the projects: oc create serviceaccount websphere - n petclinic - liberty - dev oc create serviceaccount websphere - n petclinic - liberty - stage oc create serviceaccount websphere - n petclinic - liberty - prod oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n petclinic - liberty - dev oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n petclinic - liberty - stage oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n petclinic - liberty - prod","title":"Create a service account"},{"location":"spring/#deploy-jenkins","text":"Some Red Hat OpenShift clusters are configured to automatically provision a Jenkins instance in a build project. The steps below can be used if your cluster is not configured for automatic Jenkins provisioning: oc project petclinic - liberty - build oc new - app jenkins - persistent","title":"Deploy Jenkins"},{"location":"spring/#update-the-jenkins-service-account","text":"During provisioning of the Jenkins master a service account with the name jenkins is created. This service account has privileges to create new artifacts only in the project that it is running in. In this scenario Jenkins will need to create artifacts in the dev , stage and prod projects. Issue the commands below to allow the jenkins service account to edit artifacts in the dev , stage and prod projects. oc policy add - role - to - user edit system : serviceaccount : petclinic - liberty - build : jenkins - n petclinic - liberty - dev oc policy add - role - to - user edit system : serviceaccount : petclinic - liberty - build : jenkins - n petclinic - liberty - stage oc policy add - role - to - user edit system : serviceaccount : petclinic - liberty - build : jenkins - n petclinic - liberty - prod","title":"Update the Jenkins service account"},{"location":"spring/#import-the-deployment-templates","text":"Red Hat OpenShift templates are used to make artifact creation easier and repeatable. The template definition provided here defines a Kubernetes Service , Route and DeploymentConfig for the CustomerOrderServices application. The gse-spring-deploy template defines the following: - service listening on ports 9080 , 9443 and 9082 - route to expose the 9443 port externally - DeploymentConfig to host the Open Liberty container. - The image for the container is taken from the ImageStream that will be populated by the Jenkins pipeline. - environment variables are defined for the DB2 database used by the application allowing for environment specific information to be injected - Probes for liveness and readiness are defined to check port 9443 is active - The securityContext is set to allow read/write access to the filesystem and to run the container as user 1001 - The deployment will be updated if a new image is loaded to the ImageStream or if a change to the configuration is detected. Issue the commands below to load the template named gse-spring-deploy in the dev , stage and prod projects. oc create - f template - liberty - deploy . yaml - n petclinic - liberty - dev oc create - f template - liberty - deploy . yaml - n petclinic - liberty - stage oc create - f template - liberty - deploy . yaml - n petclinic - liberty - prod","title":"Import the deployment templates"},{"location":"spring/#create-the-deployment-definitions","text":"In this step the gse-spring-deploy template will be used to create a Red Hat OpenShift application named petclinic-liberty in the dev , stage and prod namespaces. The result will be: - service listening on ports 9080 , 9443 and 9082 - route to expose the 9443 port externally - DeploymentConfig to host the Open Liberty container. The deployment config will wait for a docker image to be loaded in to the ImageStream by the Jenkins pipeline. Issue the following commands to create the applications from the template: oc new - app gse - spring - deploy - p APPLICATION_NAME = petclinic - liberty - n petclinic - liberty - dev oc new - app gse - spring - deploy - p APPLICATION_NAME = petclinic - liberty - n petclinic - liberty - stage oc new - app gse - spring - deploy - p APPLICATION_NAME = petclinic - liberty - n petclinic - liberty - prod","title":"Create the deployment definitions"},{"location":"spring/#import-the-build-templates","text":"In this step a template for the build process will be loaded in to the build project. The template provided here defines the following artifacts: An ImageStream for the application image. This will be populated by the Jenkins Pipeline An ImageStream for Open Liberty which will pull down the latest version of the openliberty/open-liberty:springBoot2-ubi-min image and will monitor DockerHub for any updates. A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub Parameters to allow the Open Liberty image and GitHub repository to be provided when the template is instantiated Issue the commands below to load the template named gse-springboot-build in the build projects. oc create - f template - liberty - build . yaml - n petclinic - liberty - build","title":"Import the build templates"},{"location":"spring/#create-the-build-definitions","text":"In this step the gse-springboot-build template will be used to create a Red Hat OpenShift application named petclinic-liberty in the build namespaces. The result will be: - An ImageStream for the application image. This will be populated by the Jenkins Pipeline - An ImageStream for Open Liberty which will pull down the latest version of the openliberty/open-liberty:springBoot2-ubi-min image and will monitor DockerHub for any updates. - A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image - A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub (with the URL provided as a parameter when the application is created) Issue the following commands to create the application from the template: oc new - app gse - springboot - build - p APPLICATION_NAME = petclinic - liberty - p SOURCE_URL = \" https://github.com/ibm-cloud-architecture/cloudpak-for-applications \" - n petclinic - liberty - build","title":"Create the build definitions"},{"location":"spring/#run-the-pipeline","text":"The newly created pipeline can be started from the Red Hat OpenShift console which allows access to the Jenkins logs but also tracks the progress in the OCP console. Navigate to Application Console --> Pet Clinic on Liberty - Build --> Builds --> Pipelines and click the Start Pipeline button When the pipeline starts, click the view log link to go to the Jenkins administration console. Note that it may take a couple of minutes before the view log link appears on the first pipeline build When prompted, log in with your OpenShift account and grant the required access permissions. The Jenkins console log will be displayed as shown below: Return to the OpenShift Console and track the progress of the pipeline The pipeline will eventually stop at the Promotion Gate for approval to deploy to Production. Click the Input Required link as shown below When the Promote application to Production question is displayed, click Proceed Return to the OpenShift Console and validate that the pipeline is now complete","title":"Run the pipeline"},{"location":"spring/#validate-the-application","text":"Now that the pipeline is complete, validate the Pet Clinic application is deployed and running in dev , stage and prod In the OpenShift Console, navigate to Application Console --> Pet Clinic on Liberty - Dev --> Applications --> Deployments and click on the link in the Latest Version column Information about the deployment will be displayed including the image that is being used (note the tag on the image as it will be the same in the stage and prod deployments). After a few minutes the container will be marked as ready Click Applications --> Routes and click on the route for the application. Note that the URL is < application_name >-< project_name >.< ocp cluster url >. In this case the project name is petclinic-liberty-dev The application home page will be displayed. Click Find Owners and then Find Owner to view a list of owners from the database. Repeat the validations for the stage and prod Projects.","title":"Validate the Application"},{"location":"spring/#summary","text":"This application has been modified from the initial Spring Framework version to Spring Boot v2 to run on Open Liberty and deployed by the IBM CloudPak for Applications.","title":"Summary"},{"location":"spring/spring-build/","text":"Spring Modernization Spring modernization describes the process of upgrading existing Spring Framework and Spring Boot v1 applications to use Spring Boot v2 . Pivotal have made some changes to the list of supported versions of Spring that may require modernization of existing applications to Spring Boot v2. With the Open Liberty project, IBM have provided support for Spring Boot and made optimizations to the runtime and Docker images specifically for Spring Boot . There are some benefits of running Spring Boot applications on the Open Liberty runtime: Performance. Benchmarks have shows that Liberty perform better than Tomcat on both throughput and response time. Size. The memory footprint of Liberty is smaller than tomcat, but more importantly the Spring Boot libraries can be separated from the runtime libraries. When we look at the way Docker builds it's images using layers, the application portion is much smaller if we build out images in the optimized way. This means faster build time, and if you're storing every version of your application you build, the delta between versions is much smaller Support. If you're running on OpenShift using IBM Cloud Paks, using the Liberty runtime is included in your licensing model, so using Liberty comes at no additional cost, but if need any support for Liberty it's included. If you stick with standard Spring Boot with tomcat, you'll have to either run on an unsupported platform, or pay for additional support for the tomcat runtime. Consistent runtime model. Running Liberty has many best practices especially in the Kubernetes/OpenShift world. There are built in metrics and monitoring tools which are specifically designed to be integrated into OpenShift. Using Liberty allows you to leverage many if these automatic connections to better maintain your environment. This repository holds a solution that is the result of a modernization for an Spring application that was upgraded to Spring Boot on Open Liberty and deployed by the IBM CloudPak for Applications to RedHat OpenShift. Table of Contents Application Overview How the Application was Modernized Code Changes Build Deploy Deploy the Application Getting the project repository Create the Security Context Constraint Create the projects Create a service account Deploy Jenkins Update the Jenkins service account Import the deployment templates Create the deployment definitions Import the build templates Create the build definitions Run the pipeline Validate the Application Summary Application Overview Pet Clinic is a demonstration application that was created in 2003 to show the features and functions of the Spring Framework. A description of the original application can be found in the The Pet Clinic Application section of the readme How the Application was Modernized In order to modernize the application from the Spring Framework to Spring Boot and Open Liberty running on OpenShift, the application went through code changes , build and deploy phases. Code Changes Pet Clinic was modernized by the open source community in 2013 from Spring Framework 2.5 to Spring Framework 3.0 and then in 2016 to Spring Boot. The process of making the code changes is outside of the scope of this document, however the general modernization process is described below: Spring Framework Upgrades Spring Boot Upgrades Spring Framework to Spring Boot Build The build phase created the Dockerfile for the application. A Spring Boot application JAR or WAR file is a self-contained artifact. It packages all of the application dependencies inside the final artifact alongside the application content, including an embedded server implementation, such as Tomcat, Jetty, or Undertow. The result is a fat artifact that is easy to run on any server that has a JVM. It also results in a large artifact, even for the smallest hello world Spring Boot web application. The Pet Clinic Dockerfile for the self-contained jar is shown below: FROM openliberty / open - liberty : springBoot2 - ubi - min COPY --chown=1001:0 spring-petclinic/target/spring-petclinic-2.1.0.BUILD-SNAPSHOT.jar /config/dropins/spring/ In order to optimize the Docker image, the Dual Layer Approach that IBM has created is used and resulted in the Dockerfile shown below: FROM openliberty / open - liberty : springBoot2 - ubi - min as staging USER root COPY spring - petclinic / target / spring - petclinic - 2 . 1 . 0 . BUILD - SNAPSHOT . jar / staging / fatClinic . jar RUN springBootUtility thin \\ --sourceAppPath=/staging/fatClinic.jar \\ --targetThinAppPath=/staging/thinClinic.jar \\ --targetLibCachePath=/staging/lib.index.cache FROM openliberty / open - liberty : springBoot2 - ubi - min USER root COPY --from=staging /staging/lib.index.cache /opt/ol/wlp/usr/shared/resources/lib.index.cache COPY --from=staging /staging/thinClinic.jar /config/dropins/spring/thinClinic.jar RUN chown - R 1001 . 0 / config && chmod - R g + rw / config RUN chown - R 1001 . 0 / opt / ol / wlp / usr / shared / resources / lib . index . cache && chmod - R g + rw / opt / ol / wlp / usr / shared / resources / lib . index . cache USER 1001 The final file can be found here: Dockerfile The containerized application was tested locally before the code and configuration files were committed to the git repository Deploy The deploy phase created the Jenkins, Kubernetes and Red Hat OpenShift artifacts required to automate the build and deployment pipeline for the application. For illustration purposes, the application was deployed to three different Red Hat OpenShift projects to simulate development , staging and production . The diagram below shows the flow through the pipeline. A more detailed description can be found here The steps were: Configure the Red Hat OpenShift Cluster for WebSphere by creating the necessary SecurityContextConstraints definition. The file can be found here: scc.yaml Create the Red Hat OpenShift build template that would be used to define the Red Hat OpenShift artifacts related to the build process including ImageStream and BuildConfig definitions. The file can be found here: template-libery-build.yaml Create the Red Hat OpenShift deployment template that would be used to define the Red Hat OpenShift artifacts related to the Pet Clinic application including DeploymentConfig , Service and Route definitions. The file can be found here: template-libery-deploy.yaml Create the Jenkins Jenkinsfile for the pipeline. The Jenkinsfile defines the steps that the pipeline takes to build the Pet Clinic application, create an immutable Docker Image and then move the image through the dev , stage and prod environments. The file can be found here: Jenkinsfile Create the build project, load the build template and configure Jenkins Create the dev , stage and prod projects and load the deployment template Verify the pipeline. Detailed, step-by-step instructions on how to replicate these steps are provided here Deploy the Application The following steps will deploy the modernized Pet Clinic application in a Open Liberty container to a Red Hat OpenShift cluster. Prerequisites You will need the following: Git CLI Red Hat OpenShift 3.11 with Cluster Admin permissions oc CLI DB2 Database Getting the project repository You can clone the repository from its main GitHub repository page and checkout the appropriate branch for this version of the application. git clone https : // github . com / ibm - cloud - architecture / cloudpak - for - applications . git cd cloudpak - for - applications git checkout spring Create the Security Context Constraint In order to deploy and run the Open Liberty Docker image in an OpenShift cluster, we first need to configure certain security aspects for the cluster. The Security Context Constraint provided here grants the service account that the Open Liberty Docker container is running under the required privileges to function correctly. A cluster administrator can use the file provided here with the following command to create the Security Context Constraint (SCC): cd Deployment / OpenShift oc apply - f ssc . yaml Create the projects Four Red Hat OpenShift projects are required in this scenario: - Build: this project will contain the Jenkins server and the artifacts used to build the application image - Dev: this is the development environment for this application - Stage: this is the staging environment for this application - Prod: this is the production environment for this application The file provided here contains the definitions for the four projects in a single file to make creation easier Issue the command shown below to create the projects oc create - f liberty - projects . yaml Create a service account It is a good Kubernetes practice to create a service account for your applications. A service account provides an identity for processes that run in a Pod. In this step we will create a new service account with the name websphere in each of the dev , stage and prod projects and add the Security Context Constraint created above to them. Issue the commands shown below to create the websphere service account and bind the ibm-websphere-scc to it in each of the projects: oc create serviceaccount websphere - n petclinic - liberty - dev oc create serviceaccount websphere - n petclinic - liberty - stage oc create serviceaccount websphere - n petclinic - liberty - prod oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n petclinic - liberty - dev oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n petclinic - liberty - stage oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n petclinic - liberty - prod Deploy Jenkins Some Red Hat OpenShift clusters are configured to automatically provision a Jenkins instance in a build project. The steps below can be used if your cluster is not configured for automatic Jenkins provisioning: oc project petclinic - liberty - build oc new - app jenkins - persistent Update the Jenkins service account During provisioning of the Jenkins master a service account with the name jenkins is created. This service account has privileges to create new artifacts only in the project that it is running in. In this scenario Jenkins will need to create artifacts in the dev , stage and prod projects. Issue the commands below to allow the jenkins service account to edit artifacts in the dev , stage and prod projects. oc policy add - role - to - user edit system : serviceaccount : petclinic - liberty - build : jenkins - n petclinic - liberty - dev oc policy add - role - to - user edit system : serviceaccount : petclinic - liberty - build : jenkins - n petclinic - liberty - stage oc policy add - role - to - user edit system : serviceaccount : petclinic - liberty - build : jenkins - n petclinic - liberty - prod Import the deployment templates Red Hat OpenShift templates are used to make artifact creation easier and repeatable. The template definition provided here defines a Kubernetes Service , Route and DeploymentConfig for the CustomerOrderServices application. The gse-spring-deploy template defines the following: - service listening on ports 9080 , 9443 and 9082 - route to expose the 9443 port externally - DeploymentConfig to host the Open Liberty container. - The image for the container is taken from the ImageStream that will be populated by the Jenkins pipeline. - environment variables are defined for the DB2 database used by the application allowing for environment specific information to be injected - Probes for liveness and readiness are defined to check port 9443 is active - The securityContext is set to allow read/write access to the filesystem and to run the container as user 1001 - The deployment will be updated if a new image is loaded to the ImageStream or if a change to the configuration is detected. Issue the commands below to load the template named gse-spring-deploy in the dev , stage and prod projects. oc create - f template - liberty - deploy . yaml - n petclinic - liberty - dev oc create - f template - liberty - deploy . yaml - n petclinic - liberty - stage oc create - f template - liberty - deploy . yaml - n petclinic - liberty - prod Create the deployment definitions In this step the gse-spring-deploy template will be used to create a Red Hat OpenShift application named petclinic-liberty in the dev , stage and prod namespaces. The result will be: - service listening on ports 9080 , 9443 and 9082 - route to expose the 9443 port externally - DeploymentConfig to host the Open Liberty container. The deployment config will wait for a docker image to be loaded in to the ImageStream by the Jenkins pipeline. Issue the following commands to create the applications from the template: oc new - app gse - spring - deploy - p APPLICATION_NAME = petclinic - liberty - n petclinic - liberty - dev oc new - app gse - spring - deploy - p APPLICATION_NAME = petclinic - liberty - n petclinic - liberty - stage oc new - app gse - spring - deploy - p APPLICATION_NAME = petclinic - liberty - n petclinic - liberty - prod Import the build templates In this step a template for the build process will be loaded in to the build project. The template provided here defines the following artifacts: An ImageStream for the application image. This will be populated by the Jenkins Pipeline An ImageStream for Open Liberty which will pull down the latest version of the openliberty/open-liberty:springBoot2-ubi-min image and will monitor DockerHub for any updates. A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub Parameters to allow the Open Liberty image and GitHub repository to be provided when the template is instantiated Issue the commands below to load the template named gse-springboot-build in the build projects. oc create - f template - liberty - build . yaml - n petclinic - liberty - build Create the build definitions In this step the gse-springboot-build template will be used to create a Red Hat OpenShift application named petclinic-liberty in the build namespaces. The result will be: - An ImageStream for the application image. This will be populated by the Jenkins Pipeline - An ImageStream for Open Liberty which will pull down the latest version of the openliberty/open-liberty:springBoot2-ubi-min image and will monitor DockerHub for any updates. - A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image - A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub (with the URL provided as a parameter when the application is created) Issue the following commands to create the application from the template: oc new - app gse - springboot - build - p APPLICATION_NAME = petclinic - liberty - p SOURCE_URL = \" https://github.com/ibm-cloud-architecture/cloudpak-for-applications \" - n petclinic - liberty - build Run the pipeline The newly created pipeline can be started from the Red Hat OpenShift console which allows access to the Jenkins logs but also tracks the progress in the OCP console. Navigate to Application Console --> Pet Clinic on Liberty - Build --> Builds --> Pipelines and click the Start Pipeline button When the pipeline starts, click the view log link to go to the Jenkins administration console. Note that it may take a couple of minutes before the view log link appears on the first pipeline build When prompted, log in with your OpenShift account and grant the required access permissions. The Jenkins console log will be displayed as shown below: Return to the OpenShift Console and track the progress of the pipeline The pipeline will eventually stop at the Promotion Gate for approval to deploy to Production. Click the Input Required link as shown below When the Promote application to Production question is displayed, click Proceed Return to the OpenShift Console and validate that the pipeline is now complete Validate the Application Now that the pipeline is complete, validate the Pet Clinic application is deployed and running in dev , stage and prod In the OpenShift Console, navigate to Application Console --> Pet Clinic on Liberty - Dev --> Applications --> Deployments and click on the link in the Latest Version column Information about the deployment will be displayed including the image that is being used (note the tag on the image as it will be the same in the stage and prod deployments). After a few minutes the container will be marked as ready Click Applications --> Routes and click on the route for the application. Note that the URL is < application_name >-< project_name >.< ocp cluster url >. In this case the project name is petclinic-liberty-dev The application home page will be displayed. Click Find Owners and then Find Owner to view a list of owners from the database. Repeat the validations for the stage and prod Projects. Summary This application has been modified from the initial Spring Framework version to Spring Boot v2 to run on Open Liberty and deployed by the IBM CloudPak for Applications.","title":"Spring Modernization"},{"location":"spring/spring-build/#spring-modernization","text":"Spring modernization describes the process of upgrading existing Spring Framework and Spring Boot v1 applications to use Spring Boot v2 . Pivotal have made some changes to the list of supported versions of Spring that may require modernization of existing applications to Spring Boot v2. With the Open Liberty project, IBM have provided support for Spring Boot and made optimizations to the runtime and Docker images specifically for Spring Boot . There are some benefits of running Spring Boot applications on the Open Liberty runtime: Performance. Benchmarks have shows that Liberty perform better than Tomcat on both throughput and response time. Size. The memory footprint of Liberty is smaller than tomcat, but more importantly the Spring Boot libraries can be separated from the runtime libraries. When we look at the way Docker builds it's images using layers, the application portion is much smaller if we build out images in the optimized way. This means faster build time, and if you're storing every version of your application you build, the delta between versions is much smaller Support. If you're running on OpenShift using IBM Cloud Paks, using the Liberty runtime is included in your licensing model, so using Liberty comes at no additional cost, but if need any support for Liberty it's included. If you stick with standard Spring Boot with tomcat, you'll have to either run on an unsupported platform, or pay for additional support for the tomcat runtime. Consistent runtime model. Running Liberty has many best practices especially in the Kubernetes/OpenShift world. There are built in metrics and monitoring tools which are specifically designed to be integrated into OpenShift. Using Liberty allows you to leverage many if these automatic connections to better maintain your environment. This repository holds a solution that is the result of a modernization for an Spring application that was upgraded to Spring Boot on Open Liberty and deployed by the IBM CloudPak for Applications to RedHat OpenShift.","title":"Spring Modernization"},{"location":"spring/spring-build/#table-of-contents","text":"Application Overview How the Application was Modernized Code Changes Build Deploy Deploy the Application Getting the project repository Create the Security Context Constraint Create the projects Create a service account Deploy Jenkins Update the Jenkins service account Import the deployment templates Create the deployment definitions Import the build templates Create the build definitions Run the pipeline Validate the Application Summary","title":"Table of Contents"},{"location":"spring/spring-build/#application-overview","text":"Pet Clinic is a demonstration application that was created in 2003 to show the features and functions of the Spring Framework. A description of the original application can be found in the The Pet Clinic Application section of the readme","title":"Application Overview"},{"location":"spring/spring-build/#how-the-application-was-modernized","text":"In order to modernize the application from the Spring Framework to Spring Boot and Open Liberty running on OpenShift, the application went through code changes , build and deploy phases.","title":"How the Application was Modernized"},{"location":"spring/spring-build/#code-changes","text":"Pet Clinic was modernized by the open source community in 2013 from Spring Framework 2.5 to Spring Framework 3.0 and then in 2016 to Spring Boot. The process of making the code changes is outside of the scope of this document, however the general modernization process is described below: Spring Framework Upgrades Spring Boot Upgrades Spring Framework to Spring Boot","title":"Code Changes"},{"location":"spring/spring-build/#build","text":"The build phase created the Dockerfile for the application. A Spring Boot application JAR or WAR file is a self-contained artifact. It packages all of the application dependencies inside the final artifact alongside the application content, including an embedded server implementation, such as Tomcat, Jetty, or Undertow. The result is a fat artifact that is easy to run on any server that has a JVM. It also results in a large artifact, even for the smallest hello world Spring Boot web application. The Pet Clinic Dockerfile for the self-contained jar is shown below: FROM openliberty / open - liberty : springBoot2 - ubi - min COPY --chown=1001:0 spring-petclinic/target/spring-petclinic-2.1.0.BUILD-SNAPSHOT.jar /config/dropins/spring/ In order to optimize the Docker image, the Dual Layer Approach that IBM has created is used and resulted in the Dockerfile shown below: FROM openliberty / open - liberty : springBoot2 - ubi - min as staging USER root COPY spring - petclinic / target / spring - petclinic - 2 . 1 . 0 . BUILD - SNAPSHOT . jar / staging / fatClinic . jar RUN springBootUtility thin \\ --sourceAppPath=/staging/fatClinic.jar \\ --targetThinAppPath=/staging/thinClinic.jar \\ --targetLibCachePath=/staging/lib.index.cache FROM openliberty / open - liberty : springBoot2 - ubi - min USER root COPY --from=staging /staging/lib.index.cache /opt/ol/wlp/usr/shared/resources/lib.index.cache COPY --from=staging /staging/thinClinic.jar /config/dropins/spring/thinClinic.jar RUN chown - R 1001 . 0 / config && chmod - R g + rw / config RUN chown - R 1001 . 0 / opt / ol / wlp / usr / shared / resources / lib . index . cache && chmod - R g + rw / opt / ol / wlp / usr / shared / resources / lib . index . cache USER 1001 The final file can be found here: Dockerfile The containerized application was tested locally before the code and configuration files were committed to the git repository","title":"Build"},{"location":"spring/spring-build/#deploy","text":"The deploy phase created the Jenkins, Kubernetes and Red Hat OpenShift artifacts required to automate the build and deployment pipeline for the application. For illustration purposes, the application was deployed to three different Red Hat OpenShift projects to simulate development , staging and production . The diagram below shows the flow through the pipeline. A more detailed description can be found here The steps were: Configure the Red Hat OpenShift Cluster for WebSphere by creating the necessary SecurityContextConstraints definition. The file can be found here: scc.yaml Create the Red Hat OpenShift build template that would be used to define the Red Hat OpenShift artifacts related to the build process including ImageStream and BuildConfig definitions. The file can be found here: template-libery-build.yaml Create the Red Hat OpenShift deployment template that would be used to define the Red Hat OpenShift artifacts related to the Pet Clinic application including DeploymentConfig , Service and Route definitions. The file can be found here: template-libery-deploy.yaml Create the Jenkins Jenkinsfile for the pipeline. The Jenkinsfile defines the steps that the pipeline takes to build the Pet Clinic application, create an immutable Docker Image and then move the image through the dev , stage and prod environments. The file can be found here: Jenkinsfile Create the build project, load the build template and configure Jenkins Create the dev , stage and prod projects and load the deployment template Verify the pipeline. Detailed, step-by-step instructions on how to replicate these steps are provided here","title":"Deploy"},{"location":"spring/spring-build/#deploy-the-application","text":"The following steps will deploy the modernized Pet Clinic application in a Open Liberty container to a Red Hat OpenShift cluster.","title":"Deploy the Application"},{"location":"spring/spring-build/#prerequisites","text":"You will need the following: Git CLI Red Hat OpenShift 3.11 with Cluster Admin permissions oc CLI DB2 Database","title":"Prerequisites"},{"location":"spring/spring-build/#getting-the-project-repository","text":"You can clone the repository from its main GitHub repository page and checkout the appropriate branch for this version of the application. git clone https : // github . com / ibm - cloud - architecture / cloudpak - for - applications . git cd cloudpak - for - applications git checkout spring","title":"Getting the project repository"},{"location":"spring/spring-build/#create-the-security-context-constraint","text":"In order to deploy and run the Open Liberty Docker image in an OpenShift cluster, we first need to configure certain security aspects for the cluster. The Security Context Constraint provided here grants the service account that the Open Liberty Docker container is running under the required privileges to function correctly. A cluster administrator can use the file provided here with the following command to create the Security Context Constraint (SCC): cd Deployment / OpenShift oc apply - f ssc . yaml","title":"Create the Security Context Constraint"},{"location":"spring/spring-build/#create-the-projects","text":"Four Red Hat OpenShift projects are required in this scenario: - Build: this project will contain the Jenkins server and the artifacts used to build the application image - Dev: this is the development environment for this application - Stage: this is the staging environment for this application - Prod: this is the production environment for this application The file provided here contains the definitions for the four projects in a single file to make creation easier Issue the command shown below to create the projects oc create - f liberty - projects . yaml","title":"Create the projects"},{"location":"spring/spring-build/#create-a-service-account","text":"It is a good Kubernetes practice to create a service account for your applications. A service account provides an identity for processes that run in a Pod. In this step we will create a new service account with the name websphere in each of the dev , stage and prod projects and add the Security Context Constraint created above to them. Issue the commands shown below to create the websphere service account and bind the ibm-websphere-scc to it in each of the projects: oc create serviceaccount websphere - n petclinic - liberty - dev oc create serviceaccount websphere - n petclinic - liberty - stage oc create serviceaccount websphere - n petclinic - liberty - prod oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n petclinic - liberty - dev oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n petclinic - liberty - stage oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n petclinic - liberty - prod","title":"Create a service account"},{"location":"spring/spring-build/#deploy-jenkins","text":"Some Red Hat OpenShift clusters are configured to automatically provision a Jenkins instance in a build project. The steps below can be used if your cluster is not configured for automatic Jenkins provisioning: oc project petclinic - liberty - build oc new - app jenkins - persistent","title":"Deploy Jenkins"},{"location":"spring/spring-build/#update-the-jenkins-service-account","text":"During provisioning of the Jenkins master a service account with the name jenkins is created. This service account has privileges to create new artifacts only in the project that it is running in. In this scenario Jenkins will need to create artifacts in the dev , stage and prod projects. Issue the commands below to allow the jenkins service account to edit artifacts in the dev , stage and prod projects. oc policy add - role - to - user edit system : serviceaccount : petclinic - liberty - build : jenkins - n petclinic - liberty - dev oc policy add - role - to - user edit system : serviceaccount : petclinic - liberty - build : jenkins - n petclinic - liberty - stage oc policy add - role - to - user edit system : serviceaccount : petclinic - liberty - build : jenkins - n petclinic - liberty - prod","title":"Update the Jenkins service account"},{"location":"spring/spring-build/#import-the-deployment-templates","text":"Red Hat OpenShift templates are used to make artifact creation easier and repeatable. The template definition provided here defines a Kubernetes Service , Route and DeploymentConfig for the CustomerOrderServices application. The gse-spring-deploy template defines the following: - service listening on ports 9080 , 9443 and 9082 - route to expose the 9443 port externally - DeploymentConfig to host the Open Liberty container. - The image for the container is taken from the ImageStream that will be populated by the Jenkins pipeline. - environment variables are defined for the DB2 database used by the application allowing for environment specific information to be injected - Probes for liveness and readiness are defined to check port 9443 is active - The securityContext is set to allow read/write access to the filesystem and to run the container as user 1001 - The deployment will be updated if a new image is loaded to the ImageStream or if a change to the configuration is detected. Issue the commands below to load the template named gse-spring-deploy in the dev , stage and prod projects. oc create - f template - liberty - deploy . yaml - n petclinic - liberty - dev oc create - f template - liberty - deploy . yaml - n petclinic - liberty - stage oc create - f template - liberty - deploy . yaml - n petclinic - liberty - prod","title":"Import the deployment templates"},{"location":"spring/spring-build/#create-the-deployment-definitions","text":"In this step the gse-spring-deploy template will be used to create a Red Hat OpenShift application named petclinic-liberty in the dev , stage and prod namespaces. The result will be: - service listening on ports 9080 , 9443 and 9082 - route to expose the 9443 port externally - DeploymentConfig to host the Open Liberty container. The deployment config will wait for a docker image to be loaded in to the ImageStream by the Jenkins pipeline. Issue the following commands to create the applications from the template: oc new - app gse - spring - deploy - p APPLICATION_NAME = petclinic - liberty - n petclinic - liberty - dev oc new - app gse - spring - deploy - p APPLICATION_NAME = petclinic - liberty - n petclinic - liberty - stage oc new - app gse - spring - deploy - p APPLICATION_NAME = petclinic - liberty - n petclinic - liberty - prod","title":"Create the deployment definitions"},{"location":"spring/spring-build/#import-the-build-templates","text":"In this step a template for the build process will be loaded in to the build project. The template provided here defines the following artifacts: An ImageStream for the application image. This will be populated by the Jenkins Pipeline An ImageStream for Open Liberty which will pull down the latest version of the openliberty/open-liberty:springBoot2-ubi-min image and will monitor DockerHub for any updates. A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub Parameters to allow the Open Liberty image and GitHub repository to be provided when the template is instantiated Issue the commands below to load the template named gse-springboot-build in the build projects. oc create - f template - liberty - build . yaml - n petclinic - liberty - build","title":"Import the build templates"},{"location":"spring/spring-build/#create-the-build-definitions","text":"In this step the gse-springboot-build template will be used to create a Red Hat OpenShift application named petclinic-liberty in the build namespaces. The result will be: - An ImageStream for the application image. This will be populated by the Jenkins Pipeline - An ImageStream for Open Liberty which will pull down the latest version of the openliberty/open-liberty:springBoot2-ubi-min image and will monitor DockerHub for any updates. - A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image - A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub (with the URL provided as a parameter when the application is created) Issue the following commands to create the application from the template: oc new - app gse - springboot - build - p APPLICATION_NAME = petclinic - liberty - p SOURCE_URL = \" https://github.com/ibm-cloud-architecture/cloudpak-for-applications \" - n petclinic - liberty - build","title":"Create the build definitions"},{"location":"spring/spring-build/#run-the-pipeline","text":"The newly created pipeline can be started from the Red Hat OpenShift console which allows access to the Jenkins logs but also tracks the progress in the OCP console. Navigate to Application Console --> Pet Clinic on Liberty - Build --> Builds --> Pipelines and click the Start Pipeline button When the pipeline starts, click the view log link to go to the Jenkins administration console. Note that it may take a couple of minutes before the view log link appears on the first pipeline build When prompted, log in with your OpenShift account and grant the required access permissions. The Jenkins console log will be displayed as shown below: Return to the OpenShift Console and track the progress of the pipeline The pipeline will eventually stop at the Promotion Gate for approval to deploy to Production. Click the Input Required link as shown below When the Promote application to Production question is displayed, click Proceed Return to the OpenShift Console and validate that the pipeline is now complete","title":"Run the pipeline"},{"location":"spring/spring-build/#validate-the-application","text":"Now that the pipeline is complete, validate the Pet Clinic application is deployed and running in dev , stage and prod In the OpenShift Console, navigate to Application Console --> Pet Clinic on Liberty - Dev --> Applications --> Deployments and click on the link in the Latest Version column Information about the deployment will be displayed including the image that is being used (note the tag on the image as it will be the same in the stage and prod deployments). After a few minutes the container will be marked as ready Click Applications --> Routes and click on the route for the application. Note that the URL is < application_name >-< project_name >.< ocp cluster url >. In this case the project name is petclinic-liberty-dev The application home page will be displayed. Click Find Owners and then Find Owner to view a list of owners from the database. Repeat the validations for the stage and prod Projects.","title":"Validate the Application"},{"location":"spring/spring-build/#summary","text":"This application has been modified from the initial Spring Framework version to Spring Boot v2 to run on Open Liberty and deployed by the IBM CloudPak for Applications.","title":"Summary"},{"location":"was90/","text":"Cloud Pak for Applications: Operational Modernization Solution Introduction Operational modernization gives an operations team the opportunity to embrace modern operations best practices without putting change requirements on the development team. Modernizing from WebSphere Network Deployment (ND) to the traditional WebSphere Application Server Base V9 runtime in a container allows the application to be moved to the cloud without code changes. The scaling, routing, clustering, high availability and continuous availability functionality that WebSphere ND was required for previously can be provided by the container runtime and allows the operations team to run cloud-native and older applications in the same environment with the same standardized logging, monitoring and security frameworks. While traditional WebSphere isn't a 'built for the cloud' runtime like WebSphere Liberty, it can still be run in container and will receive the benefits of the consistency and reliability of containers as well as helping to improve DevOps and speed to market. This type of modernization shouldn't require any code changes and can be driven by the operations team. This path gets the application in to a container with the least amount of effort but doesn't modernize the application or the runtime. As organizations modernize to cloud platforms, new technologies and methodologies will be used for build, deployment and management of applications. While this modernization will be focused on cloud-native (built for the cloud) applications, using the traditional WebSphere container will allow common technologies and methodologies to be used regardless of the runtime. The diagram below shows the high level decision flow where IBM Cloud Transformation Advisor is used to analyze existing assets and a decision is made to not make code changes to the application and use the traditional WebSphere container as the target runtime. This repository holds a solution that is the result of a operational modernization for an existing WebSphere Java EE application that was moved from WebSphere ND v8.5.5 to the traditional WebSphere Base v9 container and is deployed by the IBM CloudPak for Applications to RedHat OpenShift. Table of Contents Application Overview How the Application was Modernized Analysis Build Deploy Deploy the Application Getting the project repository Create the Security Context Constraint Create the projects Create a service account Deploy Jenkins Update the Jenkins service account Import the deployment templates Create the deployment definitions Import the build templates Create the build definitions Run the pipeline Validate the Application Summary Application Overview The Customer Order Services application is a simple store-front shopping application, built during the early days of the Web 2.0 movement. Users interact directly with a browser-based interface and manage their cart to submit orders. This application is built using the traditional 3-Tier Architecture model, with an HTTP server, an application server, and a supporting database. There are several components of the overall application architecture: - Starting with the database, the application leverages two SQL-based databases running on IBM DB2 . - The application exposes its data model through an Enterprise JavaBean layer, named CustomerOrderServices . This components leverages the Java Persistence API to exposed the backend data model to calling services with minimal coding effort. - This build of the application uses JavaEE6 features for EJBs and JPA. - The next tier of the application, named CustomerOrderServicesWeb , exposes the necessary business APIs via REST-based web services. This component leverages the JAX-RS libraries for creating Java-based REST services with minimal coding effort. - This build of the application is using JAX-RS 1.1 version of the respective capability. - The application's user interface is exposed through the CustomerOrderServicesWeb component as well, in the form of a Dojo Toolkit -based JavaScript application. Delivering the user interface and business APIs in the same component is one major inhibitor our migration strategy will help to alleviate in the long-term. - Finally, there is an additional integration testing component, named CustomerOrderServicesTest that is built to quickly validate an application's build and deployment to a given application server. This test component contains both JPA and JAX-RS -based tests. How the Application was Modernized In order to modernize the application from WebSphere ND v8.5.5 to the WebSphere Base v9 container running on OpenShift, the application went through analysis , build and deploy phases. Analysis IBM Cloud Transformation Advisor was used to analyze the existing Customer Order Services application and the WebSphere ND runtime. The steps were: Install IBM Cloud Transformation Advisor either in to a Kubernetes Cluster or locally Download and execute the Data Collector against the existing WebSphere ND runtime Upload the results of the data collection in to IBM Cloud Transformation Advisor and review the analysis. A screenshot of the analysis is shown below: In the case of the CustomerOrderServicesApp.ear application, IBM Cloud Transformation Advisor has determined that the migration to WebSphere Traditional on Private Cloud is of Moderate complexity and that there are four Severe Issues that have been detected. Drilling down in to Detailed Migration Analysis Report that is part of the application analysis, it is apparent that IBM Cloud Transformation Advisor has detected that there are issues with JPA, specifically that the second-level cache and the JPA configuration properties must be migrated and with JAX-RS (missing Apache and Wink packages). These issues are related to a decision that was taken by IBM to allow WebSphere Application Server V9 to run in either JPA 2.0 or JPA 2.1 mode as described here and in either JAX-RS 2.0 or JAX-RS 1.1 mode as described here . In order to run in JPA 2.1 mode and JAX-RS 2.0 mode, the changes highlighted by IBM Cloud Transformation Advisor must be made to the application. However, this application can run in JPA 2.0 mode and JAX-RS 1.1 mode with no changes . In summary, no code changes are required to move this application to the traditional WebSphere Base v9 runtime and the decision was taken to proceed with the operational modernization. Detailed, step-by-step instructions on how to replicate these steps are provided here Build The build phase created the traditional WebSphere container configuration artifacts. The steps were: Update the existing wsadmin scripts that currently configure the runtime environment to configure the JPA 2.0 and JAXRS 1.1 engines. The final versions of the file can be found here: cosConfig.py The Dockerfile required to build the immutable Docker Image containing the application and traditional WebSphere was created. The final file can be found here: Dockerfile The containerized application was tested locally before the code and configuration files were committed to the git repository Detailed, step-by-step instructions on how to replicate these steps are provided here Deploy The deploy phase created the Jenkins, Kubernetes and RedHat OpenShift artifacts required to automate the build and deployment pipeline for the application. For illustration purposes, the application was deployed to three different RedHat OpenShift projects to simulate development , staging and production . The diagram below shows the flow through the pipeline. A more detailed description can be found here The steps were: Configure the RedHat OpenShift Cluster for WebSphere by creating the necessary SecurityContextConstraints definition. The file can be found here: scc.yaml Create the RedHat OpenShift build template that would be used to define the RedHat OpenShift artifacts related to the build process including ImageStream and BuildConfig definitions. The file can be found here: template-twas-build.yaml Create the RedHat OpenShift deployment template that would be used to define the RedHat OpenShift artifacts related to the Customer Order Services application including DeploymentConfig , Service and Route definitions. The file can be found here: template-twas-deploy.yaml Create the Jenkins Jenkinsfile for the pipeline. The Jenkinsfile defines the steps that the pipeline takes to build the Customer Order Services application EAR file, create an immutable Docker Image and then move the image through the dev , stage and prod environments. The file can be found here: Jenkinsfile Create the build project, load the build template and configure Jenkins Create the dev , stage and prod projects and load the deployment template Verify the pipeline. Detailed, step-by-step instructions on how to replicate these steps are provided here Deploy the Application The following steps will deploy the modernized Customer Order Services application in a traditional WebSphere container to a RedHat OpenShift cluster. Prerequisites You will need the following: Git CLI RedHat OpenShift 3.11 with Cluster Admin permissions oc CLI DB2 Database Getting the project repository You can clone the repository from its main GitHub repository page and checkout the appropriate branch for this version of the application. git clone https : // github . com / ibm - cloud - architecture / cloudpak - for - applications . git cd cloudpak - for - applications git checkout was90 Create application database infrastructure As said in the prerequisites section above, the Customer Order Services application uses uses DB2 as its database. Follow these steps to create the appropriate database, tables and data the application needs to: Copy the createOrderDB.sql and initialDataSet.sql files you can find in the Common directory of this repository over to the db2 host machine (or git clone the repository) in order to execute them later. Ssh into the db2 host Change to the db2 instance user: `su {database_instance_name}`` Start db2: db2start Create the ORDERDB database: db2 create database ORDERDB Connect to the ORDERDB database: db2 connect to ORDERDB Execute the createOrderDB.sql script you copied over in step 1 in order to create the appropriate tables, relationships, primary keys, etc: db2 -tf createOrderDB.sql Execute the initialDataSet.sql script you copied over in step 1 to populate the ORDERDB database with the needed initial data set: db2 -tf initialDataSet.sql If you want to re-run the scripts, please make sure you drop the databases and create them again. Create the Security Context Constraint In order to deploy and run the WebSphere Base Docker image in an OpenShift cluster, we first need to configure certain security aspects for the cluster. The Security Context Constraint provided here grants the service account that the WebSphere Base Docker container is running under the required privileges to function correctly. A cluster administrator can use the file provided here with the following command to create the Security Context Constraint (SCC): cd Deployment / OpenShift oc apply - f ssc . yaml Create the projects Four RedHat OpenShift projects are required in this scenario: - Build: this project will contain the Jenkins server and the artifacts used to build the application image - Dev: this is the development environment for this application - Stage: this is the staging environment for this application - Prod: this is the production environment for this application The file provided here contains the definitions for the four projects in a single file to make creation easier Issue the command shown below to create the projects oc create - f liberty - projects . yaml Create a service account It is a good Kubernetes practice to create a service account for your applications. A service account provides an identity for processes that run in a Pod. In this step we will create a new service account with the name websphere in each of the dev , stage and prod projects and add the Security Context Constraint created above to them. Issue the commands shown below to create the websphere service account and bind the ibm-websphere-scc to it in each of the projects: oc create serviceaccount websphere - n cos - twas - dev oc create serviceaccount websphere - n cos - twas - stage oc create serviceaccount websphere - n cos - twas - prod oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - twas - dev oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - twas - stage oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - twas - prod Deploy Jenkins Some RedHat OpenShift clusters are configured to automatically provision a Jenkins instance in a build project. The steps below can be used if your cluster is not configured for automatic Jenkins provisioning: oc project cos - liberty - build oc new - app jenkins - persistent Update the Jenkins service account During provisioning of the Jenkins master a service account with the name jenkins is created. This service account has privileges to create new artifacts only in the project that it is running in. In this scenario Jenkins will need to create artifacts in the dev , stage and prod projects. Issue the commands below to allow the jenkins service account to edit artifacts in the dev , stage and prod projects. oc policy add - role - to - user edit system : serviceaccount : cos - twas - build : jenkins - n cos - twas - dev oc policy add - role - to - user edit system : serviceaccount : cos - twas - build : jenkins - n cos - twas - stage oc policy add - role - to - user edit system : serviceaccount : cos - twas - build : jenkins - n cos - twas - prod Import the deployment templates RedHat OpenShift templates are used to make artifact creation easier and repeatable. The template definition provided here defines a Kubernetes Service , Route and DeploymentConfig for the CustomerOrderServices application. The gse-twas-deploy template defines the following: - service listening on ports 9080 , 9443 and 9082 - route to expose the 9443 port externally - DeploymentConfig to host the WebSphere Base container. - The image for the container is taken from the ImageStream that will be populated by the Jenkins pipeline. - environment variables are defined for the DB2 database used by the application allowing for environment specific information to be injected - Probes for liveness and readiness are defined to check port 9443 is active - The securityContext is set to allow read/write access to the filesystem and to run the container as user 1001 - The deployment will be updated if a new image is loaded to the ImageStream or if a change to the configuration is detected. Issue the commands below to load the template named gse-twas-deploy in the dev , stage and prod projects. oc create - f template - twas - deploy . yaml - n cos - twas - dev oc create - f template - twas - deploy . yaml - n cos - twas - stage oc create - f template - twas - deploy . yaml - n cos - twas - prod Create the deployment definitions In this step the gse-twas-deploy template will be used to create a RedHat OpenShift application named cos-twas in the dev , stage and prod namespaces. The result will be: - service listening on ports 9080 , 9443 and 9082 - route to expose the 9443 port externally - DeploymentConfig to host the WebSphere Base container. The deployment config will wait for a docker image to be loaded in to the ImageStream by the Jenkins pipeline. Issue the following commands to create the applications from the template: oc new - app gse - twas - deploy - p APPLICATION_NAME = cos - twas - n cos - twas - dev oc new - app gse - twas - deploy - p APPLICATION_NAME = cos - twas - n cos - twas - stage oc new - app gse - twas - deploy - p APPLICATION_NAME = cos - twas - n cos - twas - prod Import the build templates In this step a template for the build process will be loaded in to the build project. The template provided here defines the following artifacts: An ImageStream for the application image. This will be populated by the Jenkins Pipeline An ImageStream for WebSphere Base which will pull down the latest version of the ibmcom/websphere-traditional:latest-ubi image and will monitor DockerHub for any updates. A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub Parameters to allow the WebSphere Base image and GitHub repository to be provided when the template is instantiated Issue the commands below to load the template named gse-twas-build in the build projects. oc create - f template - twas - build . yaml - n cos - twas - build Create the build definitions In this step the gse-twas-build template will be used to create a RedHat OpenShift application named cos-twas in the build namespaces. The result will be: - An ImageStream for the application image. This will be populated by the Jenkins Pipeline - An ImageStream for WebSphere Base which will pull down the latest version of the ibmcom/websphere-traditional:latest-ubi image and will monitor DockerHub for any updates. - A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image - A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub (with the URL provided as a parameter when the application is created) Issue the following commands to create the application from the template: oc new - app gse - twas - build - p APPLICATION_NAME = cos - twas - p SOURCE_URL = \" https://github.com/ibm-cloud-architecture/cloudpak-for-applications \" - n cos - twas - build Run the pipeline The newly created pipeline can be started from the RedHat OpenShift console which allows access to the Jenkins logs but also tracks the progress in the OCP console. Navigate to Application Console --> Customer Order Services on twas - Build --> Builds --> Pipelines and click the Start Pipeline button When the pipeline starts, click the view log link to go to the Jenkins administration console. Note that it may take a couple of minutes before the view log link appears on the first pipeline build When prompted, log in with your OpenShift account and grant the required access permissions. The Jenkins console log will be displayed as shown below: Return to the OpenShift Console and track the progress of the pipeline The pipeline will eventually stop at the Promotion Gate for approval to deploy to Production. Click the Input Required link as shown below When the Promote application to Production question is displayed, click Proceed Return to the OpenShift Console and validate that the pipeline is now complete Validate the Application Now that the pipeline is complete, validate the Customer Order Services application is deployed and running in dev , stage and prod Now that the pipeline is complete, validate the Customer Order Services application is deployed and running in dev , stage and prod In the OpenShift Console, navigate to Application Console --> Customer Order Services on twas - Dev --> Applications --> Deployments and click on the link in the Latest Version column Information about the deployment will be displayed including the image that is being used (note the tag on the image as it will be the same in the stage and prod deployments). After a few minutes the container will be marked as ready Click Applications --> Routes and click on the route for the application. Note that the URL is < application_name >-< project_name >.< ocp cluster url >. In this case the project name is cos-twas-dev Add /CustomerOrderServicesWeb to the end of the URL in the browser to access the application Log in to the application with username: rbarcia and password: bl0wfish Repeat the validations for the stage and prod Projects. Summary This application has been modified from the initial WebSphere ND v8.5.5 version to run on traditional WebSphere and deployed by the IBM CloudPak for Applications.","title":"Operational modernization"},{"location":"was90/#cloud-pak-for-applications-operational-modernization-solution","text":"","title":"Cloud Pak for Applications: Operational Modernization Solution"},{"location":"was90/#introduction","text":"Operational modernization gives an operations team the opportunity to embrace modern operations best practices without putting change requirements on the development team. Modernizing from WebSphere Network Deployment (ND) to the traditional WebSphere Application Server Base V9 runtime in a container allows the application to be moved to the cloud without code changes. The scaling, routing, clustering, high availability and continuous availability functionality that WebSphere ND was required for previously can be provided by the container runtime and allows the operations team to run cloud-native and older applications in the same environment with the same standardized logging, monitoring and security frameworks. While traditional WebSphere isn't a 'built for the cloud' runtime like WebSphere Liberty, it can still be run in container and will receive the benefits of the consistency and reliability of containers as well as helping to improve DevOps and speed to market. This type of modernization shouldn't require any code changes and can be driven by the operations team. This path gets the application in to a container with the least amount of effort but doesn't modernize the application or the runtime. As organizations modernize to cloud platforms, new technologies and methodologies will be used for build, deployment and management of applications. While this modernization will be focused on cloud-native (built for the cloud) applications, using the traditional WebSphere container will allow common technologies and methodologies to be used regardless of the runtime. The diagram below shows the high level decision flow where IBM Cloud Transformation Advisor is used to analyze existing assets and a decision is made to not make code changes to the application and use the traditional WebSphere container as the target runtime. This repository holds a solution that is the result of a operational modernization for an existing WebSphere Java EE application that was moved from WebSphere ND v8.5.5 to the traditional WebSphere Base v9 container and is deployed by the IBM CloudPak for Applications to RedHat OpenShift.","title":"Introduction"},{"location":"was90/#table-of-contents","text":"Application Overview How the Application was Modernized Analysis Build Deploy Deploy the Application Getting the project repository Create the Security Context Constraint Create the projects Create a service account Deploy Jenkins Update the Jenkins service account Import the deployment templates Create the deployment definitions Import the build templates Create the build definitions Run the pipeline Validate the Application Summary","title":"Table of Contents"},{"location":"was90/#application-overview","text":"The Customer Order Services application is a simple store-front shopping application, built during the early days of the Web 2.0 movement. Users interact directly with a browser-based interface and manage their cart to submit orders. This application is built using the traditional 3-Tier Architecture model, with an HTTP server, an application server, and a supporting database. There are several components of the overall application architecture: - Starting with the database, the application leverages two SQL-based databases running on IBM DB2 . - The application exposes its data model through an Enterprise JavaBean layer, named CustomerOrderServices . This components leverages the Java Persistence API to exposed the backend data model to calling services with minimal coding effort. - This build of the application uses JavaEE6 features for EJBs and JPA. - The next tier of the application, named CustomerOrderServicesWeb , exposes the necessary business APIs via REST-based web services. This component leverages the JAX-RS libraries for creating Java-based REST services with minimal coding effort. - This build of the application is using JAX-RS 1.1 version of the respective capability. - The application's user interface is exposed through the CustomerOrderServicesWeb component as well, in the form of a Dojo Toolkit -based JavaScript application. Delivering the user interface and business APIs in the same component is one major inhibitor our migration strategy will help to alleviate in the long-term. - Finally, there is an additional integration testing component, named CustomerOrderServicesTest that is built to quickly validate an application's build and deployment to a given application server. This test component contains both JPA and JAX-RS -based tests.","title":"Application Overview"},{"location":"was90/#how-the-application-was-modernized","text":"In order to modernize the application from WebSphere ND v8.5.5 to the WebSphere Base v9 container running on OpenShift, the application went through analysis , build and deploy phases.","title":"How the Application was Modernized"},{"location":"was90/#analysis","text":"IBM Cloud Transformation Advisor was used to analyze the existing Customer Order Services application and the WebSphere ND runtime. The steps were: Install IBM Cloud Transformation Advisor either in to a Kubernetes Cluster or locally Download and execute the Data Collector against the existing WebSphere ND runtime Upload the results of the data collection in to IBM Cloud Transformation Advisor and review the analysis. A screenshot of the analysis is shown below: In the case of the CustomerOrderServicesApp.ear application, IBM Cloud Transformation Advisor has determined that the migration to WebSphere Traditional on Private Cloud is of Moderate complexity and that there are four Severe Issues that have been detected. Drilling down in to Detailed Migration Analysis Report that is part of the application analysis, it is apparent that IBM Cloud Transformation Advisor has detected that there are issues with JPA, specifically that the second-level cache and the JPA configuration properties must be migrated and with JAX-RS (missing Apache and Wink packages). These issues are related to a decision that was taken by IBM to allow WebSphere Application Server V9 to run in either JPA 2.0 or JPA 2.1 mode as described here and in either JAX-RS 2.0 or JAX-RS 1.1 mode as described here . In order to run in JPA 2.1 mode and JAX-RS 2.0 mode, the changes highlighted by IBM Cloud Transformation Advisor must be made to the application. However, this application can run in JPA 2.0 mode and JAX-RS 1.1 mode with no changes . In summary, no code changes are required to move this application to the traditional WebSphere Base v9 runtime and the decision was taken to proceed with the operational modernization. Detailed, step-by-step instructions on how to replicate these steps are provided here","title":"Analysis"},{"location":"was90/#build","text":"The build phase created the traditional WebSphere container configuration artifacts. The steps were: Update the existing wsadmin scripts that currently configure the runtime environment to configure the JPA 2.0 and JAXRS 1.1 engines. The final versions of the file can be found here: cosConfig.py The Dockerfile required to build the immutable Docker Image containing the application and traditional WebSphere was created. The final file can be found here: Dockerfile The containerized application was tested locally before the code and configuration files were committed to the git repository Detailed, step-by-step instructions on how to replicate these steps are provided here","title":"Build"},{"location":"was90/#deploy","text":"The deploy phase created the Jenkins, Kubernetes and RedHat OpenShift artifacts required to automate the build and deployment pipeline for the application. For illustration purposes, the application was deployed to three different RedHat OpenShift projects to simulate development , staging and production . The diagram below shows the flow through the pipeline. A more detailed description can be found here The steps were: Configure the RedHat OpenShift Cluster for WebSphere by creating the necessary SecurityContextConstraints definition. The file can be found here: scc.yaml Create the RedHat OpenShift build template that would be used to define the RedHat OpenShift artifacts related to the build process including ImageStream and BuildConfig definitions. The file can be found here: template-twas-build.yaml Create the RedHat OpenShift deployment template that would be used to define the RedHat OpenShift artifacts related to the Customer Order Services application including DeploymentConfig , Service and Route definitions. The file can be found here: template-twas-deploy.yaml Create the Jenkins Jenkinsfile for the pipeline. The Jenkinsfile defines the steps that the pipeline takes to build the Customer Order Services application EAR file, create an immutable Docker Image and then move the image through the dev , stage and prod environments. The file can be found here: Jenkinsfile Create the build project, load the build template and configure Jenkins Create the dev , stage and prod projects and load the deployment template Verify the pipeline. Detailed, step-by-step instructions on how to replicate these steps are provided here","title":"Deploy"},{"location":"was90/#deploy-the-application","text":"The following steps will deploy the modernized Customer Order Services application in a traditional WebSphere container to a RedHat OpenShift cluster.","title":"Deploy the Application"},{"location":"was90/#prerequisites","text":"You will need the following: Git CLI RedHat OpenShift 3.11 with Cluster Admin permissions oc CLI DB2 Database","title":"Prerequisites"},{"location":"was90/#getting-the-project-repository","text":"You can clone the repository from its main GitHub repository page and checkout the appropriate branch for this version of the application. git clone https : // github . com / ibm - cloud - architecture / cloudpak - for - applications . git cd cloudpak - for - applications git checkout was90","title":"Getting the project repository"},{"location":"was90/#create-application-database-infrastructure","text":"As said in the prerequisites section above, the Customer Order Services application uses uses DB2 as its database. Follow these steps to create the appropriate database, tables and data the application needs to: Copy the createOrderDB.sql and initialDataSet.sql files you can find in the Common directory of this repository over to the db2 host machine (or git clone the repository) in order to execute them later. Ssh into the db2 host Change to the db2 instance user: `su {database_instance_name}`` Start db2: db2start Create the ORDERDB database: db2 create database ORDERDB Connect to the ORDERDB database: db2 connect to ORDERDB Execute the createOrderDB.sql script you copied over in step 1 in order to create the appropriate tables, relationships, primary keys, etc: db2 -tf createOrderDB.sql Execute the initialDataSet.sql script you copied over in step 1 to populate the ORDERDB database with the needed initial data set: db2 -tf initialDataSet.sql If you want to re-run the scripts, please make sure you drop the databases and create them again.","title":"Create application database infrastructure"},{"location":"was90/#create-the-security-context-constraint","text":"In order to deploy and run the WebSphere Base Docker image in an OpenShift cluster, we first need to configure certain security aspects for the cluster. The Security Context Constraint provided here grants the service account that the WebSphere Base Docker container is running under the required privileges to function correctly. A cluster administrator can use the file provided here with the following command to create the Security Context Constraint (SCC): cd Deployment / OpenShift oc apply - f ssc . yaml","title":"Create the Security Context Constraint"},{"location":"was90/#create-the-projects","text":"Four RedHat OpenShift projects are required in this scenario: - Build: this project will contain the Jenkins server and the artifacts used to build the application image - Dev: this is the development environment for this application - Stage: this is the staging environment for this application - Prod: this is the production environment for this application The file provided here contains the definitions for the four projects in a single file to make creation easier Issue the command shown below to create the projects oc create - f liberty - projects . yaml","title":"Create the projects"},{"location":"was90/#create-a-service-account","text":"It is a good Kubernetes practice to create a service account for your applications. A service account provides an identity for processes that run in a Pod. In this step we will create a new service account with the name websphere in each of the dev , stage and prod projects and add the Security Context Constraint created above to them. Issue the commands shown below to create the websphere service account and bind the ibm-websphere-scc to it in each of the projects: oc create serviceaccount websphere - n cos - twas - dev oc create serviceaccount websphere - n cos - twas - stage oc create serviceaccount websphere - n cos - twas - prod oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - twas - dev oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - twas - stage oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - twas - prod","title":"Create a service account"},{"location":"was90/#deploy-jenkins","text":"Some RedHat OpenShift clusters are configured to automatically provision a Jenkins instance in a build project. The steps below can be used if your cluster is not configured for automatic Jenkins provisioning: oc project cos - liberty - build oc new - app jenkins - persistent","title":"Deploy Jenkins"},{"location":"was90/#update-the-jenkins-service-account","text":"During provisioning of the Jenkins master a service account with the name jenkins is created. This service account has privileges to create new artifacts only in the project that it is running in. In this scenario Jenkins will need to create artifacts in the dev , stage and prod projects. Issue the commands below to allow the jenkins service account to edit artifacts in the dev , stage and prod projects. oc policy add - role - to - user edit system : serviceaccount : cos - twas - build : jenkins - n cos - twas - dev oc policy add - role - to - user edit system : serviceaccount : cos - twas - build : jenkins - n cos - twas - stage oc policy add - role - to - user edit system : serviceaccount : cos - twas - build : jenkins - n cos - twas - prod","title":"Update the Jenkins service account"},{"location":"was90/#import-the-deployment-templates","text":"RedHat OpenShift templates are used to make artifact creation easier and repeatable. The template definition provided here defines a Kubernetes Service , Route and DeploymentConfig for the CustomerOrderServices application. The gse-twas-deploy template defines the following: - service listening on ports 9080 , 9443 and 9082 - route to expose the 9443 port externally - DeploymentConfig to host the WebSphere Base container. - The image for the container is taken from the ImageStream that will be populated by the Jenkins pipeline. - environment variables are defined for the DB2 database used by the application allowing for environment specific information to be injected - Probes for liveness and readiness are defined to check port 9443 is active - The securityContext is set to allow read/write access to the filesystem and to run the container as user 1001 - The deployment will be updated if a new image is loaded to the ImageStream or if a change to the configuration is detected. Issue the commands below to load the template named gse-twas-deploy in the dev , stage and prod projects. oc create - f template - twas - deploy . yaml - n cos - twas - dev oc create - f template - twas - deploy . yaml - n cos - twas - stage oc create - f template - twas - deploy . yaml - n cos - twas - prod","title":"Import the deployment templates"},{"location":"was90/#create-the-deployment-definitions","text":"In this step the gse-twas-deploy template will be used to create a RedHat OpenShift application named cos-twas in the dev , stage and prod namespaces. The result will be: - service listening on ports 9080 , 9443 and 9082 - route to expose the 9443 port externally - DeploymentConfig to host the WebSphere Base container. The deployment config will wait for a docker image to be loaded in to the ImageStream by the Jenkins pipeline. Issue the following commands to create the applications from the template: oc new - app gse - twas - deploy - p APPLICATION_NAME = cos - twas - n cos - twas - dev oc new - app gse - twas - deploy - p APPLICATION_NAME = cos - twas - n cos - twas - stage oc new - app gse - twas - deploy - p APPLICATION_NAME = cos - twas - n cos - twas - prod","title":"Create the deployment definitions"},{"location":"was90/#import-the-build-templates","text":"In this step a template for the build process will be loaded in to the build project. The template provided here defines the following artifacts: An ImageStream for the application image. This will be populated by the Jenkins Pipeline An ImageStream for WebSphere Base which will pull down the latest version of the ibmcom/websphere-traditional:latest-ubi image and will monitor DockerHub for any updates. A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub Parameters to allow the WebSphere Base image and GitHub repository to be provided when the template is instantiated Issue the commands below to load the template named gse-twas-build in the build projects. oc create - f template - twas - build . yaml - n cos - twas - build","title":"Import the build templates"},{"location":"was90/#create-the-build-definitions","text":"In this step the gse-twas-build template will be used to create a RedHat OpenShift application named cos-twas in the build namespaces. The result will be: - An ImageStream for the application image. This will be populated by the Jenkins Pipeline - An ImageStream for WebSphere Base which will pull down the latest version of the ibmcom/websphere-traditional:latest-ubi image and will monitor DockerHub for any updates. - A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image - A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub (with the URL provided as a parameter when the application is created) Issue the following commands to create the application from the template: oc new - app gse - twas - build - p APPLICATION_NAME = cos - twas - p SOURCE_URL = \" https://github.com/ibm-cloud-architecture/cloudpak-for-applications \" - n cos - twas - build","title":"Create the build definitions"},{"location":"was90/#run-the-pipeline","text":"The newly created pipeline can be started from the RedHat OpenShift console which allows access to the Jenkins logs but also tracks the progress in the OCP console. Navigate to Application Console --> Customer Order Services on twas - Build --> Builds --> Pipelines and click the Start Pipeline button When the pipeline starts, click the view log link to go to the Jenkins administration console. Note that it may take a couple of minutes before the view log link appears on the first pipeline build When prompted, log in with your OpenShift account and grant the required access permissions. The Jenkins console log will be displayed as shown below: Return to the OpenShift Console and track the progress of the pipeline The pipeline will eventually stop at the Promotion Gate for approval to deploy to Production. Click the Input Required link as shown below When the Promote application to Production question is displayed, click Proceed Return to the OpenShift Console and validate that the pipeline is now complete","title":"Run the pipeline"},{"location":"was90/#validate-the-application","text":"Now that the pipeline is complete, validate the Customer Order Services application is deployed and running in dev , stage and prod Now that the pipeline is complete, validate the Customer Order Services application is deployed and running in dev , stage and prod In the OpenShift Console, navigate to Application Console --> Customer Order Services on twas - Dev --> Applications --> Deployments and click on the link in the Latest Version column Information about the deployment will be displayed including the image that is being used (note the tag on the image as it will be the same in the stage and prod deployments). After a few minutes the container will be marked as ready Click Applications --> Routes and click on the route for the application. Note that the URL is < application_name >-< project_name >.< ocp cluster url >. In this case the project name is cos-twas-dev Add /CustomerOrderServicesWeb to the end of the URL in the browser to access the application Log in to the application with username: rbarcia and password: bl0wfish Repeat the validations for the stage and prod Projects.","title":"Validate the Application"},{"location":"was90/#summary","text":"This application has been modified from the initial WebSphere ND v8.5.5 version to run on traditional WebSphere and deployed by the IBM CloudPak for Applications.","title":"Summary"},{"location":"was90/tWAS-analyze/","text":"traditional WebSphere - Analyze This section covers how to use IBM Cloud Transformation Advisor to analyze an existing traditional WebSphere application. For this scenario the traditional WebSphere Application Server runtime is chosen as the target runtime and the intention is to migrate this application without code changes. Migrating to the containerized version of traditional WebSphere Application Server will prepare the organization for: moving workloads to the cloud. improving DevOps and speed-to-market. receiving the benefits of the consistency and reliability of containers. Summary This section has the following steps: Introduction to IBM Cloud Transformation Advisor Install IBM Cloud Transformation Advisor Download and run the Data Collector Upload and analyze the results Determine the migration/modernization path and next steps Introduction to IBM Cloud Transformation Advisor IBM Cloud Transformation Advisor helps you access, analyze and modernize middleware based apps into IBM Cloud(s). It categorizes Java EE apps and MQ queue managers as simple, medium and complex based on migration complexity and provides guidance for modernization. IBM Cloud Transformation Advisor can accelerate the process to move your on-premises apps to cloud, minimize migration errors and risks, and reduce time to market in five steps. You can use IBM Transformation Advisor for these activities: - Identify the Java EE programming models in an app - Determine the complexity of apps by reviewing a high-level inventory of the content and structure of each app - Highlight the Java EE programming model and WebSphere API differences between the profile types - Learn any Java EE specification implementation differences that might affect the app The tool also provides suggestions for the right-fit IBM WebSphere Application Server edition and offers advice, practices, and potential solutions to move apps to WebSphere Liberty or to newer versions of WebSphere Application Server. Install IBM Cloud Transformation Advisor IBM Cloud Transformation Advisor is split in to two components (the analysis engine and the data collector ). You can choose to between installing the analysis engine in to an IBM Cloud Private Cluster or locally on a machine with Docker. Installing IBM Cloud Transformation Advisor in to your IBM Cloud Private Cluster Installing IBM Cloud Transformation Advisor Beta Edition locally Download the Data Collector Once IBM Cloud Transformation Advisor is installed, it is necessary to create a new Workspace and Collection and then download the Data Collector that will be used to examine the existing environment and applications. Open IBM Cloud Transformation Advisor in a browser and click the button to create a new Workspace Enter a Workspace name such as CloudPak_for_Applications and click Next Enter a Collection name such as WAS855_AppSrv01 and click Let's go When the No recommendations available page is displayed, click the Data Collector button When the Data Collector page is displayed, select the Source Operating System for your environment and click the Download button to download the Data Collector. This results in a file with a name similar to transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz being downloaded. Run the Data Collector Upload the Data Collector zip file that was downloaded from IBM Cloud Transformation Advisor in the previous step to the machine that the WebSphere ND Deployment Manager or the Standalone WebSphere Application Server is installed. The directory used arbitrary. Navigate to the directory you uploaded the transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz file to and issue the following commands to extract the Data Collector: bash mkdir datacollector cd datacollector mv transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz . tar -zxvf transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz cd transformationadvisor-* It is necessary to modify the scan performed by the Data Collector to include the org.pwte package as the Data Collector doesn't scan org.* packages by default. Open the conf/customCmd.properties file and modify it as shown below: bash evaluation=--evaluate --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp --includePackages=org.pwte migration_liberty=--analyze --sourceAppServer=was855 --targetAppServer=liberty --targetCloud=dockerIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp migration_was=--analyze --sourceAppServer=was855 --targetAppServer=was90 --targetCloud=vmIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp #inventory=--inventory --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #featureList=--featureList --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #java_opt=-Xmx2g The following command assumes that WebSphere Application Server v855 is installed to /opt/IBM/WebSphere/AppServer855 with a profile named AppSrv01 and that the administration user is wasadmin with a password of wasadmin . Modify and issue the following command as necessary to execute the Data Collector against the WebSphere environment: bash ./bin/transformationadvisor -w /opt/IBM/WebSphere/AppServer855 -p AppSrv01 wasadmin wasadmin When prompted, accept the license agreement . The Data Collection process will now start and will analyze all of the applications installed in the WebSphere Application Server environment and will also collect the related Java EE artifacts such as Data Sources and JMS definitions. When the analysis is complete, the Data Collector will attempt to upload the collection results to IBM Cloud Transformation Advisor. If this is successful, you can skip to the Analyze the Recommendations section. If not, you will receive an error at the end of Data Collection and will find a file named AppSrv01.zip in your current directory as shown below. bash ~/datacollector/transformationadvisor-1.9.6# ls -la *.zip -rw-r--r-- 1 root root 625493 Jun 12 12:58 AppSrv01.zip Download this Data Collector Results zip file ready for uploading to IBM Cloud Transformation Advisor in the next section Upload the Data Collector results In this section the results from the Data Collector will be uploaded to IBM Cloud Transformation Advisor. In the IBM Cloud Transformation Advisor web browser session, click the Recomendations link in the top left corner and then click the Upload data button as shown below When the Upload data dialog is displayed, use the Drop or Add File button to select the Data Collector Results zip file that was downloaded in the previous section. Click Upload After a few moments the upload of the data collector results will be completed. Analyze the Recommendations Once the Data Collector Results have been uploaded to IBM Cloud Transformation Advisor a set of recommendations will be created and shown on the Recommendations page. In this section the recommendations will be analyzed and interpreted. The default recommendations are based on a target runtime of Liberty on Private Cloud . In this scenario the desired target runtime is WebSphere Traditional on Private Cloud . Use the Preferred migration drop down to select WebSphere Traditional on Private Cloud as shown below. The Data Collector analyzed all of the applications running on the traditional WebSphere profile a displays a row in the chart for each application. In the case of the CustomerOrderServicesApp.ear application, IBM Cloud Transformation Advisor has determined that the migration to WebSphere Traditional on Private Cloud is of Moderate complexity and that there are four Severe Issues that have been detected. Click on the CustomerOrderServicesApp.ear application name to see more information. Review the analysis results and scroll down to the Technology Issues section. Note that IBM Cloud Transformation Advisor has detected that there are issues with JPA, specifically that the second-level cache and the JPA configuration properties must be migrated and with JAX-RS (missing Apache and Wink packages). These issues are related to a decision that was taken by IBM to allow WebSphere Application Server V9 to run in either JPA 2.0 or JPA 2.1 mode as described here and in either JAX-RS 2.0 or JAX-RS 1.1 mode as described here . In order to run in JPA 2.1 mode and JAX-RS 2.0 mode, the changes highlighted by IBM Cloud Transformation Advisor must be made to the application. However, this application can run in JPA 2.0 mode and JAX-RS 1.1 mode with no changes . In order to review the IBM Cloud Transformation Advisor results in more detail, scroll to the bottom of the analysis page and click on the Analysis Report link When the warning dialog is displayed, click OK The Detailed Migration Analysis Report will be displayed which show the results of the migration rules that were executed by the Data Collector and returned results. Scroll down to the Severe Rules section and click on the Show rule help link for each of the results. Review the recommendations. Final Analysis The intention of this traditional WebSphere V855 --> traditional WebSphere V9 (Private Cloud) scenario is to migrate the Customer Order Services application to the new runtime without code changes. IBM Cloud Transformation Advisor was used to analyze the application for compatibility with traditional WebSphere V9 (Private Cloud) and determined that code changes would be required. IBM Cloud Transformation Advisor took the approach that the application should be modified to run with the JPA 2.1 runtime and the JAX-RS 2.0 runtime instead of giving the option to use the JPA 2.0 runtime and the JAX-RS 1.1 runtime which would have resulted in not requiring any code changes to the application. The remainder of this scenario is based on the decision to use the JPA 2.0 runtime option and the JAX-RS 1.1 runtime option in traditional WebSphere V9 (Private Cloud) and as such no code changes will be required to this application. Now proceed to the traditional WebSphere - Build section where the process of extracting the application configuration from the WebSphere V855 Application Server profile will be covered step-by-step","title":"traditional WebSphere - Analyze"},{"location":"was90/tWAS-analyze/#traditional-websphere-analyze","text":"This section covers how to use IBM Cloud Transformation Advisor to analyze an existing traditional WebSphere application. For this scenario the traditional WebSphere Application Server runtime is chosen as the target runtime and the intention is to migrate this application without code changes. Migrating to the containerized version of traditional WebSphere Application Server will prepare the organization for: moving workloads to the cloud. improving DevOps and speed-to-market. receiving the benefits of the consistency and reliability of containers.","title":"traditional WebSphere - Analyze"},{"location":"was90/tWAS-analyze/#summary","text":"This section has the following steps: Introduction to IBM Cloud Transformation Advisor Install IBM Cloud Transformation Advisor Download and run the Data Collector Upload and analyze the results Determine the migration/modernization path and next steps","title":"Summary"},{"location":"was90/tWAS-analyze/#introduction-to-ibm-cloud-transformation-advisor","text":"IBM Cloud Transformation Advisor helps you access, analyze and modernize middleware based apps into IBM Cloud(s). It categorizes Java EE apps and MQ queue managers as simple, medium and complex based on migration complexity and provides guidance for modernization. IBM Cloud Transformation Advisor can accelerate the process to move your on-premises apps to cloud, minimize migration errors and risks, and reduce time to market in five steps. You can use IBM Transformation Advisor for these activities: - Identify the Java EE programming models in an app - Determine the complexity of apps by reviewing a high-level inventory of the content and structure of each app - Highlight the Java EE programming model and WebSphere API differences between the profile types - Learn any Java EE specification implementation differences that might affect the app The tool also provides suggestions for the right-fit IBM WebSphere Application Server edition and offers advice, practices, and potential solutions to move apps to WebSphere Liberty or to newer versions of WebSphere Application Server.","title":"Introduction to IBM Cloud Transformation Advisor"},{"location":"was90/tWAS-analyze/#install-ibm-cloud-transformation-advisor","text":"IBM Cloud Transformation Advisor is split in to two components (the analysis engine and the data collector ). You can choose to between installing the analysis engine in to an IBM Cloud Private Cluster or locally on a machine with Docker. Installing IBM Cloud Transformation Advisor in to your IBM Cloud Private Cluster Installing IBM Cloud Transformation Advisor Beta Edition locally","title":"Install IBM Cloud Transformation Advisor"},{"location":"was90/tWAS-analyze/#download-the-data-collector","text":"Once IBM Cloud Transformation Advisor is installed, it is necessary to create a new Workspace and Collection and then download the Data Collector that will be used to examine the existing environment and applications. Open IBM Cloud Transformation Advisor in a browser and click the button to create a new Workspace Enter a Workspace name such as CloudPak_for_Applications and click Next Enter a Collection name such as WAS855_AppSrv01 and click Let's go When the No recommendations available page is displayed, click the Data Collector button When the Data Collector page is displayed, select the Source Operating System for your environment and click the Download button to download the Data Collector. This results in a file with a name similar to transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz being downloaded.","title":"Download the Data Collector"},{"location":"was90/tWAS-analyze/#run-the-data-collector","text":"Upload the Data Collector zip file that was downloaded from IBM Cloud Transformation Advisor in the previous step to the machine that the WebSphere ND Deployment Manager or the Standalone WebSphere Application Server is installed. The directory used arbitrary. Navigate to the directory you uploaded the transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz file to and issue the following commands to extract the Data Collector: bash mkdir datacollector cd datacollector mv transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz . tar -zxvf transformationadvisor-Linux_CloudPak_for_Applications_WAS855_AppSrv01.tgz cd transformationadvisor-* It is necessary to modify the scan performed by the Data Collector to include the org.pwte package as the Data Collector doesn't scan org.* packages by default. Open the conf/customCmd.properties file and modify it as shown below: bash evaluation=--evaluate --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp --includePackages=org.pwte migration_liberty=--analyze --sourceAppServer=was855 --targetAppServer=liberty --targetCloud=dockerIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp migration_was=--analyze --sourceAppServer=was855 --targetAppServer=was90 --targetCloud=vmIBMCloud --includePackages=org.pwte --excludePackages=com.ibm,com.informix,com.microsoft,com.sybase,com.sun,java,javax,net,oracle,sqlj,_ibmjsp #inventory=--inventory --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #featureList=--featureList --excludeFiles=\".*/directory/LargeXMLFileName.xml\" #java_opt=-Xmx2g The following command assumes that WebSphere Application Server v855 is installed to /opt/IBM/WebSphere/AppServer855 with a profile named AppSrv01 and that the administration user is wasadmin with a password of wasadmin . Modify and issue the following command as necessary to execute the Data Collector against the WebSphere environment: bash ./bin/transformationadvisor -w /opt/IBM/WebSphere/AppServer855 -p AppSrv01 wasadmin wasadmin When prompted, accept the license agreement . The Data Collection process will now start and will analyze all of the applications installed in the WebSphere Application Server environment and will also collect the related Java EE artifacts such as Data Sources and JMS definitions. When the analysis is complete, the Data Collector will attempt to upload the collection results to IBM Cloud Transformation Advisor. If this is successful, you can skip to the Analyze the Recommendations section. If not, you will receive an error at the end of Data Collection and will find a file named AppSrv01.zip in your current directory as shown below. bash ~/datacollector/transformationadvisor-1.9.6# ls -la *.zip -rw-r--r-- 1 root root 625493 Jun 12 12:58 AppSrv01.zip Download this Data Collector Results zip file ready for uploading to IBM Cloud Transformation Advisor in the next section","title":"Run the Data Collector"},{"location":"was90/tWAS-analyze/#upload-the-data-collector-results","text":"In this section the results from the Data Collector will be uploaded to IBM Cloud Transformation Advisor. In the IBM Cloud Transformation Advisor web browser session, click the Recomendations link in the top left corner and then click the Upload data button as shown below When the Upload data dialog is displayed, use the Drop or Add File button to select the Data Collector Results zip file that was downloaded in the previous section. Click Upload After a few moments the upload of the data collector results will be completed.","title":"Upload the Data Collector results"},{"location":"was90/tWAS-analyze/#analyze-the-recommendations","text":"Once the Data Collector Results have been uploaded to IBM Cloud Transformation Advisor a set of recommendations will be created and shown on the Recommendations page. In this section the recommendations will be analyzed and interpreted. The default recommendations are based on a target runtime of Liberty on Private Cloud . In this scenario the desired target runtime is WebSphere Traditional on Private Cloud . Use the Preferred migration drop down to select WebSphere Traditional on Private Cloud as shown below. The Data Collector analyzed all of the applications running on the traditional WebSphere profile a displays a row in the chart for each application. In the case of the CustomerOrderServicesApp.ear application, IBM Cloud Transformation Advisor has determined that the migration to WebSphere Traditional on Private Cloud is of Moderate complexity and that there are four Severe Issues that have been detected. Click on the CustomerOrderServicesApp.ear application name to see more information. Review the analysis results and scroll down to the Technology Issues section. Note that IBM Cloud Transformation Advisor has detected that there are issues with JPA, specifically that the second-level cache and the JPA configuration properties must be migrated and with JAX-RS (missing Apache and Wink packages). These issues are related to a decision that was taken by IBM to allow WebSphere Application Server V9 to run in either JPA 2.0 or JPA 2.1 mode as described here and in either JAX-RS 2.0 or JAX-RS 1.1 mode as described here . In order to run in JPA 2.1 mode and JAX-RS 2.0 mode, the changes highlighted by IBM Cloud Transformation Advisor must be made to the application. However, this application can run in JPA 2.0 mode and JAX-RS 1.1 mode with no changes . In order to review the IBM Cloud Transformation Advisor results in more detail, scroll to the bottom of the analysis page and click on the Analysis Report link When the warning dialog is displayed, click OK The Detailed Migration Analysis Report will be displayed which show the results of the migration rules that were executed by the Data Collector and returned results. Scroll down to the Severe Rules section and click on the Show rule help link for each of the results. Review the recommendations.","title":"Analyze the Recommendations"},{"location":"was90/tWAS-analyze/#final-analysis","text":"The intention of this traditional WebSphere V855 --> traditional WebSphere V9 (Private Cloud) scenario is to migrate the Customer Order Services application to the new runtime without code changes. IBM Cloud Transformation Advisor was used to analyze the application for compatibility with traditional WebSphere V9 (Private Cloud) and determined that code changes would be required. IBM Cloud Transformation Advisor took the approach that the application should be modified to run with the JPA 2.1 runtime and the JAX-RS 2.0 runtime instead of giving the option to use the JPA 2.0 runtime and the JAX-RS 1.1 runtime which would have resulted in not requiring any code changes to the application. The remainder of this scenario is based on the decision to use the JPA 2.0 runtime option and the JAX-RS 1.1 runtime option in traditional WebSphere V9 (Private Cloud) and as such no code changes will be required to this application. Now proceed to the traditional WebSphere - Build section where the process of extracting the application configuration from the WebSphere V855 Application Server profile will be covered step-by-step","title":"Final Analysis"},{"location":"was90/tWAS-build/","text":"traditional WebSphere - Build This section covers how to containerize an existing application running on traditional WebSphere Application Server. To do this we have two options on how to migrate. Option 1 (descibed below): Create or use existing wsadmin scripts. Option 2: Use the WebSphere Configuration Migration Tool for IBM Cloud (WCMT4IC) to extract the configuration from your existing traditional WebSphere Application Server environment. We're going to focus on the method of using existing scripts, since it better matches the best practice for CI/CD pipelines. Once the environment is running as is, we'll learn how to make modifications to the configuration without having to rebuild your container, and why this is important. The final versions of the files created in this section can be found in the was90 branch of this repo Summary: This section has the following steps: Build the image using existing scripts Test the application locally Using environment variables in properties files to allow configuration to be injected in to the runtime environment dynamically. Advanced: put your passwords in secrets. Building a traditional WebSphere Docker container using existing wsadmin scripts. For this example, we're going to use the Customer Order Application as an example. We already have a wsadmin script which will configure the environment and install the application. Review the contents of the wsadmin script . This is a legacy application which runs on older frameworks; most notably it uses JPA 2.0 and JAX-RS 1.1. These are not the default in WAS 9 (as they are in WAS8.5.5, but they are supported). It is important to modify these settings in the scripts if you don't want to make modifications to your application at this time. We are going to install the application using a properties file, which is an alternative way to install the application, or apply any configuration. The application could also be installed using the wsadmin jython script, but we chose to use the properties file to show the two methods configurations can be applied at build time. Review the contents of the properties file To run these files during container creation we copy them into /work/config/ folder using the Dockerfile. Then we run the /work/configure.sh which will start the server and run the scripts and apply the properties file configuration. We also copy the db2 drivers and the application, and anything else the applications needs into the container. Review the contents of the Dockerfile . ARG WEBSPHERE_VERSION = 9 . 0 . 0 . 11 # Migration requires websphere - traditional >= 9 . 0 . 0 . 11 FROM ibmcom / websphere - traditional :$ WEBSPHERE_VERSION as migration # Hardcode password for admin console COPY -- chown = was : 0 tWAS / PASSWORD / tmp / PASSWORD COPY -- chown = was : 0 resources / db2 / / opt / IBM / db2drivers / COPY -- chown = was : 0 resources / jmx_exporter / jmx_prometheus_javaagent - 0 . 11 . 0 . jar / opt / IBM / jmx_exporter / COPY -- chown = was : 0 resources / jmx_exporter / jmx - config . yaml / opt / IBM / jmx_exporter / # Uncomment to test locally # COPY -- chown = was : 0 tWAS / jvm . props / work / config COPY -- chown = was : 0 tWAS / cosConfig . py / work / config / COPY -- chown = was : 0 tWAS / app - update . props / work / config / app - update . props COPY -- chown = was : 0 tWAS / CustomerOrderServicesApp - 0 . 1 . 0 - SNAPSHOT . ear / work / config / CustomerOrderServicesApp - 0 . 1 . 0 - SNAPSHOT . ear RUN / work / configure . sh Let's try it... Build the image Now it's time to build the image. This guide assumes you have Docker installed. The code existing in the was90 branch. Clone the branch to copy the files to your system git clone -- branch was90 https : // github . com / ibm - cloud - architecture / cloudpak - for - applications . git cd cloudpak - for - applications docker build -- tag customer - order - app . You should see the following messages if image was successfully built (image id may vary): Successfully built f8d3eafdd30a Successfully tagged customer - order - app : latest Validate that image is in the repository using command (you should also see original websphere-traditional image): $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE customer - order - app latest f8d3eafdd30a 3 days ago 1 . 85 GB ibmcom / websphere - traditional 9 . 0 . 0 . 11 2985 c9be759f 10 days ago 1 . 78 GB Test the image To test the image locally you are also going to need a database to connect to locally. We aren't going to go into this much. Since we want the DB2 and tWAS containers to be able to talk to each other we create a network they can share $ docker network create customer - order Now we start DB2. This DB2 contains scripts to populate the database with sample data for our application on startup. $ docker run - d --network customer-order --privileged=true -e LICENSE=accept -e DB2INST1_PASSWORD=db2inst1 -e DBNAME=orderdb --name db2-customer-order vandepol/db2-cos This take a while to start up since it creates the database from scratch. to access this database from the tWAS container, we use the hostname = container name. In this case the hostname would be db2-customer-order You are going to test the image in local Docker engine. Issue the following command to start the container: $ docker run - d --network customer-order --name twas-customer-order -p 9443:9443 -p 9043:9043 customer-order-app:latest Check if container started successfully using command: $d ocker logs - f 9 ee253a59956869c6 WASX7303I : The following options are passed to the scripting environment and are available as arguments that are stored in the argv variable : \" [/work/config-ibm/webContainer.props] \" Starting server ................... ADMU3000I : Server server1 open for e - business ; process id is 494 HPEL is enabled run logViewer . sh Right now the problem we have is that the database hostname in the scripts we used to configure the application doesn't match the hostname of the db2 docker container we started. That leads us to our next section on modifying the configuration without rebuilding the container. We could simply modify the jython scripts and change the hostname of the databases to db2-customer-order , however we're going to do it using configuration files. Modifying Configuration without rebuilding container When building the traditional WebSphere container there may be configuration you will want to add or change between environments. You don't want to have to rebuild your container just because a database hostname changed, or for password changes. To solve this problem, you can load configuration changes at server start-up using properties files. In this section we'll show how to extract the configuration into properties files. In the Deploy section we'll describe how to create the configMaps and load them at server start-up when deploying using the cloudpak. We have pre-extracted configuration files in the runtimeConfig folder, however if you want to extract your own you can follow the steps below. We start with a server environment setup and running with the resources defined as you desire. We can use the docker container we created above. We can run this locally. docker run -d --rm --name twas-customer-order -p 9443:9443 -p 9043:9043 -p9080:9080 -v /myconfigs/:/etc/websphere/ customer-order-app:latest Here we expose port 9043 for the WebSphere admin console. We also mount a volume so that we can extract the properties files we're generating locally. Next, we ssh into the container to execute commands to extract the configuration. To enter the shell script of our container locally we run the command: docker exec -it twas-customer-order bash Once in the container shell, we execute the following commands /opt/IBM/WebSphere/AppServer/bin/wsadmin.sh -lang jython you will be prompted for your credentials. Use the credentials you would use in the Admin Console. in our example we have: wasadmin/passw0rd Run the following command to extract the Datasource configuration and the Password information: AdminTask . extractConfigProperties ( ' [ - propertiesFileName / etc / websphere / Datasource . inds . props - configData DataSource = INDS - options [[ PortablePropertiesFile true ]]] ' ) AdminTask . extractConfigProperties ( ' [ - propertiesFileName / etc / websphere / Datasource . orderds . props - configData DataSource = OrderDS - options [[ PortablePropertiesFile true ]]] ' ) AdminTask . extractConfigProperties ( ' [ - propertiesFileName / etc / websphere / JAASAuth . db2password . props - configData Security = - filterMechanism SELECTED_SUBTYPES - selectedSubTypes [ JAASAuthData ] - options [[ PortablePropertiesFile true ]]] ' ) The syntax for these commands can get complicated. Documentation available here Sample Configuration Files Datasource.inds.props Datasource.orderds.props JAASAuth.db2password.props If we want to change something in this configuration, we can make a change to these properties files. To apply this to the container we can do this at container build time by copying the properties file into the /work/config folder as described in the Modifying Artifact section . However, if you want to modify these at deploy time without rebuilding the docker container (ideal for passwords and database hostnames which change between environments), you can do a volume mount to /etc/websphere , and these will be applied at server start-up. Let's try this! Open the properties files, Datasource.inds.props and Datasource.orderds.props . Here let's change the server name of the database we're accessing to the name of our local db2 container we start above. serverName = db2 - customer - order # String If our container is still running from before we want to stop it so that we can restart it and allow for the new properties to be loaded. docker stop twas-customer-order Restart the docker container and map the properties files again. Remember to connect to the same network of the db2 container we started above docker run -d --rm --name twas-customer-order --network customer-order -p 9443:9443 -p 9043:9043 -p 9080:9080 -v /runtimeConfigs/:/etc/websphere/ customer-order-app:guide We can look at the logs to see if the properties files are loaded. TIP : the properties files MUST end in .props. it will not detect different file types in the /etc/websphere folder at server start-up. $ docker logs twas-customer-order -f ENABLE_BASIC_LOGGING is Configure logging mode WASX7357I: By request, this scripting client is not connected to any server process. Certain configuration and application operations will be available in local mode. + Found config-files under /etc/websphere. Executing... WASX7357I: By request, this scripting client is not connected to any server process. Certain configuration and application operations will be available in local mode. WASX7303I: The following options are passed to the scripting environment and are available as arguments that are stored in the argv variable: \"[/etc/websphere/Datasource.inds.props]\" WASX7357I: By request, this scripting client is not connected to any server process. Certain configuration and application operations will be available in local mode. WASX7303I: The following options are passed to the scripting environment and are available as arguments that are stored in the argv variable: \"[/etc/websphere/Datasource.orderds.props]\" WASX7357I: By request, this scripting client is not connected to any server process. Certain configuration and application operations will be available in local mode. WASX7303I: The following options are passed to the scripting environment and are available as arguments that are stored in the argv variable: \"[/etc/websphere/JAASAuth.db2password.props]\" WASX7357I: By request, this scripting client is not connected to any server process. Certain configuration and application operations will be available in local mode. How we can log into the admin console to see our changes. Verify the application After the server is successfully started, you can connect to it using browser. Use http://localhost:9043/ibm/console to access admin console of the server running in the container. Login using your original credentials, that where migrated. Also validate that application has successfully started And the Datasources were also migrated Finally open the application https://localhost:9443/CustomerOrderServicesWeb use rbarcia/bl0wfish for username and password if prompted. You have successfully migrated application to the traditional WebSphere Application Server container. Good job! Advanced: Put your passwords in Secrets! One issue with the above scenario is that the contents of the JAASAuth.db2password.props is in plain text (or XOR encrypted...which isn't hard to crack). Typically, we wouldn't want to store this in a properties file, which would then be stored as a configmap in Kubernetes. We'd want to use secrets. The way secrets work is that the properties loaded into the secret are loaded as environment variables at container start-up. Ideally, we would want to replace values in the properties files we've loaded above with environment variables, this is currently not supported in the cloudpak (Feature request made to development); however, we have a work around to load these values ourselves. First, we would replace a value with reference to an environment variable. Let's use that the port number as an example again. syncQueryTimeoutWithTransactionTimeout = # boolean freeResourcesOnClose = false # String traceFileCount = null ** portNumber =! { inds . port . number } # integer ** stripTrailingZerosForDecimalNumbers = null translateForBitData = null webSphereDefaultQueryTimeout = # integer clientApplicationInformation = # String Here we have assigned the portNumber to the environment variable inds.port.number . We have created a simple script to run prior to server startup which will replace all references to environment variables with the environment variable values. The script we created is available here 1 2 3 4 5 6 7 8 #!/bin/sh mkdir /etc/websphere/orig cp /etc/websphere/*.props /etc/websphere/orig/ for f in $( find /etc/websphere/orig/ -regex '.*\\.props' ) ; do envsubst < $f > \"/etc/websphere/ $( basename $f ) \" ; done One issue is that we need to install envsubst into the container, which is included inside the gettext package. We have to run apt-get install to install this. Next we need to copy our script into the container, and lastly we need to modify the startup command to run this command prior to server start-up. Here is our solution to this: RUN apt - get update && apt - get install - y gettext COPY --chown=was:0 envsubst-cmd.sh /work/ CMD [ \"sh\" , \"-c\" , \"/work/envsubst-cmd.sh && /work/start_server.sh\" ] Now we should be able to start up the container and pass a new value as an environment variable to change the port number. docker run -d --rm --name customer-order-app -p9443:9443 -p 9043:9043 -v /myconfigs/:/etc/websphere/ -e inds.port.number=50002 customer-order-app:latest To specify environment variable in the properties file you can replace the value with ${inds.port.number} Conclusion In conclusion, we've taken our estisting traditional WebSphere application. We've loaded the container with the server and application configuration. We've installed the application, and modified the configuration if required. This sets us up for integrating all of this into a CI/CD pipeline and then deploying this to Kubernetes. We're on our way to the cloud!","title":"traditional WebSphere - Build"},{"location":"was90/tWAS-build/#traditional-websphere-build","text":"This section covers how to containerize an existing application running on traditional WebSphere Application Server. To do this we have two options on how to migrate. Option 1 (descibed below): Create or use existing wsadmin scripts. Option 2: Use the WebSphere Configuration Migration Tool for IBM Cloud (WCMT4IC) to extract the configuration from your existing traditional WebSphere Application Server environment. We're going to focus on the method of using existing scripts, since it better matches the best practice for CI/CD pipelines. Once the environment is running as is, we'll learn how to make modifications to the configuration without having to rebuild your container, and why this is important. The final versions of the files created in this section can be found in the was90 branch of this repo","title":"traditional WebSphere - Build"},{"location":"was90/tWAS-build/#summary","text":"This section has the following steps: Build the image using existing scripts Test the application locally Using environment variables in properties files to allow configuration to be injected in to the runtime environment dynamically. Advanced: put your passwords in secrets.","title":"Summary:"},{"location":"was90/tWAS-build/#building-a-traditional-websphere-docker-container-using-existing-wsadmin-scripts","text":"For this example, we're going to use the Customer Order Application as an example. We already have a wsadmin script which will configure the environment and install the application. Review the contents of the wsadmin script . This is a legacy application which runs on older frameworks; most notably it uses JPA 2.0 and JAX-RS 1.1. These are not the default in WAS 9 (as they are in WAS8.5.5, but they are supported). It is important to modify these settings in the scripts if you don't want to make modifications to your application at this time. We are going to install the application using a properties file, which is an alternative way to install the application, or apply any configuration. The application could also be installed using the wsadmin jython script, but we chose to use the properties file to show the two methods configurations can be applied at build time. Review the contents of the properties file To run these files during container creation we copy them into /work/config/ folder using the Dockerfile. Then we run the /work/configure.sh which will start the server and run the scripts and apply the properties file configuration. We also copy the db2 drivers and the application, and anything else the applications needs into the container. Review the contents of the Dockerfile . ARG WEBSPHERE_VERSION = 9 . 0 . 0 . 11 # Migration requires websphere - traditional >= 9 . 0 . 0 . 11 FROM ibmcom / websphere - traditional :$ WEBSPHERE_VERSION as migration # Hardcode password for admin console COPY -- chown = was : 0 tWAS / PASSWORD / tmp / PASSWORD COPY -- chown = was : 0 resources / db2 / / opt / IBM / db2drivers / COPY -- chown = was : 0 resources / jmx_exporter / jmx_prometheus_javaagent - 0 . 11 . 0 . jar / opt / IBM / jmx_exporter / COPY -- chown = was : 0 resources / jmx_exporter / jmx - config . yaml / opt / IBM / jmx_exporter / # Uncomment to test locally # COPY -- chown = was : 0 tWAS / jvm . props / work / config COPY -- chown = was : 0 tWAS / cosConfig . py / work / config / COPY -- chown = was : 0 tWAS / app - update . props / work / config / app - update . props COPY -- chown = was : 0 tWAS / CustomerOrderServicesApp - 0 . 1 . 0 - SNAPSHOT . ear / work / config / CustomerOrderServicesApp - 0 . 1 . 0 - SNAPSHOT . ear RUN / work / configure . sh Let's try it...","title":"Building a traditional WebSphere Docker container using existing wsadmin scripts."},{"location":"was90/tWAS-build/#build-the-image","text":"Now it's time to build the image. This guide assumes you have Docker installed. The code existing in the was90 branch. Clone the branch to copy the files to your system git clone -- branch was90 https : // github . com / ibm - cloud - architecture / cloudpak - for - applications . git cd cloudpak - for - applications docker build -- tag customer - order - app . You should see the following messages if image was successfully built (image id may vary): Successfully built f8d3eafdd30a Successfully tagged customer - order - app : latest Validate that image is in the repository using command (you should also see original websphere-traditional image): $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE customer - order - app latest f8d3eafdd30a 3 days ago 1 . 85 GB ibmcom / websphere - traditional 9 . 0 . 0 . 11 2985 c9be759f 10 days ago 1 . 78 GB","title":"Build the image"},{"location":"was90/tWAS-build/#test-the-image","text":"To test the image locally you are also going to need a database to connect to locally. We aren't going to go into this much. Since we want the DB2 and tWAS containers to be able to talk to each other we create a network they can share $ docker network create customer - order Now we start DB2. This DB2 contains scripts to populate the database with sample data for our application on startup. $ docker run - d --network customer-order --privileged=true -e LICENSE=accept -e DB2INST1_PASSWORD=db2inst1 -e DBNAME=orderdb --name db2-customer-order vandepol/db2-cos This take a while to start up since it creates the database from scratch. to access this database from the tWAS container, we use the hostname = container name. In this case the hostname would be db2-customer-order You are going to test the image in local Docker engine. Issue the following command to start the container: $ docker run - d --network customer-order --name twas-customer-order -p 9443:9443 -p 9043:9043 customer-order-app:latest Check if container started successfully using command: $d ocker logs - f 9 ee253a59956869c6 WASX7303I : The following options are passed to the scripting environment and are available as arguments that are stored in the argv variable : \" [/work/config-ibm/webContainer.props] \" Starting server ................... ADMU3000I : Server server1 open for e - business ; process id is 494 HPEL is enabled run logViewer . sh Right now the problem we have is that the database hostname in the scripts we used to configure the application doesn't match the hostname of the db2 docker container we started. That leads us to our next section on modifying the configuration without rebuilding the container. We could simply modify the jython scripts and change the hostname of the databases to db2-customer-order , however we're going to do it using configuration files.","title":"Test the image"},{"location":"was90/tWAS-build/#modifying-configuration-without-rebuilding-container","text":"When building the traditional WebSphere container there may be configuration you will want to add or change between environments. You don't want to have to rebuild your container just because a database hostname changed, or for password changes. To solve this problem, you can load configuration changes at server start-up using properties files. In this section we'll show how to extract the configuration into properties files. In the Deploy section we'll describe how to create the configMaps and load them at server start-up when deploying using the cloudpak. We have pre-extracted configuration files in the runtimeConfig folder, however if you want to extract your own you can follow the steps below. We start with a server environment setup and running with the resources defined as you desire. We can use the docker container we created above. We can run this locally. docker run -d --rm --name twas-customer-order -p 9443:9443 -p 9043:9043 -p9080:9080 -v /myconfigs/:/etc/websphere/ customer-order-app:latest Here we expose port 9043 for the WebSphere admin console. We also mount a volume so that we can extract the properties files we're generating locally. Next, we ssh into the container to execute commands to extract the configuration. To enter the shell script of our container locally we run the command: docker exec -it twas-customer-order bash Once in the container shell, we execute the following commands /opt/IBM/WebSphere/AppServer/bin/wsadmin.sh -lang jython you will be prompted for your credentials. Use the credentials you would use in the Admin Console. in our example we have: wasadmin/passw0rd Run the following command to extract the Datasource configuration and the Password information: AdminTask . extractConfigProperties ( ' [ - propertiesFileName / etc / websphere / Datasource . inds . props - configData DataSource = INDS - options [[ PortablePropertiesFile true ]]] ' ) AdminTask . extractConfigProperties ( ' [ - propertiesFileName / etc / websphere / Datasource . orderds . props - configData DataSource = OrderDS - options [[ PortablePropertiesFile true ]]] ' ) AdminTask . extractConfigProperties ( ' [ - propertiesFileName / etc / websphere / JAASAuth . db2password . props - configData Security = - filterMechanism SELECTED_SUBTYPES - selectedSubTypes [ JAASAuthData ] - options [[ PortablePropertiesFile true ]]] ' ) The syntax for these commands can get complicated. Documentation available here Sample Configuration Files Datasource.inds.props Datasource.orderds.props JAASAuth.db2password.props If we want to change something in this configuration, we can make a change to these properties files. To apply this to the container we can do this at container build time by copying the properties file into the /work/config folder as described in the Modifying Artifact section . However, if you want to modify these at deploy time without rebuilding the docker container (ideal for passwords and database hostnames which change between environments), you can do a volume mount to /etc/websphere , and these will be applied at server start-up. Let's try this! Open the properties files, Datasource.inds.props and Datasource.orderds.props . Here let's change the server name of the database we're accessing to the name of our local db2 container we start above. serverName = db2 - customer - order # String If our container is still running from before we want to stop it so that we can restart it and allow for the new properties to be loaded. docker stop twas-customer-order Restart the docker container and map the properties files again. Remember to connect to the same network of the db2 container we started above docker run -d --rm --name twas-customer-order --network customer-order -p 9443:9443 -p 9043:9043 -p 9080:9080 -v /runtimeConfigs/:/etc/websphere/ customer-order-app:guide We can look at the logs to see if the properties files are loaded. TIP : the properties files MUST end in .props. it will not detect different file types in the /etc/websphere folder at server start-up. $ docker logs twas-customer-order -f ENABLE_BASIC_LOGGING is Configure logging mode WASX7357I: By request, this scripting client is not connected to any server process. Certain configuration and application operations will be available in local mode. + Found config-files under /etc/websphere. Executing... WASX7357I: By request, this scripting client is not connected to any server process. Certain configuration and application operations will be available in local mode. WASX7303I: The following options are passed to the scripting environment and are available as arguments that are stored in the argv variable: \"[/etc/websphere/Datasource.inds.props]\" WASX7357I: By request, this scripting client is not connected to any server process. Certain configuration and application operations will be available in local mode. WASX7303I: The following options are passed to the scripting environment and are available as arguments that are stored in the argv variable: \"[/etc/websphere/Datasource.orderds.props]\" WASX7357I: By request, this scripting client is not connected to any server process. Certain configuration and application operations will be available in local mode. WASX7303I: The following options are passed to the scripting environment and are available as arguments that are stored in the argv variable: \"[/etc/websphere/JAASAuth.db2password.props]\" WASX7357I: By request, this scripting client is not connected to any server process. Certain configuration and application operations will be available in local mode. How we can log into the admin console to see our changes.","title":"Modifying Configuration without rebuilding container"},{"location":"was90/tWAS-build/#verify-the-application","text":"After the server is successfully started, you can connect to it using browser. Use http://localhost:9043/ibm/console to access admin console of the server running in the container. Login using your original credentials, that where migrated. Also validate that application has successfully started And the Datasources were also migrated Finally open the application https://localhost:9443/CustomerOrderServicesWeb use rbarcia/bl0wfish for username and password if prompted. You have successfully migrated application to the traditional WebSphere Application Server container. Good job!","title":"Verify the application"},{"location":"was90/tWAS-build/#advanced-put-your-passwords-in-secrets","text":"One issue with the above scenario is that the contents of the JAASAuth.db2password.props is in plain text (or XOR encrypted...which isn't hard to crack). Typically, we wouldn't want to store this in a properties file, which would then be stored as a configmap in Kubernetes. We'd want to use secrets. The way secrets work is that the properties loaded into the secret are loaded as environment variables at container start-up. Ideally, we would want to replace values in the properties files we've loaded above with environment variables, this is currently not supported in the cloudpak (Feature request made to development); however, we have a work around to load these values ourselves. First, we would replace a value with reference to an environment variable. Let's use that the port number as an example again. syncQueryTimeoutWithTransactionTimeout = # boolean freeResourcesOnClose = false # String traceFileCount = null ** portNumber =! { inds . port . number } # integer ** stripTrailingZerosForDecimalNumbers = null translateForBitData = null webSphereDefaultQueryTimeout = # integer clientApplicationInformation = # String Here we have assigned the portNumber to the environment variable inds.port.number . We have created a simple script to run prior to server startup which will replace all references to environment variables with the environment variable values. The script we created is available here 1 2 3 4 5 6 7 8 #!/bin/sh mkdir /etc/websphere/orig cp /etc/websphere/*.props /etc/websphere/orig/ for f in $( find /etc/websphere/orig/ -regex '.*\\.props' ) ; do envsubst < $f > \"/etc/websphere/ $( basename $f ) \" ; done One issue is that we need to install envsubst into the container, which is included inside the gettext package. We have to run apt-get install to install this. Next we need to copy our script into the container, and lastly we need to modify the startup command to run this command prior to server start-up. Here is our solution to this: RUN apt - get update && apt - get install - y gettext COPY --chown=was:0 envsubst-cmd.sh /work/ CMD [ \"sh\" , \"-c\" , \"/work/envsubst-cmd.sh && /work/start_server.sh\" ] Now we should be able to start up the container and pass a new value as an environment variable to change the port number. docker run -d --rm --name customer-order-app -p9443:9443 -p 9043:9043 -v /myconfigs/:/etc/websphere/ -e inds.port.number=50002 customer-order-app:latest To specify environment variable in the properties file you can replace the value with ${inds.port.number}","title":"Advanced:  Put your passwords in Secrets!"},{"location":"was90/tWAS-build/#conclusion","text":"In conclusion, we've taken our estisting traditional WebSphere application. We've loaded the container with the server and application configuration. We've installed the application, and modified the configuration if required. This sets us up for integrating all of this into a CI/CD pipeline and then deploying this to Kubernetes. We're on our way to the cloud!","title":"Conclusion"},{"location":"was90/tWAS-deploy/","text":"Introduction This section covers how to deploy the application to RedHat OpenShift using an automated CI/CD pipeline. The diagram below shows the flow of the pipeline which starts when the developer checks their code in to Git and ends with the application being deployed in Production. the developer checks their code in to git a webhook automatically triggers the Jenkins Pipeline in the RHOS cluster the pipeline checks out the code from git and uses maven to build and test the application the oc start build command is used to build a docker image for the application the image is added to the ImageStream and pushed to the docker registry in the RHOS cluster the image is then tagged for the dev project the deployment running in the dev project is restarted using the newly created image the image is then tagged for the stage project the deployment running in the stage project is restarted using the newly created image the developer is prompted to approve the deployment to production the image is then tagged for the prod project the deployment running in the prod project is restarted using the newly created image The final versions of the files created in this section can be found in the was90 branch of this repo Summary This section has the following steps: RedHat OpenShift terminology and constructs The Jenkinsfile for the pipeline Create the Security Context Constraint required for WebSphere to run on RedHat OpenShift Create the build , dev , stage and prod projects Create a new Service Account and bind it to the Security Context Constraint in the dev , stage and prod projects Deploy Jenkins in the build namespace Grant the jenkins service account edit privileges in the dev , stage and prod projects Import the deployment template in to the dev , stage and prod projects Create the deployment config , service and route definitions using the template in the dev , stage and prod projects Import the build template in to the build project Create the ImageStream and BuildConfig definitions in the build project Trigger the pipeline and validate the application is running RedHat OpenShift constructs It is assumed that the reader is familiar with the following basic constructs provided by Kubernetes: Pod Deployment ServiceAccount Service RedHat OpenShift has the following constructs that are used in this scenario: - Route - ImageStream - BuildConfig - DeploymentConfig - SecurityContextConstraints - Application - Template The Jenkinsfile for the pipeline A Jenkinsfile contains the definition of a pipeline in a format that can be stored in source control. In addition to standard Jenkins Pipeline Syntax, the OpenShift provides a Domain Specific Language (DSL) through the OpenShift Jenkins Client Plug-in. OpenShift DSL is easily readable that interacts with OpenShift API server, giving more control over the build, deployment, and promotion of applications on OpenShift cluster. The Jenkinsfile for this application can be found here The Jenkinsfile has the following stages : preamble : outputs some variable valus to the console Maven Build : builds the application EAR file using maven Unit Test : tests the application using maven Build twas App Image : uses the BuildConfig to build the Docker Image using the provided Dockerfile . This automatically tags the image in the build Project's ImageStream Promote to Dev : tags the image to the dev Project's ImageStream which results in the deployment in the dev Project being restarted with the new image Promote to Stage : tags the image to the stage Project's ImageStream which results in the deployment in the stage Project being restarted with the new image Promotion gate : prompts the user to approve promotion to production Promote to Prod : the image to the prod Project's ImageStream which results in the deployment in the prod Project being restarted with the new image Create the Security Context Constraint In order to deploy and run the WebSphere Base Docker image in an OpenShift cluster, we first need to configure certain security aspects for the cluster. The Security Context Constraint provided here grants the service account that the WebSphere Base Docker container is running under the required privileges to function correctly. A cluster administrator can use the file provided here with the following command to create the Security Context Constraint (SCC): oc apply - f ssc . yaml Create the projects Four RedHat OpenShift projects are required in this scenario: - Build: this project will contain the Jenkins server and the artifacts used to build the application image - Dev: this is the development environment for this application - Stage: this is the staging environment for this application - Prod: this is the production environment for this application The file provided here contains the definitions for the four projects in a single file to make creation easier Issue the command shown below to create the projects oc create - f twas - projects . yaml Create a service account It is a good Kubernetes practice to create a service account for your applications. A service account provides an identity for processes that run in a Pod. In this step we will create a new service account with the name websphere in each of the dev , stage and prod projects and add the Security Context Constraint created above to them. Issue the commands shown below to create the websphere service account and bind the ibm-websphere-scc to it in each of the projects: oc create serviceaccount websphere - n cos - twas - dev oc create serviceaccount websphere - n cos - twas - stage oc create serviceaccount websphere - n cos - twas - prod oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - twas - dev oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - twas - stage oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - twas - prod Deploy Jenkins Some RedHat OpenShift clusters are configured to automatically provision a Jenkins instance in a build project. The steps below can be used if your cluster is not configured for automatic Jenkins provisioning: oc project cos - twas - build oc new - app jenkins - persistent Update the Jenkins service account During provisioning of the Jenkins master a service account with the name jenkins is created. This service account has privileges to create new artifacts only in the project that it is running in. In this scenario Jenkins will need to create artifacts in the dev , stage and prod projects. Issue the commands below to allow the jenkins service account to edit artifacts in the dev , stage and prod projects. oc policy add - role - to - user edit system : serviceaccount : cos - twas - build : jenkins - n cos - twas - dev oc policy add - role - to - user edit system : serviceaccount : cos - twas - build : jenkins - n cos - twas - stage oc policy add - role - to - user edit system : serviceaccount : cos - twas - build : jenkins - n cos - twas - prod Import the deployment templates RedHat OpenShift templates are used to make artifact creation easier and repeatable. The template definition provided here defines a Kubernetes Service , Route and DeploymentConfig for the CustomerOrderServices application. The gse-twas-deploy template defines the following: - service listening on ports 9080 , 9443 and 9082 - route to expose the 9443 port externally - DeploymentConfig to host the WebSphere Base container. - The image for the container is taken from the ImageStream that will be populated by the Jenkins pipeline. - environment variables are defined for the DB2 database used by the application allowing for environment specific information to be injected - Probes for liveness and readiness are defined to check port 9443 is active - The securityContext is set to allow read/write access to the filesystem and to run the container as user 1001 - The deployment will be updated if a new image is loaded to the ImageStream or if a change to the configuration is detected. Issue the commands below to load the template named gse-twas-deploy in the dev , stage and prod projects. oc create - f template - twas - deploy . yaml - n cos - twas - dev oc create - f template - twas - deploy . yaml - n cos - twas - stage oc create - f template - twas - deploy . yaml - n cos - twas - prod Create the deployment definitions In this step the gse-twas-deploy template will be used to create a RedHat OpenShift application named cos-twas in the dev , stage and prod namespaces. The result will be: - service listening on ports 9080 , 9443 and 9082 - route to expose the 9443 port externally - DeploymentConfig to host the WebSphere Base container. The deployment config will wait for a docker image to be loaded in to the ImageStream by the Jenkins pipeline. Issue the following commands to create the applications from the template: oc new - app gse - twas - deploy - p APPLICATION_NAME = cos - twas - n cos - twas - dev oc new - app gse - twas - deploy - p APPLICATION_NAME = cos - twas - n cos - twas - stage oc new - app gse - twas - deploy - p APPLICATION_NAME = cos - twas - n cos - twas - prod Import the build templates In this step a template for the build process will be loaded in to the build project. The template provided here defines the following artifacts: An ImageStream for the application image. This will be populated by the Jenkins Pipeline An ImageStream for WebSphere Base which will pull down the latest version of the ibmcom/websphere-traditional:latest-ubi image and will monitor DockerHub for any updates. A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub Parameters to allow the WebSphere Base image and GitHub repository to be provided when the template is instantiated Issue the commands below to load the template named gse-twas-build in the build projects. oc create - f template - twas - build . yaml - n cos - twas - build Create the build definitions In this step the gse-twas-build template will be used to create a RedHat OpenShift application named cos-twas in the build namespaces. The result will be: - An ImageStream for the application image. This will be populated by the Jenkins Pipeline - An ImageStream for WebSphere Base which will pull down the latest version of the ibmcom/websphere-traditional:latest-ubi image and will monitor DockerHub for any updates. - A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image - A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub (with the URL provided as a parameter when the application is created) Issue the following commands to create the application from the template: oc new - app gse - twas - build - p APPLICATION_NAME = cos - twas - p SOURCE_URL = \" https://github.com/ibm-cloud-architecture/cloudpak-for-applications \" - n cos - twas - build Run the pipeline The newly created pipeline can be started from the RedHat OpenShift console which allows access to the Jenkins logs but also tracks the progress in the OCP console. Navigate to Application Console --> Customer Order Services on twas - Build --> Builds --> Pipelines and click the Start Pipeline button When the pipeline starts, click the view log link to go to the Jenkins administration console. Note that it may take a couple of minutes before the view log link appears on the first pipeline build When prompted, log in with your OpenShift account and grant the required access permissions. The Jenkins console log will be displayed as shown below: Return to the OpenShift Console and track the progress of the pipeline The pipeline will eventually stop at the Promotion Gate for approval to deploy to Production. Click the Input Required link as shown below When the Promote application to Production question is displayed, click Proceed Return to the OpenShift Console and validate that the pipeline is now complete Validate the Deployments Now that the pipeline is complete, validate the Customer Order Services application is deployed and running in dev , stage and prod In the OpenShift Console, navigate to Application Console --> Customer Order Services on twas - Dev --> Applications --> Deployments and click on the link in the Latest Version column Information about the deployment will be displayed including the image that is being used (note the tag on the image as it will be the same in the stage and prod deployments). After a few minutes the container will be marked as ready Click Applications --> Routes and click on the route for the application. Note that the URL is < application_name >-< project_name >.< ocp cluster url >. In this case the project name is cos-twas-dev Add /CustomerOrderServicesWeb to the end of the URL in the browser to access the application Repeat the validations for the stage and prod Projects. Review and Next Steps In this section you configured a CI/CD pipeline for the CustomerOrderServices application that builds a single immutable image for the latest version of the application and then deploys it to three different environments.","title":"Introduction"},{"location":"was90/tWAS-deploy/#introduction","text":"This section covers how to deploy the application to RedHat OpenShift using an automated CI/CD pipeline. The diagram below shows the flow of the pipeline which starts when the developer checks their code in to Git and ends with the application being deployed in Production. the developer checks their code in to git a webhook automatically triggers the Jenkins Pipeline in the RHOS cluster the pipeline checks out the code from git and uses maven to build and test the application the oc start build command is used to build a docker image for the application the image is added to the ImageStream and pushed to the docker registry in the RHOS cluster the image is then tagged for the dev project the deployment running in the dev project is restarted using the newly created image the image is then tagged for the stage project the deployment running in the stage project is restarted using the newly created image the developer is prompted to approve the deployment to production the image is then tagged for the prod project the deployment running in the prod project is restarted using the newly created image The final versions of the files created in this section can be found in the was90 branch of this repo","title":"Introduction"},{"location":"was90/tWAS-deploy/#summary","text":"This section has the following steps: RedHat OpenShift terminology and constructs The Jenkinsfile for the pipeline Create the Security Context Constraint required for WebSphere to run on RedHat OpenShift Create the build , dev , stage and prod projects Create a new Service Account and bind it to the Security Context Constraint in the dev , stage and prod projects Deploy Jenkins in the build namespace Grant the jenkins service account edit privileges in the dev , stage and prod projects Import the deployment template in to the dev , stage and prod projects Create the deployment config , service and route definitions using the template in the dev , stage and prod projects Import the build template in to the build project Create the ImageStream and BuildConfig definitions in the build project Trigger the pipeline and validate the application is running","title":"Summary"},{"location":"was90/tWAS-deploy/#redhat-openshift-constructs","text":"It is assumed that the reader is familiar with the following basic constructs provided by Kubernetes: Pod Deployment ServiceAccount Service RedHat OpenShift has the following constructs that are used in this scenario: - Route - ImageStream - BuildConfig - DeploymentConfig - SecurityContextConstraints - Application - Template","title":"RedHat OpenShift constructs"},{"location":"was90/tWAS-deploy/#the-jenkinsfile-for-the-pipeline","text":"A Jenkinsfile contains the definition of a pipeline in a format that can be stored in source control. In addition to standard Jenkins Pipeline Syntax, the OpenShift provides a Domain Specific Language (DSL) through the OpenShift Jenkins Client Plug-in. OpenShift DSL is easily readable that interacts with OpenShift API server, giving more control over the build, deployment, and promotion of applications on OpenShift cluster. The Jenkinsfile for this application can be found here The Jenkinsfile has the following stages : preamble : outputs some variable valus to the console Maven Build : builds the application EAR file using maven Unit Test : tests the application using maven Build twas App Image : uses the BuildConfig to build the Docker Image using the provided Dockerfile . This automatically tags the image in the build Project's ImageStream Promote to Dev : tags the image to the dev Project's ImageStream which results in the deployment in the dev Project being restarted with the new image Promote to Stage : tags the image to the stage Project's ImageStream which results in the deployment in the stage Project being restarted with the new image Promotion gate : prompts the user to approve promotion to production Promote to Prod : the image to the prod Project's ImageStream which results in the deployment in the prod Project being restarted with the new image","title":"The Jenkinsfile for the pipeline"},{"location":"was90/tWAS-deploy/#create-the-security-context-constraint","text":"In order to deploy and run the WebSphere Base Docker image in an OpenShift cluster, we first need to configure certain security aspects for the cluster. The Security Context Constraint provided here grants the service account that the WebSphere Base Docker container is running under the required privileges to function correctly. A cluster administrator can use the file provided here with the following command to create the Security Context Constraint (SCC): oc apply - f ssc . yaml","title":"Create the Security Context Constraint"},{"location":"was90/tWAS-deploy/#create-the-projects","text":"Four RedHat OpenShift projects are required in this scenario: - Build: this project will contain the Jenkins server and the artifacts used to build the application image - Dev: this is the development environment for this application - Stage: this is the staging environment for this application - Prod: this is the production environment for this application The file provided here contains the definitions for the four projects in a single file to make creation easier Issue the command shown below to create the projects oc create - f twas - projects . yaml","title":"Create the projects"},{"location":"was90/tWAS-deploy/#create-a-service-account","text":"It is a good Kubernetes practice to create a service account for your applications. A service account provides an identity for processes that run in a Pod. In this step we will create a new service account with the name websphere in each of the dev , stage and prod projects and add the Security Context Constraint created above to them. Issue the commands shown below to create the websphere service account and bind the ibm-websphere-scc to it in each of the projects: oc create serviceaccount websphere - n cos - twas - dev oc create serviceaccount websphere - n cos - twas - stage oc create serviceaccount websphere - n cos - twas - prod oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - twas - dev oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - twas - stage oc adm policy add - scc - to - user ibm - websphere - scc - z websphere - n cos - twas - prod","title":"Create a service account"},{"location":"was90/tWAS-deploy/#deploy-jenkins","text":"Some RedHat OpenShift clusters are configured to automatically provision a Jenkins instance in a build project. The steps below can be used if your cluster is not configured for automatic Jenkins provisioning: oc project cos - twas - build oc new - app jenkins - persistent","title":"Deploy Jenkins"},{"location":"was90/tWAS-deploy/#update-the-jenkins-service-account","text":"During provisioning of the Jenkins master a service account with the name jenkins is created. This service account has privileges to create new artifacts only in the project that it is running in. In this scenario Jenkins will need to create artifacts in the dev , stage and prod projects. Issue the commands below to allow the jenkins service account to edit artifacts in the dev , stage and prod projects. oc policy add - role - to - user edit system : serviceaccount : cos - twas - build : jenkins - n cos - twas - dev oc policy add - role - to - user edit system : serviceaccount : cos - twas - build : jenkins - n cos - twas - stage oc policy add - role - to - user edit system : serviceaccount : cos - twas - build : jenkins - n cos - twas - prod","title":"Update the Jenkins service account"},{"location":"was90/tWAS-deploy/#import-the-deployment-templates","text":"RedHat OpenShift templates are used to make artifact creation easier and repeatable. The template definition provided here defines a Kubernetes Service , Route and DeploymentConfig for the CustomerOrderServices application. The gse-twas-deploy template defines the following: - service listening on ports 9080 , 9443 and 9082 - route to expose the 9443 port externally - DeploymentConfig to host the WebSphere Base container. - The image for the container is taken from the ImageStream that will be populated by the Jenkins pipeline. - environment variables are defined for the DB2 database used by the application allowing for environment specific information to be injected - Probes for liveness and readiness are defined to check port 9443 is active - The securityContext is set to allow read/write access to the filesystem and to run the container as user 1001 - The deployment will be updated if a new image is loaded to the ImageStream or if a change to the configuration is detected. Issue the commands below to load the template named gse-twas-deploy in the dev , stage and prod projects. oc create - f template - twas - deploy . yaml - n cos - twas - dev oc create - f template - twas - deploy . yaml - n cos - twas - stage oc create - f template - twas - deploy . yaml - n cos - twas - prod","title":"Import the deployment templates"},{"location":"was90/tWAS-deploy/#create-the-deployment-definitions","text":"In this step the gse-twas-deploy template will be used to create a RedHat OpenShift application named cos-twas in the dev , stage and prod namespaces. The result will be: - service listening on ports 9080 , 9443 and 9082 - route to expose the 9443 port externally - DeploymentConfig to host the WebSphere Base container. The deployment config will wait for a docker image to be loaded in to the ImageStream by the Jenkins pipeline. Issue the following commands to create the applications from the template: oc new - app gse - twas - deploy - p APPLICATION_NAME = cos - twas - n cos - twas - dev oc new - app gse - twas - deploy - p APPLICATION_NAME = cos - twas - n cos - twas - stage oc new - app gse - twas - deploy - p APPLICATION_NAME = cos - twas - n cos - twas - prod","title":"Create the deployment definitions"},{"location":"was90/tWAS-deploy/#import-the-build-templates","text":"In this step a template for the build process will be loaded in to the build project. The template provided here defines the following artifacts: An ImageStream for the application image. This will be populated by the Jenkins Pipeline An ImageStream for WebSphere Base which will pull down the latest version of the ibmcom/websphere-traditional:latest-ubi image and will monitor DockerHub for any updates. A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub Parameters to allow the WebSphere Base image and GitHub repository to be provided when the template is instantiated Issue the commands below to load the template named gse-twas-build in the build projects. oc create - f template - twas - build . yaml - n cos - twas - build","title":"Import the build templates"},{"location":"was90/tWAS-deploy/#create-the-build-definitions","text":"In this step the gse-twas-build template will be used to create a RedHat OpenShift application named cos-twas in the build namespaces. The result will be: - An ImageStream for the application image. This will be populated by the Jenkins Pipeline - An ImageStream for WebSphere Base which will pull down the latest version of the ibmcom/websphere-traditional:latest-ubi image and will monitor DockerHub for any updates. - A binary BuildConfig that will be used by the Jenkins Pipeline to build the application Docker image - A jenkinsfile BuildConfig that defines the Pipeline using the Jenkinsfile in GitHub (with the URL provided as a parameter when the application is created) Issue the following commands to create the application from the template: oc new - app gse - twas - build - p APPLICATION_NAME = cos - twas - p SOURCE_URL = \" https://github.com/ibm-cloud-architecture/cloudpak-for-applications \" - n cos - twas - build","title":"Create the build definitions"},{"location":"was90/tWAS-deploy/#run-the-pipeline","text":"The newly created pipeline can be started from the RedHat OpenShift console which allows access to the Jenkins logs but also tracks the progress in the OCP console. Navigate to Application Console --> Customer Order Services on twas - Build --> Builds --> Pipelines and click the Start Pipeline button When the pipeline starts, click the view log link to go to the Jenkins administration console. Note that it may take a couple of minutes before the view log link appears on the first pipeline build When prompted, log in with your OpenShift account and grant the required access permissions. The Jenkins console log will be displayed as shown below: Return to the OpenShift Console and track the progress of the pipeline The pipeline will eventually stop at the Promotion Gate for approval to deploy to Production. Click the Input Required link as shown below When the Promote application to Production question is displayed, click Proceed Return to the OpenShift Console and validate that the pipeline is now complete","title":"Run the pipeline"},{"location":"was90/tWAS-deploy/#validate-the-deployments","text":"Now that the pipeline is complete, validate the Customer Order Services application is deployed and running in dev , stage and prod In the OpenShift Console, navigate to Application Console --> Customer Order Services on twas - Dev --> Applications --> Deployments and click on the link in the Latest Version column Information about the deployment will be displayed including the image that is being used (note the tag on the image as it will be the same in the stage and prod deployments). After a few minutes the container will be marked as ready Click Applications --> Routes and click on the route for the application. Note that the URL is < application_name >-< project_name >.< ocp cluster url >. In this case the project name is cos-twas-dev Add /CustomerOrderServicesWeb to the end of the URL in the browser to access the application Repeat the validations for the stage and prod Projects.","title":"Validate the Deployments"},{"location":"was90/tWAS-deploy/#review-and-next-steps","text":"In this section you configured a CI/CD pipeline for the CustomerOrderServices application that builds a single immutable image for the latest version of the application and then deploys it to three different environments.","title":"Review and Next Steps"}]}